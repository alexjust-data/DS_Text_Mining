{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894f082",
   "metadata": {
    "id": "1894f082"
   },
   "source": [
    "\n",
    "# Procesamiento y análisis de información textual\n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos en los módulos 1, 2 y 3. Concretamente trataremos 5 temas.\n",
    "\n",
    "<ul>\n",
    "<li>1. Preparación del dataset: pre-procesamiento de texto.\n",
    "<li>2. Obtención de datos a partir de información textual.\n",
    "<li>3. Detección de tópicos.\n",
    "<li>4. Clasificación de textos.\n",
    "<li>5. Evaluación: comparación de modelos y discusión de resultados.\n",
    "</ul>\n",
    "    \n",
    "El <b>propósito</b> de la práctica es descubrir rasgos característicos de un conjunto de reseñas de productos de software de <i>Amazon</i>, utilizando las técnicas explicadas y ver si es posible clasificar automáticamente una opinión como positiva o negativa con métodos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bf012",
   "metadata": {
    "id": "bb0bf012"
   },
   "source": [
    "<b> Descripción del Dataset</b>\n",
    "\n",
    "- Título: Amazon Review Data\n",
    "\n",
    "- Fuente: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/#complete-data\n",
    "\n",
    " Utilizaremos un subconjunto de las reseñas disponibles en el dataset <i>Software</i>, el cual contiene opiniones realizadas por los clientes a diferentes productos de software disponibles en Amazon. El dataset original tiene 459.436 instancias, aunque en esta práctica vamos a trabajar con menos del 5% de ellas.\n",
    "\n",
    "En cuanto a la estructura del dataset, éste tiene 12 atributos:\n",
    "\n",
    "- <b>overall</b>: rating o calificación del producto; el valor va desde 1 (insatisfecho), hasta 5 (satisfecho).<br>\n",
    "- <b>verified</b>: indica si el mensaje ha sido verificado.<br>\n",
    "- <b>reviewTime</b>: fecha de la reseña (sin procesar).<br>\n",
    "- <b>reviewerID</b>: ID del revisor, por ejemplo A2SUAM1J3GNN3B.<br>\n",
    "- <b>asin</b>: ID del producto, p. 0000013714.<br>\n",
    "- <b>style</b>: diccionario de los metadatos del producto; por ejemplo, \"Platform\": \"Mac\".<br>\n",
    "- <b>reviewerName</b>: nombre del revisor.<br>\n",
    "- <b>reviewText</b>: texto de la reseña.<br>\n",
    "- <b>summary</b>: resumen de la reseña.<br>\n",
    "- <b>unixReviewTime</b>: fecha de la reseña (en formato Unix).<br>\n",
    "- <b>vote</b>:votos útiles de la revisión <br>\n",
    "- <b>image</b>: imágenes que los usuarios publican después de haber recibido el producto.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1333f6b",
   "metadata": {
    "id": "d1333f6b"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d20de",
   "metadata": {
    "id": "3d8d20de"
   },
   "source": [
    "# 1. Preparación del dataset: pre-procesamiento de texto \n",
    "\n",
    "\n",
    "En este primer apartado realizaremos la carga del dataset original, seleccionaremos un subconjunto de las instancias y prepararemos (limpieza) el texto de las reseñas antes de realizar el análisis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hqRgiWGkHfVL",
   "metadata": {
    "id": "hqRgiWGkHfVL"
   },
   "source": [
    "## 1.1 Carga y creación del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbf74bac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "bbf74bac",
    "outputId": "d5f1f3ee-8b0a-46f1-dbfc-fc5181ee7c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset (15000, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>04 8, 2009</td>\n",
       "      <td>A2CNXL7UGZYFG4</td>\n",
       "      <td>B000QO76HU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E. HOFFMAN</td>\n",
       "      <td>STOP RIGHT THERE ! This is a real review that is not motivated by deception.\\nThe simple four (4) rules that YNAB software helps you follow is the combination that ha...</td>\n",
       "      <td>YNAB - WHY IT CHANGED MY LIFE - APRIL 2009</td>\n",
       "      <td>1239148800</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>09 4, 2008</td>\n",
       "      <td>A3EZD11AFUX23K</td>\n",
       "      <td>B0017I8NQM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jason P. Gold</td>\n",
       "      <td>Please see [...] for a comparison of the features in pre-ribbon versions of Word to WordPerfect.  It is enlightening!\\n\\nIt is hard to believe it is two years since C...</td>\n",
       "      <td>The Better Wordprocessor</td>\n",
       "      <td>1220486400</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13167</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 2, 2013</td>\n",
       "      <td>A2H7NSYY9Q1UJZ</td>\n",
       "      <td>B000GD5DS0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wallace Pierce</td>\n",
       "      <td>Just did not fit my needs.  My computer crashed &amp; I had to replace my money program, so I ordered the 2007 version thinking it might be an improvement.  It was an imp...</td>\n",
       "      <td>My opinion of the Money 2007 Deluxe version.</td>\n",
       "      <td>1380672000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>06 28, 2015</td>\n",
       "      <td>A1XJ7XUR2W0J8V</td>\n",
       "      <td>B00RKZKFUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jerry De La Cruz</td>\n",
       "      <td>Everything this little program does can be done yourself with some programming in your word processor.  But why bother; this thing does it simply and intuitively.  Th...</td>\n",
       "      <td>The optional little graphics are a nice touch. Make sure you use a blank sheet ...</td>\n",
       "      <td>1435449600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>08 13, 2018</td>\n",
       "      <td>A6OUFMC6D07OF</td>\n",
       "      <td>B0144NYGJY</td>\n",
       "      <td>{'Platform:': ' Key Card [12 month]'}</td>\n",
       "      <td>Barry</td>\n",
       "      <td>worked and useful as described</td>\n",
       "      <td>worked and useful as described</td>\n",
       "      <td>1534118400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall  verified   reviewTime      reviewerID        asin  \\\n",
       "11499      5.0     False   04 8, 2009  A2CNXL7UGZYFG4  B000QO76HU   \n",
       "6475       5.0     False   09 4, 2008  A3EZD11AFUX23K  B0017I8NQM   \n",
       "13167      2.0      True   10 2, 2013  A2H7NSYY9Q1UJZ  B000GD5DS0   \n",
       "862        4.0      True  06 28, 2015  A1XJ7XUR2W0J8V  B00RKZKFUI   \n",
       "5970       5.0     False  08 13, 2018   A6OUFMC6D07OF  B0144NYGJY   \n",
       "\n",
       "                                       style      reviewerName  \\\n",
       "11499                                    NaN        E. HOFFMAN   \n",
       "6475                                     NaN     Jason P. Gold   \n",
       "13167                                    NaN    Wallace Pierce   \n",
       "862                                      NaN  Jerry De La Cruz   \n",
       "5970   {'Platform:': ' Key Card [12 month]'}             Barry   \n",
       "\n",
       "                                                                                                                                                                      reviewText  \\\n",
       "11499  STOP RIGHT THERE ! This is a real review that is not motivated by deception.\\nThe simple four (4) rules that YNAB software helps you follow is the combination that ha...   \n",
       "6475   Please see [...] for a comparison of the features in pre-ribbon versions of Word to WordPerfect.  It is enlightening!\\n\\nIt is hard to believe it is two years since C...   \n",
       "13167  Just did not fit my needs.  My computer crashed & I had to replace my money program, so I ordered the 2007 version thinking it might be an improvement.  It was an imp...   \n",
       "862    Everything this little program does can be done yourself with some programming in your word processor.  But why bother; this thing does it simply and intuitively.  Th...   \n",
       "5970                                                                                                                                              worked and useful as described   \n",
       "\n",
       "                                                                                  summary  \\\n",
       "11499                                          YNAB - WHY IT CHANGED MY LIFE - APRIL 2009   \n",
       "6475                                                             The Better Wordprocessor   \n",
       "13167                                        My opinion of the Money 2007 Deluxe version.   \n",
       "862    The optional little graphics are a nice touch. Make sure you use a blank sheet ...   \n",
       "5970                                                       worked and useful as described   \n",
       "\n",
       "       unixReviewTime  vote image  \n",
       "11499      1239148800   2.0   NaN  \n",
       "6475       1220486400  12.0   NaN  \n",
       "13167      1380672000   NaN   NaN  \n",
       "862        1435449600   NaN   NaN  \n",
       "5970       1534118400   NaN   NaN  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargamos el archivo de reseñas:\n",
    "df = pd.read_csv('Amazon_software.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print('Tamaño del dataset', df.shape)\n",
    "\n",
    "df.sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VVM24VzSQFWJ",
   "metadata": {
    "id": "VVM24VzSQFWJ"
   },
   "source": [
    "Para realizar la práctica, sólo necesitaremos los textos de las reseñas y las calificaciones. Por tanto, eliminaremos las columnas innecesarias y nos quedaremos solo con las columnas <b>overall</b> y <b>reviewText</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99646f8f-4bfb-4c5b-942e-48b550ee5aeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "99646f8f-4bfb-4c5b-942e-48b550ee5aeb",
    "outputId": "02e35a49-b018-48fa-8414-4de312ab7aa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>5.0</td>\n",
       "      <td>STOP RIGHT THERE ! This is a real review that is not motivated by deception.\\nThe simple four (4) rules that YNAB software helps you follow is the combination that ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Please see [...] for a comparison of the features in pre-ribbon versions of Word to WordPerfect.  It is enlightening!\\n\\nIt is hard to believe it is two years since C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13167</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Just did not fit my needs.  My computer crashed &amp; I had to replace my money program, so I ordered the 2007 version thinking it might be an improvement.  It was an imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Everything this little program does can be done yourself with some programming in your word processor.  But why bother; this thing does it simply and intuitively.  Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>5.0</td>\n",
       "      <td>worked and useful as described</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall  \\\n",
       "11499      5.0   \n",
       "6475       5.0   \n",
       "13167      2.0   \n",
       "862        4.0   \n",
       "5970       5.0   \n",
       "\n",
       "                                                                                                                                                                      reviewText  \n",
       "11499  STOP RIGHT THERE ! This is a real review that is not motivated by deception.\\nThe simple four (4) rules that YNAB software helps you follow is the combination that ha...  \n",
       "6475   Please see [...] for a comparison of the features in pre-ribbon versions of Word to WordPerfect.  It is enlightening!\\n\\nIt is hard to believe it is two years since C...  \n",
       "13167  Just did not fit my needs.  My computer crashed & I had to replace my money program, so I ordered the 2007 version thinking it might be an improvement.  It was an imp...  \n",
       "862    Everything this little program does can be done yourself with some programming in your word processor.  But why bother; this thing does it simply and intuitively.  Th...  \n",
       "5970                                                                                                                                              worked and useful as described  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expandir la visualización de la columna de reseñas:\n",
    "pd.set_option('display.max_colwidth', 170)\n",
    "\n",
    "#Seleccionamos las columnas con las que trabajaremos:\n",
    "df = df[['overall', 'reviewText']] # calificación de la reseña y texto de la reseña.\n",
    "\n",
    "#Presentar 5 elementos de datos:\n",
    "df.sample(5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ecdf4211-88d4-4cf7-8cfc-e9f3593e8b2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "ecdf4211-88d4-4cf7-8cfc-e9f3593e8b2e",
    "outputId": "ccb2a944-ea99-43f0-9275-871fa7a133bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graficamos un histograma para tener una idea de la distribución de las reseñas numéricas:\n",
    "df.overall.hist(bins=[1.0, 2.0, 3.0, 4.0, 5.0], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864a2ae-4515-4e33-a0d1-18f02740b307",
   "metadata": {
    "id": "5864a2ae-4515-4e33-a0d1-18f02740b307"
   },
   "source": [
    "Como observamos en el histograma, hay más calificaciones positivas (de 4 puntos o más), que negativas (de 1 y 2 puntos). En la siguiente tabla veamos cuál es la tasa de votaciones por cada nivel de rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1371761-c6c5-4bba-8057-93d6f3a0b222",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1371761-c6c5-4bba-8057-93d6f3a0b222",
    "outputId": "2ef3151d-8c56-4962-849f-94763c370444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall\n",
       "5.0    46.226667\n",
       "1.0    22.606667\n",
       "4.0    15.546667\n",
       "3.0     8.766667\n",
       "2.0     6.853333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribución de calificaciones por ubicación:\n",
    "df['overall'].value_counts(normalize=True) * 100 # valor porcentual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5f881-45d7-45e2-804a-ebb93ca8773c",
   "metadata": {
    "id": "16c5f881-45d7-45e2-804a-ebb93ca8773c"
   },
   "source": [
    "La tabla anterior indica que, aproximadamente 1 de cada 2 clientes de Amazon han asignado la máxima puntuación (5.0) a los productos de software o tecnología. Mientras que 1 de cada 4 clientes han asignado la peor calificación posible (1.0).\n",
    "\n",
    "Para la clasificación de textos, necesitaremos sólo las opiniones que tienen las calificaciones más altas y las más bajas, por ello, vamos a seleccionar las filas que cumplan con estas características.\n",
    "\n",
    "Las opiniones cuya calificación sea mayor que 3 serán etiquetadas con '1'; mientras que, etiquetaremos con '0' las opiniones que tengan una calificación menor a 3. Las etiquetas se asignan a la columna <b>sentiment</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16d52608",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16d52608",
    "outputId": "bef5532b-2466-4a52-ba0a-7b5be2e350ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÚMERO DE CALIFICACIONES POR CLASE:\n",
      " sentiment\n",
      "1    9266\n",
      "0    4419\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tamaño total del dataset (13685, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descartar reseñas neutrales (calificación = 3)\n",
    "cond = df['overall'] != 3\n",
    "df = df.loc[cond] # Filtrar\n",
    "\n",
    "# Creación de una nueva columna: sentiment\n",
    "df.loc[:,('sentiment')] = df['overall'].apply(lambda x : 1 if x > 3 else 0).copy()\n",
    "\n",
    "# Imprimir número de calificaciones por clase.\n",
    "print('NÚMERO DE CALIFICACIONES POR CLASE:\\n', df.sentiment.value_counts())\n",
    "print(\"\\nTamaño total del dataset\", df.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDoKtCxIbT5y",
   "metadata": {
    "id": "wDoKtCxIbT5y"
   },
   "source": [
    "Como observamos en el output anterior, luego de aplicar el filtro queddan 13685 instancias, de las cuales, aprox. el 32% de reseñas tienden a tener una connotación o sentimiento negativo, y el 68% son positivas.\n",
    "\n",
    "Para la clasificación de textos (Parte 4) vamos a trabajar con el dataset desbalanceado, pero, para la parte final de la práctica, haremos un ajuste para crear un modelo predictivo con una versión del dataset balanceado y veremos qué ocurre con las predicciones.\n",
    "\n",
    "Ahora continuemos con la exploración de las reseñas para ver qué otras tareas de preparación tenemos que realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8fad82c-97fc-4273-bf24-df02492018a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "c8fad82c-97fc-4273-bf24-df02492018a2",
    "outputId": "fa4d00ad-0d1c-4113-8688-9318d868639b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11725</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Just got to say the seller sold this to me at an amazing price.  I used the license serial number to renew my 3 PC's for a 1/3 of the cost of doing it online.  Woo Hoo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4532</th>\n",
       "      <td>4.0</td>\n",
       "      <td>The download of a)the Amazon required downloader and then b) the actual program in multiple steps went OK but for another $[...] bucks or so you can get the actual pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Was recommended by my graphic designer so I could view and make small changes without going over my graphic design budget!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6520</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Slow.  Get lots of \"flashing\" hourglass cursors when making entries.  Same as older products, but only have to by to keep download function.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Not to gd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall  \\\n",
       "11725      5.0   \n",
       "4532       4.0   \n",
       "6339       5.0   \n",
       "6520       2.0   \n",
       "628        1.0   \n",
       "\n",
       "                                                                                                                                                                      reviewText  \\\n",
       "11725   Just got to say the seller sold this to me at an amazing price.  I used the license serial number to renew my 3 PC's for a 1/3 of the cost of doing it online.  Woo Hoo!   \n",
       "4532   The download of a)the Amazon required downloader and then b) the actual program in multiple steps went OK but for another $[...] bucks or so you can get the actual pa...   \n",
       "6339                                                  Was recommended by my graphic designer so I could view and make small changes without going over my graphic design budget!   \n",
       "6520                                Slow.  Get lots of \"flashing\" hourglass cursors when making entries.  Same as older products, but only have to by to keep download function.   \n",
       "628                                                                                                                                                                    Not to gd   \n",
       "\n",
       "       sentiment  \n",
       "11725          1  \n",
       "4532           1  \n",
       "6339           1  \n",
       "6520           0  \n",
       "628            0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra de clalificaciones: (1, sentimiento positivo) y (0, sentimiento negativo)\n",
    "df.sample(5, random_state = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f1f58-ac1b-43bd-af43-3e992a51c909",
   "metadata": {
    "id": "903f1f58-ac1b-43bd-af43-3e992a51c909"
   },
   "source": [
    "Como observamos en el dataset creado, algunas reseñas son muy poco comunicativas (como \"Not to gd\"), por lo tanto, vamos a filtrar aquellas con menos de 5 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aHP4wM-zxFfU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHP4wM-zxFfU",
    "outputId": "c5ec0483-648f-45b9-efaf-8e66f9c988c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANTIDAD TOTAL DE REVIEWS ANTES DEL FILTRADO: 13685 \n",
      "\n",
      "CANTIDAD TOTAL DE REVIEWS DESPUÉS DEL FILTRADO: 12314\n",
      "NÚMERO DE CALIFICACIONES POR CLASE:\n",
      " sentiment\n",
      "1    8037\n",
      "0    4277\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tamaño total del dataset (12314, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CANTIDAD TOTAL DE REVIEWS ANTES DEL FILTRADO:\", df.shape[0], \"\\n\")\n",
    "\n",
    "def words_counter(text):\n",
    "    if type(text) != float:\n",
    "      return len(text.split()) # contar palabras de cada reseña\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "#Descartar reseñas muy cortas: inferior a 5 palabras:\n",
    "df = df[df['reviewText'].apply(words_counter) >= 5]\n",
    "print(\"CANTIDAD TOTAL DE REVIEWS DESPUÉS DEL FILTRADO:\", df.shape[0])\n",
    "\n",
    "\n",
    "# Imprimir número de calificaciones por clase.\n",
    "print('NÚMERO DE CALIFICACIONES POR CLASE:\\n', df.sentiment.value_counts())\n",
    "print(\"\\nTamaño total del dataset\", df.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zFYWBAr0XZua",
   "metadata": {
    "id": "zFYWBAr0XZua"
   },
   "source": [
    "Como observamos en el output anterior, de las 13.685 reseñas etiquetadas, luego del filtrado quedan 12.314 reseñas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gmUCKPB5xGjC",
   "metadata": {
    "id": "gmUCKPB5xGjC"
   },
   "source": [
    "## 1.2 Limpieza de texto:\n",
    "Antes de trabajar con los textos de las reseñas, hay que limpiarlos. Exploremos el tercer comentario para identificar qué tareas de limpieza necesitamos aplicar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9e67fb6-3534-4b36-9984-644fb806a154",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "a9e67fb6-3534-4b36-9984-644fb806a154",
    "outputId": "5627a080-8b9b-43e6-a2d1-6a57faaff534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ve used TurboTax since 1994.  This year they\\'ve clearly demonstrated their elitest attitude and how they really feel about their suckers, er customers.  I only do a couple stock trades a year and for that I\\'m forced to buy the Premier version ?!?!?  I decided to go with the <a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/Deluxe-package-from-H-R-Block/dp/B00PJPI6G6/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">Deluxe package from H&R Block</a> and it truly is Deluxe.  All the federal forms, 5 fed efiles (lets you prepare unlimited returns for family and friends), and state return on CD all for $25.  It imported last year\\'s TurboTax file quickly and accurately and worked great.  I\\'m switched for life.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[12][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f963f2c-a3d7-4427-b07b-59b171e096bb",
   "metadata": {
    "id": "7f963f2c-a3d7-4427-b07b-59b171e096bb"
   },
   "source": [
    "El ejemplo presentado, nos permite observar que necesitamos quitar algunos signos de puntuación y limpiar código HTML. Por ahora, dejaremos pendiente a estas tareas para realizarlo más adelante.\n",
    "\n",
    "Además, si continuamos con la exploración un del dataset, observamos que algunos textos contienen direcciones web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79c6f61f-2f43-48a9-a483-7bd7388b0a02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "79c6f61f-2f43-48a9-a483-7bd7388b0a02",
    "outputId": "2d4afe3f-7829-47f7-8f94-48107bdd4d4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amazon.com help me a lot with my collection in my diary of my listening to music'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Presentar la primera reseña que incluye una dirección web:\n",
    "df[df['reviewText'].str.contains('http', case=False, na=False)].iloc[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7299af-9745-4247-bdb6-e91843253d74",
   "metadata": {
    "id": "4d7299af-9745-4247-bdb6-e91843253d74"
   },
   "source": [
    "\n",
    "\n",
    "**Filtraje de direcciones web**:\n",
    "\n",
    "Agrega una columna al dataframe que se denomine <i>text</i> e implementa una función que a partir del contenido de <i>reviewText</i>, quite las direcciones web de los textos.\n",
    "\n",
    "\n",
    "<b>Salida esperada:</b> Enlista al menos una reseña que muestre que la función implementada funciona.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "287fc6e2-586b-44ed-84bd-cf338196cd1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "287fc6e2-586b-44ed-84bd-cf338196cd1a",
    "outputId": "87525cd3-0e7e-4c9d-9173-629253f6ed8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTES DEL FILTRADO:\n",
      " https://www.amazon.com help me a lot with my collection in my diary of my listening to music \n",
      "\n",
      "DESPUÉS DEL FILTRADO:\n",
      "  help me a lot with my collection in my diary of my listening to music \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Función para remover URLs usando expresiones regulares\n",
    "def remove_urls(text):\n",
    "    # Definición de una expresión regular para encontrar URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Reemplazar las URLs encontradas por un espacio vacío\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Crear una nueva columna 'text' aplicando la función para eliminar URLs\n",
    "df['text'] = df['reviewText'].apply(lambda x: remove_urls(x) if x is not None else x)\n",
    "\n",
    "# Ahora, para demostrar que la función ha funcionado, vamos a comparar el texto antes y después del filtrado en una instancia específica.\n",
    "# Para ello, seleccionaremos una reseña que sabemos que contiene una URL.\n",
    "\n",
    "# Ejemplo antes del filtrado\n",
    "print(\"ANTES DEL FILTRADO:\\n\", df.loc[df['reviewText'].str.contains('http', case=False, na=False)].iloc[0]['reviewText'], \"\\n\")\n",
    "\n",
    "# Ejemplo después del filtrado\n",
    "print(\"DESPUÉS DEL FILTRADO:\\n\", df.loc[df['reviewText'].str.contains('http', case=False, na=False)].iloc[0]['text'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tR7h0D2L7R1J",
   "metadata": {
    "id": "tR7h0D2L7R1J"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "En este **segundo paso de la limpieza de datos**, se construye una función que convierta el texto que contiene contracciones (una contracción es la forma abreviada de una palabra, como \"don't\") a su forma completa (como, \"do not\"). La expansión de las contracciones puede ser útil para mejorar la identificación de stopwords.\n",
    "<br>\n",
    "<br>\n",
    "<b>Salida esperada:</b> Enlista la siguiente reseña, evidenciando que las contracciones han sido extendidas. *Ejemplo de la salida esperada*:\n",
    "<br>\n",
    "<i>\"I do not really know why there were so many complaints about Windows 8. I got this in 2013 and had no problem since them. All you have to do is add on a free start button utility and it works pretty much like windows has always worked. Of course now that Windows 10 is out, maybe that is a moot point.\"</i>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fsJG5c6G6NjU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "fsJG5c6G6NjU",
    "outputId": "1cd07acf-5716-4cbf-d419-a89b45effcaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't really know why there were so many complaints about Windows 8. I got this in 2013 and had no problem since them. All you have to do is add on a free start button utility and it works pretty much like windows has always worked. Of course now that Windows 10 is out, maybe that's a moot point\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observemos el texto con contracciones (\"don't\", \"that's\"). Luego evidencia cómo éste cambia luego de la transformación:\n",
    "df.iloc[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "gwiwCHtE54bN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "gwiwCHtE54bN",
    "outputId": "26acf2dc-4637-4b1e-8425-1b89c8cc4c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not really know why there were so many complaints about Windows 8. I got this in 2013 and had not had any problem since then. All you have to do is add on a free start button utility and it works pretty much like windows has always worked. Of course now that Windows 10 is out, maybe that's a moot point.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Mapeo de contracciones a sus formas expandidas\n",
    "contractions_mapping = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "}\n",
    "\n",
    "# Función para expandir contracciones en el texto\n",
    "def expand_contractions(text, contractions_dict=contractions_mapping):\n",
    "    # Para cada contracción y su expansión en el diccionario\n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        # Usar expresiones regulares para reemplazar la contracción con su expansión\n",
    "        text = re.sub(r\"\\b{}\\b\".format(contraction), expansion, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Simulando un ejemplo de texto con contracciones\n",
    "example_text = \"I don't really know why there were so many complaints about Windows 8. I got this in 2013 and hadn't had any problem since then. All you have to do is add on a free start button utility and it works pretty much like windows has always worked. Of course now that Windows 10 is out, maybe that's a moot point.\"\n",
    "\n",
    "# Aplicando la función al ejemplo de texto\n",
    "expanded_text = expand_contractions(example_text)\n",
    "\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BDMUKkFFPhmy",
   "metadata": {
    "id": "BDMUKkFFPhmy"
   },
   "source": [
    "Como ya lo comprobamos, otra tarea de limpieza que el dataset necesita es la limpieza de código HTML de los textos. Veamos el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sgl_-fKiP2Xq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "sgl_-fKiP2Xq",
    "outputId": "4e360bbd-8c14-4357-e684-84d8e8476d09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was FRAUD.  CD already registered per intuit<a data-hook=\"product-link-linked\" class=\"a-link-normal\" href=\"/QuickBooks-Pro-2009-OLD-VERSION/dp/B001ECGT8A/ref=cm_cr_arp_d_rvw_txt?ie=UTF8\">QuickBooks Pro 2009 [OLD VERSION</a>]'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[140][3] #A7D4E7F9EKSI9\n",
    "\n",
    "#Presentar la tercera reseña que incluye una dirección web, luego de la limpieza:\n",
    "df[df['text'].str.contains('This was FRAUD.', case=False, na=False)].iloc[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XAUTu9VdUGCz",
   "metadata": {
    "id": "XAUTu9VdUGCz"
   },
   "source": [
    "Como observamos en el ejemplo, el texto de la reseña contiene la etiqueta de enlace. A través de la siguiente función vamos a quitar cualquier fragmento de código HTML de las reseñas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "vU9VwGbtP0j-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "vU9VwGbtP0j-",
    "outputId": "15a71ac5-f69a-4890-f98b-642e335def61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p5/jp6dx55n0tl16f0fwb0jj6t00000gn/T/ipykernel_5380/160081010.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This was FRAUD.  CD already registered per intuitQuickBooks Pro 2009 [OLD VERSION]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_links_remove(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "# Aplicar la función.\n",
    "df['text']=df['text'].apply(lambda x:html_links_remove(x))\n",
    "\n",
    "#Presentar la tercera reseña que tenía una dirección web. Luego de la limpieza queda:\n",
    "df[df['text'].str.contains('This was FRAUD.', case=False, na=False)].iloc[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72067352-5141-4e85-9e06-13bcf9e05cd8",
   "metadata": {
    "id": "72067352-5141-4e85-9e06-13bcf9e05cd8"
   },
   "source": [
    "Ahora completaremos una tarea que la dejamos pendiente: quitar ciertos signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b890c968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "b890c968",
    "outputId": "558bcb15-7cd9-41bd-8199-34b246dbd54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've used TurboTax since 1994.  This year they've clearly demonstrated their elitest attitude and how they really feel about their suckers  er customers.  I only do a couple stock trades a year and for that I'm forced to buy the Premier version        I decided to go with the Deluxe package from H R Block and it truly is Deluxe.  All the federal forms  5 fed efiles  lets you prepare unlimited returns for family and friends   and state return on CD all for  25.  It imported last year's TurboTax file quickly and accurately and worked great.  I'm switched for life.\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuation = ';,!\"#$%&\\()*+-<>@[\\\\]^_`{|}~?'\n",
    "\n",
    "# Función para reemplazar ciertos signos de puntuación:\n",
    "def clean_signs(text):\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), ' ', text)\n",
    "    return text\n",
    "\n",
    "# Aplicar la función a la columna 'Review_Text'\n",
    "df['text'] = df['text'].apply(clean_signs)\n",
    "\n",
    "df.iloc[12].text # Presentamos una reseña para ver cómo va quedando el texto preprocesado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19a62b-8815-4e5f-97f2-be94612663e8",
   "metadata": {
    "id": "8d19a62b-8815-4e5f-97f2-be94612663e8"
   },
   "source": [
    "Finalmente, observamos en el ejemplo que hay espacios doble, así que vamos a crear una función para eliminarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4e0e064-42c7-4836-8836-c28f31463aa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "f4e0e064-42c7-4836-8836-c28f31463aa4",
    "outputId": "6e18d12c-f76b-4369-b62f-42872d3c6993"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've used TurboTax since 1994. This year they've clearly demonstrated their elitest attitude and how they really feel about their suckers er customers. I only do a couple stock trades a year and for that I'm forced to buy the Premier version I decided to go with the Deluxe package from H R Block and it truly is Deluxe. All the federal forms 5 fed efiles lets you prepare unlimited returns for family and friends and state return on CD all for 25. It imported last year's TurboTax file quickly and accurately and worked great. I'm switched for life.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class ExtraSpacesReplacer(object):\n",
    "    \"\"\" Replaces extra spaces in a text.\n",
    "    >>> replacer = ExtraSpacesReplacer()\n",
    "    \"\"\"\n",
    "\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "        return s.strip()\n",
    "\n",
    "spaces_replacer = ExtraSpacesReplacer()\n",
    "\n",
    "df['text'] = df['text'].apply(spaces_replacer.replace)\n",
    "\n",
    "df.iloc[12][3] # Reseña luego de la limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd14cd0",
   "metadata": {
    "id": "9dd14cd0"
   },
   "source": [
    "# 2. Obtención de datos a partir de información textual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b68be",
   "metadata": {
    "id": "312b68be"
   },
   "source": [
    "## 2.1 Encontrar colocaciones\n",
    "\n",
    "Recordemos que las colocaciones son términos multipalabra, es decir, secuencias de palabras que, en conjunto, tienen un significado que difiere significativamente del significado de cada palabra individual (e.g.\"Internet Security\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41ffd0",
   "metadata": {
    "id": "de41ffd0"
   },
   "source": [
    "\n",
    "Calculamos los mejores bigramas y trigramas de las opiniones .\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "lk582wJzf5UM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lk582wJzf5UM",
    "outputId": "38ebad0c-ba11-4c30-ae5a-f08a04d5141e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /Users/alex/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# Para este apartado es necesario cargar las siguientes librerías:\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6bfa1a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6bfa1a6",
    "outputId": "0ca2ab02-d3ac-4ed1-83f9-0cf2cd9e7f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could',\n",
       " 'cup',\n",
       " 'i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importar la lista de stopwords en inglés de la libreria NLTK y agregamos algunas adicionales:\n",
    "stopwords =  [\"could\", \"cup\"]#\n",
    "stopwords = stopwords + nltk.corpus.stopwords.words('english')\n",
    "# extracto de stopwords.\n",
    "[ x for x in stopwords[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d8030",
   "metadata": {
    "id": "f79d8030"
   },
   "source": [
    "A partir del comando help(nltk.collocations.BigramAssocMeasures) explora la clase BigramAssocMeasures del módulo nltk.metrics.association y revisa las definiciones de las métricas de Likelihood Ratio (likelihood_ratio) y de Pointwise Mutual Information (pmi) se explica en el capítulo 5 del libro Foundations of Statistical Natural Language Processing (Manning & Schutze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35defbdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35defbdd",
    "outputId": "6ea7f9c3-b50f-4e19-cdfa-28101fe6e2ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |  - n_ii counts ``(w1, w2)``, i.e. the bigram being scored\n",
      " |  - n_ix counts ``(w1, *)``\n",
      " |  - n_xi counts ``(*, w2)``\n",
      " |  - n_xx counts ``(*, *)``, i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f7371",
   "metadata": {
    "id": "837f7371"
   },
   "source": [
    "Para categorizar a los tokens por su tag POS, primero vamos a convertir el texto a minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "314750c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "314750c1",
    "outputId": "3d982944-89a3-4319-c732-98a2ed619052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we run a top of the line system utilizing windows 10 pro. i personally tried to get this to work. when you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset. i let it just s'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos texto en minúscula que recoja todas las reseñas:\n",
    "\n",
    "opinions = \" \".join(df['text']).lower()\n",
    "opinions[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jtmvp_5FB1xY",
   "metadata": {
    "id": "Jtmvp_5FB1xY"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    \n",
    "**Primer paso**:\n",
    "<br>\n",
    "Obtenemos los tokens del texto de las reseñas. Etiqueta estos tokens por su PoS.\n",
    "<br>\n",
    "Utiliza los métodos *word_tokenize* para tokenizar el texto de las reseñas y  *pos_tag* para determinar la etiqueta de cada token.\n",
    "<br>\n",
    "\n",
    "<b>Salida esperada:</b> Imprime los diez primeros tokens, con su respectiva etiqueta:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c27f9b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c27f9b8",
    "outputId": "5859ed91-99f9-4b4e-cfb6-4c223ba65a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'PRP'), ('run', 'VBP'), ('a', 'DT'), ('top', 'NN'), ('of', 'IN'), ('the', 'DT'), ('line', 'NN'), ('system', 'NN'), ('utilizing', 'JJ'), ('windows', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar el texto\n",
    "tokens = word_tokenize(opinions)\n",
    "\n",
    "# Etiquetar los tokens con su PoS\n",
    "pos_tokens = pos_tag(tokens)\n",
    "\n",
    "# Imprimir los diez primeros tokens con su respectiva etiqueta PoS\n",
    "print(pos_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7287ecb",
   "metadata": {
    "id": "c7287ecb"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    \n",
    "**Segundo paso**: Calcula los 500 mejores bigramas y los 500 mejores trigramas a partir de los tokens etiquetados (e.g. [(we, PRP), ...]) del texto. Utiliza las métricas PMI y Likehood Ratio.\n",
    "\n",
    "\n",
    "<b>Condición</b>: De los 500 bigramas y trigramas, elige a los que no comienzan ni terminen con una stopword. Para el filtrado de stopwords considera:\n",
    "- La lista previamente cargada (desde el paquete NLTK), y\n",
    "- Las categorías POS que representan a palabras vacías como determinantes, preposiciones, entre otras.\n",
    "\n",
    "<br>\n",
    "<b>Salida esperada:</b> Imprime los primeros 20 n-grams obtenidos con cada métrica.\n",
    "</div>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fe029",
   "metadata": {
    "id": "de2fe029"
   },
   "source": [
    "Recordemos la clasificación de etiquetas PoS.\n",
    "\n",
    "<b>Etiquetas PoS</b>\n",
    "\n",
    "<ul>\n",
    "<li>DT: Determinante</li>\n",
    "<li>JJ: Adjetivo</li>\n",
    "<li>NN: Nombre en singular</li>\n",
    "<li>NNS: Nombre en plural</li>\n",
    "<li>VBD: Verbo en pasado</li>\n",
    "<li>VBG: Verbo en gerundio</li>\n",
    "<li>MD: Verbo modal</li>\n",
    "<li>IN: Preposición o conjunción subordinada</li>\n",
    "<li>PRP: Pronombre</li>\n",
    "<li>RB: Adverbio</li>\n",
    "<li>RP: Partícula</li>    \n",
    "<li>CC: Conjunción coordinada</li>\n",
    "<li>CD: Numeral</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43dfad33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43dfad33",
    "outputId": "c0bb618e-e81a-4c73-81dc-e5ad69990a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 mejores bigramas con PMI:\n",
      "((\"'not\", 'NNP'), ('valid', 'NNP'))\n",
      "((\"'restart\", 'NNP'), ('app.scpt', 'NN'))\n",
      "((\"'w2/1099\", 'POS'), ('reporter', 'NN'))\n",
      "((\"'wedding\", 'VBG'), ('rehearsal', 'NN'))\n",
      "(('.o1', 'JJ'), ('entires', 'NNS'))\n",
      "(('/s3d', 'NNP'), ('stereoscopic', 'NN'))\n",
      "(('/watch', 'JJ'), ('v=vrtqyte94bo', 'NN'))\n",
      "((':0', 'NN'), ('definitley', 'NN'))\n",
      "((':invalidaterect', 'NN'), ('hwnd=null', 'NN'))\n",
      "(('==additional', 'JJ'), ('features==', 'NN'))\n",
      "(('==font', 'NNP'), ('rendering==', 'NN'))\n",
      "(('==new', 'VB'), ('skins==', 'JJ'))\n",
      "(('abrasive', 'JJ'), ('screeching', 'VBG'))\n",
      "(('absurdy', 'JJ'), ('inintuitive', 'JJ'))\n",
      "(('ac3', 'JJ'), ('dts', 'NN'))\n",
      "(('accuratley', 'VB'), ('depict', 'NN'))\n",
      "(('actualizar', 'JJ'), ('mi', 'NN'))\n",
      "(('adams', 'NNS'), ('terry', 'VBP'))\n",
      "(('afs5sersg6rat5sgar', 'NN'), ('arad', 'NN'))\n",
      "(('ag', 'JJ'), ('createacard', 'JJ'))\n",
      "\n",
      "20 mejores bigramas con Likelihood Ratio:\n",
      "(('.', '.'), ('.', '.'))\n",
      "(('turbo', 'JJ'), ('tax', 'NN'))\n",
      "(('r', 'NN'), ('block', 'NN'))\n",
      "(('customer', 'NN'), ('service', 'NN'))\n",
      "(('hard', 'JJ'), ('drive', 'NN'))\n",
      "(('tech', 'JJ'), ('support', 'NN'))\n",
      "(('h', 'NN'), ('r', 'NN'))\n",
      "(('operating', 'NN'), ('system', 'NN'))\n",
      "(('year', 'NN'), ('old', 'JJ'))\n",
      "(('h', 'JJ'), ('r', 'NN'))\n",
      "(('last', 'JJ'), ('year', 'NN'))\n",
      "((\"'ve\", 'VBP'), ('used', 'VBN'))\n",
      "(('e', 'JJ'), ('mail', 'NN'))\n",
      "(('anti', 'JJ'), ('virus', 'NN'))\n",
      "(('many', 'JJ'), ('years', 'NNS'))\n",
      "(('credit', 'NN'), ('card', 'NN'))\n",
      "(('internet', 'JJ'), ('security', 'NN'))\n",
      "(('rosetta', 'NN'), ('stone', 'NN'))\n",
      "(('works', 'VBZ'), ('great', 'JJ'))\n",
      "(('windows', 'NNS'), ('xp', 'VBP'))\n",
      "\n",
      "20 mejores trigramas con PMI:\n",
      "(('ac3', 'JJ'), ('dts', 'NN'), ('passthrough', 'NN'))\n",
      "(('actualizar', 'JJ'), ('mi', 'NN'), ('archos', 'VBD'))\n",
      "(('adams', 'NNS'), ('terry', 'VBP'), ('pratchett', 'NN'))\n",
      "(('afs5sersg6rat5sgar', 'NN'), ('arad', 'NN'), ('ag', 'NN'))\n",
      "(('alienware', 'NN'), ('aurora', 'JJ'), ('alx', 'NN'))\n",
      "(('angelia', 'VB'), ('vernon', 'NN'), ('menchan', 'NN'))\n",
      "(('areeyah', 'JJ'), ('m.', 'NN'), ('castaneda', 'NN'))\n",
      "(('avec', 'NN'), ('votre', 'NN'), ('cours', 'NN'))\n",
      "(('barga', 'JJ'), ('lucca', 'NN'), ('pisa', 'NN'))\n",
      "(('be.re.sheet', 'NN'), ('ba.ra', 'IN'), ('elohim', 'JJ'))\n",
      "(('bien', 'JJ'), ('buena', 'NN'), ('presentacion', 'NN'))\n",
      "(('bonus.its', 'VBZ'), ('bogus', 'IN'), ('claim.this', 'NN'))\n",
      "(('camus', 'NN'), ('flume', 'JJ'), ('flafka', 'NN'))\n",
      "(('caymon', 'NN'), ('bahamas', 'NN'), ('jamica', 'NN'))\n",
      "(('chk', 'NN'), ('bk', 'NN'), ('ymail.com', 'NNP'))\n",
      "(('coleco', 'JJ'), ('mattel', 'NN'), ('activision', 'NN'))\n",
      "(('computadoras', 'NNS'), ('que', 'VBP'), ('tengo', 'JJ'))\n",
      "(('coodrr', 'NN'), ('danial', 'JJ'), ('lanois', 'NN'))\n",
      "(('coral', 'NN'), ('feather', 'NN'), ('dusters', 'NNS'))\n",
      "(('coutez', 'NN'), ('et', 'NN'), ('rptez', 'NN'))\n",
      "\n",
      "20 mejores trigramas con Likelihood Ratio:\n",
      "(('.', '.'), ('i', 'NN'), (\"'ve\", 'VBP'))\n",
      "(('.', '.'), ('i', 'NN'), (\"'m\", 'VBP'))\n",
      "(('.', '.'), ('i', 'NN'), ('bought', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('purchased', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('use', 'VBP'))\n",
      "(('.', '.'), ('i', 'NN'), ('found', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('tried', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('got', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('needed', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('used', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('love', 'VBP'))\n",
      "(('easy', 'JJ'), ('to', 'TO'), ('use', 'VB'))\n",
      "(('.', '.'), ('i', 'NN'), ('think', 'VBP'))\n",
      "(('.', '.'), ('i', 'NN'), ('wanted', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('know', 'VBP'))\n",
      "(('.', '.'), ('i', 'NN'), ('decided', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('installed', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('upgraded', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('started', 'VBD'))\n",
      "(('.', '.'), ('i', 'NN'), ('downloaded', 'VBD'))\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "\n",
    "#Cargamos las métricas para el cálculo de bigramas y trigramas:\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Función para determinar si un n-grama es válido según las condiciones especificadas\n",
    "def valid_ngram(ngram):\n",
    "    # Lista de etiquetas PoS que representan palabras vacías o funcionales\n",
    "    bad_tags = {'DT', 'IN', 'PRP$', 'RB', 'PRP', 'MD', 'WP', 'WRB', 'WDT', 'EX', 'PDT', 'RP', 'CC', 'UH', 'CD'}\n",
    "    # Verifica si el primer y último token del n-grama son válidos\n",
    "    first_word, last_word = ngram[0], ngram[-1]\n",
    "    return (first_word[0].lower() not in stopwords and \n",
    "            last_word[0].lower() not in stopwords) and (first_word[1] not in bad_tags and last_word[1] not in bad_tags)\n",
    "\n",
    "# Crear buscadores de collocations para bigramas y trigramas\n",
    "bigram_finder = BigramCollocationFinder.from_words(pos_tokens)\n",
    "trigram_finder = TrigramCollocationFinder.from_words(pos_tokens)\n",
    "\n",
    "# Filtrar bigramas y trigramas que no cumplen con las condiciones\n",
    "bigram_finder.apply_ngram_filter(lambda w1, w2: not valid_ngram((w1, w2)))\n",
    "trigram_finder.apply_ngram_filter(lambda w1, w2, w3: not valid_ngram((w1, w2, w3)))\n",
    "\n",
    "# Calcular y mostrar los 500 mejores bigramas y trigramas con ambas métricas\n",
    "best_500_bigrams_pmi = bigram_finder.nbest(BigramAssocMeasures.pmi, 500)\n",
    "best_500_bigrams_lr = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 500)\n",
    "best_500_trigrams_pmi = trigram_finder.nbest(TrigramAssocMeasures.pmi, 500)\n",
    "best_500_trigrams_lr = trigram_finder.nbest(TrigramAssocMeasures.likelihood_ratio, 500)\n",
    "\n",
    "# Imprimir los primeros 20 n-grams de cada lista\n",
    "print(\"20 mejores bigramas con PMI:\")\n",
    "for bigram in best_500_bigrams_pmi[:20]:\n",
    "    print(bigram)\n",
    "    \n",
    "print(\"\\n20 mejores bigramas con Likelihood Ratio:\")\n",
    "for bigram in best_500_bigrams_lr[:20]:\n",
    "    print(bigram)\n",
    "\n",
    "print(\"\\n20 mejores trigramas con PMI:\")\n",
    "for trigram in best_500_trigrams_pmi[:20]:\n",
    "    print(trigram)\n",
    "    \n",
    "print(\"\\n20 mejores trigramas con Likelihood Ratio:\")\n",
    "for trigram in best_500_trigrams_lr[:20]:\n",
    "    print(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e230beb-392e-45a6-a4b0-e57c2941f5c3",
   "metadata": {
    "id": "0e230beb-392e-45a6-a4b0-e57c2941f5c3"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "**Análisis**: Observa las salidas que genera cada métrica (PMI vs. Likehood Ratio) y explica por qué se generan las diferencias y cuál métrica genera los mejores n-gramas desde el punto de vista del dominio.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873069d0",
   "metadata": {},
   "source": [
    "\n",
    "En el contexto \"opiniones realizadas por los clientes a diferentes productos de software disponibles en Amazon\", el análisis de las salidas de cada métrica revela diferencias significativas en los tipos de n-gramas que cada una. La elección de la mejor métrica depende del objetivo del análisis. \n",
    "\n",
    "\n",
    "**Análisis de los N-gramas con PMI**  \n",
    "Los n-gramas con PMI altos son valiosos para detectar discusiones detalladas o técnicas. Sin embargo, pueden no ser tan útiles para capturar la esencia general de las reseñas o para identificar temas ampliamente relevantes para la mayoría de los usuarios. Por ejemplo:  \n",
    "* `(('ac3', 'JJ'), ('dts', 'NN'), ('passthrough', 'NN'))`: Este trigram es muy específico y probablemente relacionado con términos técnicos de audio. La especificidad indica que 'ac3' y 'dts passthrough' son conceptos técnicos probablemente no comunes fuera de contextos especializados.  \n",
    "\n",
    "* `(('actualizar', 'JJ'), ('mi', 'NN'), ('archos', 'VBD'))`: Aquí se combina un verbo en español ('actualizar') con un nombre propio ('archos'), lo que sugiere una discusión técnica sobre un producto específico, posiblemente en un contexto de soporte o actualización de software.\n",
    "\n",
    "\n",
    "**Análisis de los N-gramas con Likelihood Ratio**  \n",
    "Los n-gramas generados tienden a ser más generales y reflejar experiencias o acciones comunes mencionadas en las reseñas. En este contexto quizás tiende a ofrecer una visión más clara de las experiencias y opiniones compartidas por los usuarios. Por ejemplo:  \n",
    "\n",
    "* `(('.', '.'), ('i', 'NN'), (\"'ve\", 'VBP'))`: Estos trigramas reflejan acciones comunes en las reseñas, como haber comprado algo. Aunque la presencia de puntos (.) puede parecer menos informativa, el enfoque en acciones comunes como 'bought' (comprado) o 'used' (usado) refleja patrones de lenguaje comunes en las reseñas.  \n",
    "\n",
    "* `(('easy', 'JJ'), ('to', 'TO'), ('use', 'VB'))`: Este trigram claramente captura una opinión común en las reseñas, destacando la facilidad de uso de un producto, un tema muy relevante para potenciales consumidores.  \n",
    "\n",
    "La diferencia clave entre las métricas, entonces, radica en su enfoque: PMI destaca lo específico y potencialmente único, mientras que Likelihood Ratio tiende a enfocarse en lo común y ampliamente relevante. Dependiendo del objetivo de análisis (identificar temas generales vs. explorar detalles técnicos o específicos), elegiríamos la métrica que mejor se alinee con nuestro objetivos.\n",
    "\n",
    "---\n",
    "\n",
    " <br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883d1ca",
   "metadata": {
    "id": "3883d1ca"
   },
   "source": [
    "Ahora detectamos n-gramas que cumplen el patrón sintáctico de un sintagma nominal (e.g: adjetivo + nombre en singular/plural, nombre + nombre y nombre en singular/plural). Las palabras componentes de cada n-grama deben estar separdaas por un guión \"-\". \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<b>Salida esperada:</b> Lista de los 20 primeros n-gramas que cumplan el patrón sintáctico, como 'line_system' y 'minutes'.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50c96d2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50c96d2e",
    "outputId": "ffd9e039-7dbb-4eb3-b94f-a8532b415e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los primeros 20 n-gramas que cumplen con el patrón sintáctico son:\n",
      ".o1-entires\n",
      "/watch-v=vrtqyte94bo\n",
      ":0-definitley\n",
      ":invalidaterect-hwnd=null\n",
      "==additional-features==\n",
      "ac3-dts\n",
      "actualizar-mi\n",
      "afs5sersg6rat5sgar-arad\n",
      "alaska-homesteader\n",
      "alex-armani\n",
      "alibre-deltacad\n",
      "alice-springs\n",
      "alif-baa\n",
      "alto-saxophone\n",
      "ami-officewriter\n",
      "andriod-fanboy\n",
      "apagar-manualmente\n",
      "apis-improve\n",
      "arad-ag\n",
      "archicad-plotmaker\n"
     ]
    }
   ],
   "source": [
    "# Lista de n-gramas que cumplen con los patrones sintácticos deseados\n",
    "matching_ngrams = []\n",
    "\n",
    "# Patrones sintácticos deseados en términos de etiquetas PoS\n",
    "desired_patterns = [\n",
    "    ('JJ', 'NN'), ('JJ', 'NNS'),  # Adj + Nombre singular/plural\n",
    "    ('NN', 'NN'), ('NNS', 'NN'), ('NN', 'NNS'), ('NNS', 'NNS')  # Nombre + Nombre (combinaciones)\n",
    "]\n",
    "\n",
    "# Buscar n-gramas que coincidan\n",
    "for ngram in best_500_bigrams_pmi + best_500_bigrams_lr:\n",
    "    pos_pattern = tuple(tag for word, tag in ngram) # Convertir ngrama a formato de etiquetas PoS\n",
    "    if pos_pattern in desired_patterns:             # Verificar si el patrón coincide con alguno de los deseados\n",
    "        # Formatear ngrama para unir palabras con guión y agregar a la lista de coincidencias\n",
    "        formatted_ngram = '-'.join(word for word, tag in ngram)\n",
    "        matching_ngrams.append(formatted_ngram)\n",
    "\n",
    "\n",
    "# Eliminar duplicados manteniendo el orden\n",
    "from collections import OrderedDict\n",
    "matching_ngrams = list(OrderedDict.fromkeys(matching_ngrams))\n",
    "\n",
    "# Imprimir los primeros 20 n-gramas que cumplen con el patrón\n",
    "print(\"Los primeros 20 n-gramas que cumplen con el patrón sintáctico son:\")\n",
    "for ngram in matching_ngrams[:20]:\n",
    "    print(ngram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c0a10",
   "metadata": {
    "id": "535c0a10"
   },
   "source": [
    "**Detectamos colocaciones con un modelo de detección de frases, con el módulo Phraser de Gensim. Entrena el modelo con todas las opiniones.**\n",
    "\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Primer Paso: </i> Crear la lista de sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9964125a-792f-409b-ac9c-fdb846aad084",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9964125a-792f-409b-ac9c-fdb846aad084",
    "outputId": "c5e93853-c6e9-4645-8a3d-e1c22c84085c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We run a top of the line system utilizing Windows 10 Pro',\n",
       " 'I personally tried to get this to work',\n",
       " 'When you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset',\n",
       " \"I let it just sit once to see if it ever started and after about 20 minutes all these windows started popping up saying that the file required a version of Adobe that doesn't exist and that the wrong volume was inserted\",\n",
       " 'Now we are taking all our computers off line to do a complete virus scan just in case',\n",
       " 'Avoid',\n",
       " \"At the least it's a headache and a waste of time and money\",\n",
       " \"I don't really know why there were so many complaints about Windows 8\",\n",
       " 'I got this in 2013 and had no problem since them',\n",
       " 'All you have to do is add on a free start button utility and it works pretty much like windows has always worked']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la lista de sentences:\n",
    "opinions_string = \" \".join(df['text'])\n",
    "\n",
    "opinion_sentences = opinions_string.split('. ')\n",
    "\n",
    "opinion_sentences[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a766d9-8390-4766-83d4-1cbcb7cbd816",
   "metadata": {
    "id": "d9a766d9-8390-4766-83d4-1cbcb7cbd816"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Segundo paso</i>: Convierte las reseñas en una lista de *phrases*. \n",
    "* Las phrases no deben ser stopwords. \n",
    "* Tampoco deben empezar ni acabar con una stopword. \n",
    "* Utiliza la lista de stopwords para el filtrado.\n",
    "\n",
    "<br>\n",
    "<b> Salida esperada:</b> Lista de las 20 primeras *phrases* que no sean, o no contengan stopwords.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97f2bd0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97f2bd0b",
    "outputId": "c853eefa-ad17-4179-9525-cbb5cc5b278b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_line\n",
      "windows_pro\n",
      "personally_tried\n",
      "get_work\n",
      "click_application\n",
      "file_per\n",
      "entire_system\n",
      "see_ever\n",
      "started_popping\n",
      "file_required\n",
      "version_adobe\n",
      "virus_scan\n",
      "least_headache\n",
      "waste_time\n",
      "really_know\n",
      "many_complaints\n",
      "got_problem\n",
      "start_button\n",
      "works_pretty\n",
      "much_like\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.utils\n",
    "\n",
    "# Tokenizar cada oración y filtrar stopwords\n",
    "sentences_tokenized = [[word for word in gensim.utils.simple_preprocess(sentence) if word not in stopwords] \n",
    "                       for sentence in opinion_sentences]\n",
    "\n",
    "# Entrenar el modelo Phrases\n",
    "phrases = Phrases(sentences_tokenized, min_count=1, threshold=1)\n",
    "phraser = Phraser(phrases)\n",
    "\n",
    "# Generar las phrases a partir de las oraciones tokenizadas\n",
    "phrases_list = [phraser[sentence] for sentence in sentences_tokenized]\n",
    "\n",
    "# Filtrar phrases que no comiencen ni terminen con stopwords, y que no contengan stopwords\n",
    "opinion_phrases_no_stopwords = []\n",
    "for sentence in phrases_list:\n",
    "    for phrase in sentence:\n",
    "        if \"_\" in phrase:  # Verificar si la palabra es una phrase\n",
    "            words = phrase.split(\"_\")\n",
    "            if words[0] not in stopwords and words[-1] not in stopwords and all(word not in stopwords for word in words):\n",
    "                opinion_phrases_no_stopwords.append(phrase)\n",
    "\n",
    "# Imprimir las primeras 20 phrases que cumplen con los criterios\n",
    "for x in opinion_phrases_no_stopwords[:20]:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e13a71",
   "metadata": {
    "id": "d4e13a71"
   },
   "source": [
    "## 2.2 Vectorizar palabras y términos\n",
    "\n",
    "Exploraremos la vectorización de palabras y términos con el método Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391acd67",
   "metadata": {
    "id": "391acd67"
   },
   "source": [
    "Recordemos que el paquete gensim implementa un método para entrenar modelos Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4OGwpBv8fOtp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OGwpBv8fOtp",
    "outputId": "f9c5ddad-56c8-434c-b809-ef5b57babae4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top_line',\n",
       " 'windows_pro',\n",
       " 'personally_tried',\n",
       " 'get_work',\n",
       " 'click_application',\n",
       " 'file_per',\n",
       " 'entire_system',\n",
       " 'see_ever',\n",
       " 'started_popping',\n",
       " 'file_required']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# opinion_phrases_no_stopwords = filtered_phrases  # Esta sería la lista obtenida previamente\n",
    "\n",
    "# Quitar espacios del texto:\n",
    "opinion_phrases_stripped_no_stopwords = [c.strip() for c in opinion_phrases_no_stopwords]\n",
    "opinion_phrases_stripped_no_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaa498",
   "metadata": {
    "id": "20aaa498"
   },
   "source": [
    "\n",
    "**Obtenemos targets de las opiniones y sus aspectos utilizando el modelo word2vec.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d68931",
   "metadata": {
    "id": "a9d68931"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Primer paso</i>: Convertir las phrases de cada oración en un token. Lo haremos concatenando los tokens de la phrase con el caracter '_' \n",
    "* (e.g: 'disneyland hongkong' -> 'disneyland_hongkong'). \n",
    "  Entonces, en cada oración sustituimos los bigramas que son phrases por la forma tokenizada \n",
    "* (e.g: we've been to disneyland hongkong -> we've been to disneyland_hongkong). \n",
    "\n",
    "De esta forma, las colocaciones formarán parte del vocabulario del modelo word2vec que generaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dee08237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "dee08237",
    "outputId": "60e8e199-99bc-4c22-93ad-4673626e34e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We run a top of the line system utilizing Windows 10 Pro',\n",
       " 'I personally tried to get this to work',\n",
       " 'When you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "collocation_phrases = [phrase for phrase in list(set(opinion_phrases_stripped_no_stopwords)) if ' ' in phrase]\n",
    "\n",
    "def transform_sentence(sentence):\n",
    "    transformed_sentence = sentence\n",
    "    n_grams = list(ngrams(nltk.word_tokenize(sentence), 2))\n",
    "    ngrams_t = [' '.join(gram) for gram in n_grams]\n",
    "    for ngram in ngrams_t:\n",
    "        if ngram in collocation_phrases:\n",
    "            opt = ngram.replace(' ', '_')\n",
    "            transformed_sentence = transformed_sentence.replace(ngram,opt)\n",
    "    return transformed_sentence\n",
    "\n",
    "opinion_sentences_transformed = [transform_sentence(os) for os in opinion_sentences]\n",
    "\n",
    "opinion_sentences_transformed[:3][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53930c4",
   "metadata": {
    "id": "b53930c4"
   },
   "source": [
    "\n",
    "\n",
    "<i>Segundo paso</i>: Crea una sentence stream donde todos los tokens de las oraciones están lematizados. \n",
    "* Los tokens no pueden ser stopwords ni tener un stopword al inicio o al final. \n",
    "* Para simplificar la tarea, consideramos que el lema de una colocación no cambia y su PoS es 'col'. \n",
    "* (e.g: ['We run a top of the line system utilizing Windows 10 Pro']  -> [run', 'top', 'line', 'system', 'utilize', 'window', 'pro]).\n",
    "\n",
    "<br>\n",
    "<b> Salida esperada:</b> Lista de los 10 primeros tokens lematizados (que no sean, ni contengan stopwords).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cfcef92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cfcef92",
    "outputId": "32ca8612-8b3c-43d3-9650-54efcf5f0277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complete', 'When', 'computers', 'works', 'tried', 'like', 'really', 'reset', 'volume', 'sit', 'system', 'see', 'click', 'always', 'exist', 'money', 'work', 'I', 'application', 'much', 'Windows', 'required', 'since', 'started', '20', 'file', 'windows', 'Avoid', 'back', 'scan', 'Pro', 'saying', 'utilizing', 'complaints', 'got', 'Adobe', 'headache', 'top', 'All', 'problem', 'worked', 'get', 'free', 'pretty', 'personally', 'popping', 'least', \"n't\", 'many', 'start', 'know', 'version', 'time', 'button', 'virus', 'At', '2013', 'minutes', 'taking', \"'s\", '8', 'per', 'add', 'entire', 'ever', 'case', 'wrong', 'bog', 'directions', 'inserted', 'waste', 'We', 'Now', 'let', 'line', 'would', 'run', '10', 'utility']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Simplificando la lematización, en este caso no la modificamos realmente\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Asegurar que el token no sea una stopword\n",
    "        if token not in stopwords:\n",
    "            lemmatized_tokens.append(token)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Crear sentence stream lematizado\n",
    "lemmatized_sentence_stream = [lemmatize_tokens(nltk.word_tokenize(sentence)) for sentence in opinion_sentences_transformed]\n",
    "\n",
    "# Imprimir los primeros 10 tokens lematizados que no sean ni contengan stopwords\n",
    "# Para simplificar, mostraremos tokens únicos de las primeras oraciones\n",
    "unique_lemmatized_tokens = list(set([token for sentence in lemmatized_sentence_stream[:10] for token in sentence]))\n",
    "print(unique_lemmatized_tokens[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b807880",
   "metadata": {
    "id": "7b807880"
   },
   "source": [
    "\n",
    "\n",
    "<i>Tercer paso</i>: Crea un modelo word2vec de las opiniones lematizadas. El modelo debe llamarse w2v_opinions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b033b7ad",
   "metadata": {
    "id": "b033b7ad"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# lemmatized_sentence_stream es un stream de oraciones lematizadas preparado anteriormente\n",
    "\n",
    "# Crear y entrenar el modelo Word2Vec\n",
    "w2v_opinions = Word2Vec(sentences=lemmatized_sentence_stream, \n",
    "                        vector_size=100, \n",
    "                        window=5,\n",
    "                        min_count=1, \n",
    "                        workers=4)\n",
    "\n",
    "# El modelo ahora está entrenado con las opiniones lematizadas y listo para ser utilizado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e07b3c",
   "metadata": {
    "id": "00e07b3c"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Cuarto paso</i>: A partir del vocabulario del modelo word2vec, selecciona posibles aspectos de la reseña (e.g: desktop) y lista los términos semánticamente relacionados con estos aspectos según este modelo.\n",
    "<br>\n",
    "\n",
    "<b>Salida esperada:</b> Lista los primeros 20 términos que tengan mayor relación semántica con el término seleccionado (i.e. \"desktop\").\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d10af9ee",
   "metadata": {
    "id": "d10af9ee"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario:\n",
    "vocabulary = list(w2v_opinions.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "568d1445",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "568d1445",
    "outputId": "7ea0eb29-95ca-4f54-fb33-7748e7d92105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine: 0.9707465171813965\n",
      "systems: 0.9433795213699341\n",
      "Lion: 0.9425483345985413\n",
      "virtual: 0.9420837163925171\n",
      "Mountain: 0.9395446181297302\n",
      "drivers: 0.9388763904571533\n",
      "x64: 0.936906099319458\n",
      "NT4: 0.9344995617866516\n",
      "64bit: 0.933407187461853\n",
      "Compaq: 0.9327118992805481\n",
      "dual: 0.932357132434845\n",
      "Window: 0.9318041205406189\n",
      "hardware: 0.9309105277061462\n",
      "OSX: 0.9303945302963257\n",
      "Desktop: 0.9298335909843445\n",
      "64: 0.9295209050178528\n",
      "printer: 0.9264531135559082\n",
      "Toshiba: 0.9258053302764893\n",
      "Dell: 0.9248976111412048\n",
      "10.5: 0.9242185354232788\n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que \"desktop\" está en el vocabulario antes de buscar términos relacionados\n",
    "if \"desktop\" in vocabulary:\n",
    "    # Encontrar los primeros 20 términos más relacionados con \"desktop\"\n",
    "    related_terms = w2v_opinions.wv.most_similar(\"desktop\", topn=20)\n",
    "\n",
    "    # Imprimir los términos relacionados\n",
    "    for term, similarity in related_terms:\n",
    "        print(f\"{term}: {similarity}\")\n",
    "else:\n",
    "    print(\"El término 'desktop' no se encuentra en el vocabulario.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5613ad3-1d17-4a73-8bd3-b71b76db1f8b",
   "metadata": {
    "id": "a5613ad3-1d17-4a73-8bd3-b71b76db1f8b"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<b>Análisis</b>: De los 20 términos seleccionados, elije los 5 ejemplos que consideres los más pertinentes. Menciona al menos un criterio que tomaste en cuenta para la selección.\n",
    "<br>\n",
    "<b>Salida esperada:</b>\n",
    "- Lista de al menos 5 de los términos (aspectos) que tengan mayor relación semántica con el término seleccionado (e.g. \"desktop\").\n",
    "- Argumento o criterio para realizar la selección de términos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8b783",
   "metadata": {},
   "source": [
    "Estos términos fueron elegidos porque representan componentes, sistemas operativos, o categorías de productos que frecuentemente se discuten o se asocian con computadoras de escritorio (desktops).\n",
    "\n",
    "Términos Seleccionados:\n",
    "\n",
    "* VMs (Virtual Machines): Representan un concepto relevante para discusiones sobre desarrollo de software, pruebas, y uso de múltiples sistemas operativos en una sola máquina física.\n",
    "* Compaq: Una marca específica que históricamente está asociada con computadoras de escritorio, reflejando la influencia de marcas específicas en la percepción de calidad o preferencia de los usuarios.\n",
    "* drivers: Esenciales para el funcionamiento óptimo de hardware en sistemas operativos.\n",
    "* OSX: Sistema operativo de Mac, se utiliza en computadoras de escritorio de Apple, reflejando discusiones específicas de plataforma en el ámbito tech.\n",
    "* hardware: Un término general que abarca todos los componentes físicos de una computadora, siendo central para cualquier discusión sobre computadoras de escritorio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "764f6cdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "764f6cdb",
    "outputId": "f3982abf-9f7a-4bd1-abeb-4aed3f04e01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos relacionados con 'hardware':\n",
      "  - Win7: 0.9720826745033264\n",
      "  - platform: 0.9676741361618042\n",
      "  - Upgrade: 0.9659444689750671\n",
      "  - Snow: 0.9646590352058411\n",
      "  - OSX: 0.963978111743927\n",
      "\n",
      "Términos relacionados con 'VMs':\n",
      "  - x64: 0.9238964915275574\n",
      "  - Lion: 0.9236522316932678\n",
      "  - Win98: 0.9187003374099731\n",
      "  - Mavericks: 0.9175817966461182\n",
      "  - desktop: 0.9159078001976013\n",
      "\n",
      "Términos relacionados con 'Compaq':\n",
      "  - Toshiba: 0.9703934192657471\n",
      "  - 64bit: 0.9639914631843567\n",
      "  - Dell: 0.9554469585418701\n",
      "  - SP1: 0.9541727900505066\n",
      "  - Desktop: 0.9527331590652466\n",
      "\n",
      "Términos relacionados con 'drivers':\n",
      "  - printer: 0.9734398722648621\n",
      "  - virtual: 0.9661089181900024\n",
      "  - installer: 0.9647298455238342\n",
      "  - Win7: 0.9640608429908752\n",
      "  - compatibility: 0.9591827392578125\n",
      "\n",
      "Términos relacionados con 'OSX':\n",
      "  - platform: 0.9888524413108826\n",
      "  - Desktop: 0.9802053570747375\n",
      "  - Tiger: 0.9784677028656006\n",
      "  - El: 0.9769887924194336\n",
      "  - environment: 0.976859986782074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imprimimos ejemplos de aspectos cercanos semánticamente al target según el modelo Word2Vec. Elije los términos:\n",
    "terms_to_explore = [\"hardware\", \"VMs\", \"Compaq\", \"drivers\", \"OSX\"]\n",
    "\n",
    "for term in terms_to_explore:\n",
    "    print(f\"Términos relacionados con '{term}':\")\n",
    "    if term in w2v_opinions.wv.key_to_index:\n",
    "        related_terms = w2v_opinions.wv.most_similar(term, topn=5)\n",
    "        for related_term, similarity in related_terms:\n",
    "            print(f\"  - {related_term}: {similarity}\")\n",
    "    else:\n",
    "        print(\"  El término no se encuentra en el vocabulario del modelo.\")\n",
    "    print()  # Espacio entre términos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80090d4b",
   "metadata": {
    "id": "80090d4b"
   },
   "source": [
    "# 3. Detección de temas\n",
    "\n",
    "En estos apartados exploraremos cúales son los tópicos tratados en las opiniones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb679a",
   "metadata": {
    "id": "00bb679a"
   },
   "source": [
    "## 3.1 Exploración de los temas con WordNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a2020",
   "metadata": {
    "id": "9e7a2020"
   },
   "source": [
    "En este apartado accederemos a Wordnet a través de la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "320fee0c",
   "metadata": {
    "id": "320fee0c"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd9034",
   "metadata": {
    "id": "7ccd9034"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Comprueba si, según Wordnet, existen aspectos que están alejados semánticamente del sentido del target, aunque en el modelo word2vec sean similares. Compruébalo calculando la similitud de Wu and Palmer entre el sentido de wordnet 'desktop.n.01' y algunos de sus aspectos.\n",
    "<br>\n",
    "\n",
    "<b> Salida esperada: </b>\n",
    "<br>\n",
    "- Lista de dos términos que según *Wordnet* no estén tan cercanos, y su respectivo score de similitud, y\n",
    "<br>\n",
    "- Lista de los mismos términos que según el modelo *word2vec* están más cercanos.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "30cfcb0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30cfcb0f",
    "outputId": "52515efa-9041-4d7b-996e-8a4678ff8741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud de Wu and Palmer en WordNet:\n",
      "hardware: 0.35294117647058826\n",
      "drivers: 0.3157894736842105\n",
      "\n",
      "Cercanía según el modelo Word2Vec:\n",
      "hardware: 0.9309104681015015\n",
      "drivers: 0.9388763308525085\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Cargar el synset para 'desktop' en WordNet\n",
    "desktop_synset = wn.synset('desktop.n.01')\n",
    "\n",
    "# Lista ajustada de términos para comparar en ambos WordNet y Word2Vec\n",
    "terms_to_compare = [\"hardware\", \"drivers\"]  # Asumimos \"operating_system\" para \"Operating\"\n",
    "\n",
    "# Diccionario para almacenar los resultados de similitud de WordNet\n",
    "similarity_scores_wn = {}\n",
    "\n",
    "# Diccionario para almacenar similitudes de Word2Vec\n",
    "similarity_scores_w2v = {}\n",
    "\n",
    "for term in terms_to_compare:\n",
    "    # WordNet\n",
    "    term_synsets = wn.synsets(term)\n",
    "    if term_synsets:\n",
    "        best_score = max([desktop_synset.wup_similarity(ts) for ts in term_synsets])\n",
    "        similarity_scores_wn[term] = best_score\n",
    "    else:\n",
    "        similarity_scores_wn[term] = None\n",
    "    \n",
    "    # Word2Vec\n",
    "    if term in w2v_opinions.wv.key_to_index:\n",
    "        similarity_scores_w2v[term] = w2v_opinions.wv.similarity('desktop', term)\n",
    "    else:\n",
    "        similarity_scores_w2v[term] = None\n",
    "\n",
    "# Imprimir los scores de similitud con WordNet\n",
    "print(\"Similitud de Wu and Palmer en WordNet:\")\n",
    "for term, score in similarity_scores_wn.items():\n",
    "    print(f\"{term}: {score}\")\n",
    "\n",
    "# Imprimir la cercanía según el modelo Word2Vec\n",
    "print(\"\\nCercanía según el modelo Word2Vec:\")\n",
    "for term, similarity in similarity_scores_w2v.items():\n",
    "    print(f\"{term}: {similarity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f84f3-a786-4d80-bfa1-479ec174733e",
   "metadata": {
    "id": "cc6f84f3-a786-4d80-bfa1-479ec174733e"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Análisis</i>: ¿Por qué ocurren diferencias a nivel de distancia semántica entre Wordnet y el modelo basado en Word2vec?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35e1a4",
   "metadata": {},
   "source": [
    "Las diferencias fundamentales en cómo cada uno modela y entiende las relaciones entre palabras. \n",
    "\n",
    "**Fundamento Teórico y Metodológico**:  \n",
    "- WordNet es un recurso léxico que organiza el lenguaje inglés en un conjunto de sinónimos interconectados, agrupados en conjuntos de sinónimos (synsets), cada uno expresando un concepto distinto. Las relaciones entre estos synsets siguen una estructura jerárquica y conceptual, basada en relaciones semánticas y léxicas como la hiponimia (específico a general), la hiperonimia (general a específico), y la meronimia (parte de). Estas relaciones están cuidadosamente curadas por expertos.\n",
    "  \n",
    "- Word2Vec es un modelo de aprendizaje automático que aprende representaciones vectoriales de palabras a partir de grandes corpus de texto basado en el contexto en el que aparecen las palabras. Estos contextos son capturados a través del aprendizaje de vectores en un espacio dimensional donde las palabras que comparten contextos similares son mapeadas a puntos cercanos. Este enfoque refleja patrones estadísticos del uso del lenguaje, no necesariamente relaciones conceptuales o jerárquicas definidas explícitamente.\n",
    "\n",
    "**Fuente de Información**:  \n",
    "- WordNet se basa en la estructura semántica del lenguaje definida por lingüistas, reflejando una comprensión teórica y organizada del lenguaje. Esto puede hacer que sus relaciones semánticas sean más precisas desde un punto de vista lingüístico o conceptual, pero posiblemente menos representativas de la variabilidad y riqueza del uso del lenguaje en contextos reales y cotidianos.\n",
    "\n",
    "- Word2Vec aprende de la utilización real del lenguaje en extensos corpus textuales, capturando cómo las palabras se usan y se relacionan entre sí en práctica. Esto puede incluir asociaciones semánticas basadas en conocimiento del mundo, jerga, y otros patrones que no se capturan necesariamente en la estructura jerárquica de WordNet.\n",
    "\n",
    "**Sensibilidad al Contexto**:  \n",
    "- Word2Vec es particularmente efectivo en capturar relaciones basadas en el co-ocurrencia y patrones de proximidad en el uso del lenguaje, lo que puede revelar relaciones semánticas no evidentes solo por análisis estructural o conceptual. Esto incluye sinónimos, antónimos, y otras formas de asociación semántica emergente del uso contextual.\n",
    "  \n",
    "- WordNet, mientras ofrece una visión detallada de las relaciones entre conceptos y palabras, no captura necesariamente el dinamismo del lenguaje ni las sutilezas del significado que emergen del contexto específico de uso.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209e5cc",
   "metadata": {
    "id": "3209e5cc"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Identificamos los términos monopalabra del vocabulario de word2vec que no están en Wordnet. Filtra a los términos que sean nombres o adjetivos.\n",
    "\n",
    "Del conjunto de términos que identifiques, menciona a:\n",
    "- Abreviaturas o términos específicos del dominio o expresiones típicas de la jerga tech.<br>\n",
    "\n",
    "<b>Salida esperada:</b>\n",
    "<br>\n",
    "- Cantidad de términos que no constan en *Wordnet*.\n",
    "- Lista de los 20 primeros terminos monopalabra que no constan en <i>Wordnet</i>.\n",
    "- Lista de al menos 3 términos de los enlistados que sean inherentes al vocabulario tech.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70e8e4bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70e8e4bd",
    "outputId": "109340c3-be75-4e62-9a2e-1301ac9bc479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cantidad de términos que no constan en Wordnet: 9822\n",
      "*************\n",
      "20 primeros términos monopalabra que no constan en Wordnet:\n",
      "************\n",
      "n't\n",
      "'s\n",
      "The\n",
      "would\n",
      "This\n",
      ".\n",
      "'ve\n",
      ":\n",
      "If\n",
      "'m\n",
      "Norton\n",
      "Microsoft\n",
      "You\n",
      "..\n",
      "My\n",
      "since\n",
      "without\n",
      "...\n",
      "something\n",
      "everything\n",
      "*************\n",
      "Términos inherentes al vocabulario tech:\n",
      "************\n",
      "webroot\n",
      "netbook\n",
      "Logitech\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import gensim\n",
    "\n",
    "# Asumiendo que el modelo Word2Vec se llama 'w2v_opinions'\n",
    "vocabulary = list(w2v_opinions.wv.key_to_index.keys())\n",
    "\n",
    "# Lista para almacenar términos que son monopalabra y no están en WordNet\n",
    "terms_not_in_wordnet = []\n",
    "\n",
    "# Filtrar términos monopalabra que no están en WordNet\n",
    "for term in vocabulary:\n",
    "    if \"_\" not in term and wn.synsets(term) == []:\n",
    "        terms_not_in_wordnet.append(term)\n",
    "\n",
    "# Contar la cantidad de términos encontrados\n",
    "count_terms_not_in_wordnet = len(terms_not_in_wordnet)\n",
    "print(f\"\\nCantidad de términos que no constan en Wordnet: {count_terms_not_in_wordnet}\")\n",
    "\n",
    "# Listar los primeros 20 términos\n",
    "print(\"*************\\n20 primeros términos monopalabra que no constan en Wordnet:\\n************\")\n",
    "for term in terms_not_in_wordnet[:20]:\n",
    "    print(term)\n",
    "\n",
    "# Identificar términos del dominio tech\n",
    "tech_terms = [term for term in terms_not_in_wordnet if any(substring in term for substring in ['tech', 'sys', 'net', 'web'])]\n",
    "\n",
    "# Lista de al menos 3 términos inherentes al vocabulario tech\n",
    "print(\"*************\\nTérminos inherentes al vocabulario tech:\\n************\")\n",
    "for term in tech_terms[:3]:\n",
    "    print(term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9d3df-d541-4500-84bb-1e8d8101d2ef",
   "metadata": {
    "id": "89f9d3df-d541-4500-84bb-1e8d8101d2ef"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Análisis</i>: ¿Crees que estos términos son importantes para realizar un buen análisis de sentimientos?\n",
    "<br>\n",
    "\n",
    "<b>Salida esperada:</b> Argumentos a favor y/o en contra de incorporar los términos del vocabulario que no están definidos en *Wordnet*.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a408c8",
   "metadata": {},
   "source": [
    "Incorporar términos específicos del sector tecnológico, como \"netbook\" o \"Logitech\", en el análisis de sentimientos es beneficioso ya que aportan relevancia contextual al referirse directamente a productos o marcas específicas, lo cual es esencial para interpretar correctamente el sentimiento hacia estos. Además, la inclusión de jerga técnica y nombres propios enriquece el léxico del análisis, permitiendo captar matices y opiniones específicas que se perderían con un vocabulario más genérico. También facilitan la identificación de aspectos concretos de los productos o servicios evaluados, lo cual resulta vital en análisis de sentimientos basados en aspectos para comprender las opiniones sobre características específicas.\n",
    "\n",
    "Sin embargo, la inclusión de términos altamente especializados en análisis de sentimientos generales puede resultar contraproducente por varias razones. Primero, puede llevar a una falta de relevancia y complicaciones innecesarias en el modelo, especialmente si el análisis no está destinado a un dominio específico. Además, la interpretación de términos técnicos y nombres propios suele requerir conocimiento especializado, lo que puede no ser adecuado para aplicaciones dirigidas a un público general. Por último, la ambigüedad y variabilidad inherentes a los nombres de marca y la jerga técnica, junto con la aparición constante de nuevos términos, exigen actualizaciones frecuentes del léxico y complican la gestión eficaz del vocabulario.\n",
    "\n",
    "La decisión de incorporar términos del vocabulario que no están definidos en WordNet depende en gran medida del objetivo y del contexto específico del análisis de sentimientos. Para análisis centrados en el ámbito tecnológico o cuando es crucial capturar opiniones sobre productos o marcas específicas, la inclusión de estos términos puede ser beneficiosa. Pero , para análisis más generales o aplicaciones destinadas a un público más amplio, podría ser preferible limitar la incorporación de términos altamente especializados para mantener la claridad y la generalización del análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f788a4",
   "metadata": {
    "id": "f5f788a4"
   },
   "source": [
    "## 3.2 LDA\n",
    "\n",
    "Recuerda que en el notebook del módulo 1 hemos visto la aplicación del método LDA para extraer temas de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6825ff",
   "metadata": {
    "id": "1c6825ff"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<i>Primer paso</i>: Convertir las opiniones transformadas (opinion_sentences_transformed) en listas de nombres y colocaciones. Esto es necesario ya que los nombres y las colocaciones expresan los temas de las opiniones (e.g: [['Now we are taking all our computers off line to do a complete virus scan just in case'] -> ['computer', 'line', 'virus_scan', 'case'].\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76ad1379",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76ad1379",
    "outputId": "0de5085b-e325-4378-dc07-9dca5eb1057c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['top', 'line', 'system', 'pro'],\n",
       " [],\n",
       " ['application', 'file', 'direction', 'back', 'case', 'system'],\n",
       " ['minute', 'window', 'file', 'version', 'volume'],\n",
       " ['computer', 'line', 'virus', 'case'],\n",
       " ['avoid'],\n",
       " ['headache', 'waste', 'time', 'money'],\n",
       " ['complaint'],\n",
       " ['problem'],\n",
       " ['start', 'button', 'utility', 'window'],\n",
       " ['course', 'point', 'year'],\n",
       " ['graphic', 'document'],\n",
       " ['feature', 'business'],\n",
       " ['build', 'cd', 'drive', 'build'],\n",
       " ['expectation']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def get_noun_and_collocation(sentence):\n",
    "    nouns_and_collocations = []\n",
    "    noun_tags = ['NN', 'NNS']\n",
    "    tokens_pos_tagged = pos_tag(word_tokenize(sentence))\n",
    "    for tpos in tokens_pos_tagged:\n",
    "        lemma = lem.lemmatize(tpos[0]).lower()\n",
    "        if '_' in lemma:\n",
    "            nouns_and_collocations.append(lemma)\n",
    "        elif tpos[1] in noun_tags and tpos[0] not in stopwords:\n",
    "            nouns_and_collocations.append(lemma)\n",
    "    return nouns_and_collocations\n",
    "\n",
    "noun_and_collocation_stream = [get_noun_and_collocation(opinion) for opinion in opinion_sentences_transformed]\n",
    "\n",
    "noun_and_collocation_stream[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef21d0c",
   "metadata": {
    "id": "eef21d0c"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Extraemos temas a partir de las listas de nombres y colocaciones de cada oración transformada. Experimenta con el parámetro <i>num_topics</i> hasta encontrar un conjunto de temas informativos, asignando nombres a los temas encontrados. Prueba con al menos 3 valores diferentes para el parámetro *num_topics* ( n1, n2, n3).\n",
    "<br>\n",
    "<b>Importante:</b> Para cada valor diferente de <i>num_topics<i> genera un modelo diferente, cuyos nombres sean: <i>ldamodel1</i> (n1), <i>ldamodel2</i> (n2) y <i>ldamodel3</i> (n3).\n",
    "\n",
    "<b>Salida esperada:</b> Por cada experimento, imprime las 10 palabras que más se destaquen de cada tópico.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35cbb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "# Crear un diccionario Gensim a partir de las listas de nombres y colocaciones\n",
    "dictionary = corpora.Dictionary(noun_and_collocation_stream)\n",
    "\n",
    "# Filtrar tokens que aparezcan en menos de 15 documentos o en más del 50% de los documentos\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "\n",
    "# Convertir el stream de documentos (listas de tokens) a un corpus para LDA\n",
    "corpus = [dictionary.doc2bow(text) for text in noun_and_collocation_stream]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3284e474",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3284e474",
    "outputId": "4f6f8e10-f04f-4420-cb1c-f9293265f5c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con 5 tópicos:\n",
      "Tópico 0: 0.077*\"software\" + 0.036*\"way\" + 0.034*\"system\" + 0.031*\"price\" + 0.029*\"game\" + 0.023*\"computer\" + 0.021*\"video\" + 0.021*\"everything\" + 0.018*\"bit\" + 0.018*\"drive\"\n",
      "\n",
      "Tópico 1: 0.034*\"software\" + 0.028*\"hour\" + 0.026*\"time\" + 0.024*\"support\" + 0.022*\"tool\" + 0.021*\"virus\" + 0.018*\"minute\" + 0.017*\"nothing\" + 0.017*\"update\" + 0.015*\"process\"\n",
      "\n",
      "Tópico 2: 0.105*\"version\" + 0.083*\"year\" + 0.042*\"file\" + 0.041*\"feature\" + 0.030*\"issue\" + 0.019*\"something\" + 0.018*\"data\" + 0.018*\"account\" + 0.018*\"work\" + 0.017*\"number\"\n",
      "\n",
      "Tópico 3: 0.130*\"product\" + 0.070*\"time\" + 0.052*\"computer\" + 0.035*\"user\" + 0.032*\"money\" + 0.028*\"review\" + 0.021*\"..\" + 0.020*\"company\" + 0.019*\"help\" + 0.016*\"interface\"\n",
      "\n",
      "Tópico 4: 0.136*\"program\" + 0.065*\"problem\" + 0.036*\"tax\" + 0.035*\"pc\" + 0.032*\"software\" + 0.032*\"day\" + 0.030*\"thing\" + 0.027*\"year\" + 0.025*\"customer\" + 0.021*\"window\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Experimento 1: con n1 tópicos:\n",
    "\n",
    "# Definir n1\n",
    "n1 = 5\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "ldamodel1 = gensim.models.ldamodel.LdaModel(corpus, num_topics=n1, id2word=dictionary, passes=15)\n",
    "\n",
    "# Imprimir las 10 palabras más destacadas de cada tópico para n1\n",
    "print(f\"Modelo con {n1} tópicos:\")\n",
    "for idx, topic in ldamodel1.print_topics(-1):\n",
    "    print(f\"Tópico {idx}: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "187a952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimento con 5 tópicos:\n",
      "Tópico 1: 0.093*\"software\" + 0.047*\"computer\" + 0.037*\"using\" + 0.025*\"need\" + 0.022*\"pro\" + 0.020*\"tax\" + 0.019*\"problem\" + 0.017*\"norton\" + 0.016*\"cd\" + 0.015*\"two\"\n",
      "Tópico 2: 0.064*\"product\" + 0.043*\"would\" + 0.025*\".\" + 0.024*\"even\" + 0.023*\"support\" + 0.021*\"money\" + 0.020*\"back\" + 0.018*\"office\" + 0.018*\"way\" + 0.018*\"microsoft\"\n",
      "Tópico 3: 0.059*\"n't\" + 0.029*\"get\" + 0.029*\"version\" + 0.028*\"program\" + 0.026*\"time\" + 0.026*\"work\" + 0.024*\"one\" + 0.019*\"great\" + 0.015*\"well\" + 0.013*\"thing\"\n",
      "Tópico 4: 0.068*\"'s\" + 0.048*\"use\" + 0.048*\"year\" + 0.042*\"windows\" + 0.039*\"like\" + 0.025*\"'ve\" + 0.024*\"good\" + 0.018*\"found\" + 0.015*\"10\" + 0.013*\"7\"\n",
      "Tópico 5: 0.030*\"'m\" + 0.027*\"want\" + 0.027*\"...\" + 0.024*\"buy\" + 0.022*\"find\" + 0.022*\"go\" + 0.019*\"day\" + 0.018*\"x\" + 0.016*\"review\" + 0.016*\"tutorial\"\n",
      "\n",
      "Experimento con 10 tópicos:\n",
      "Tópico 1: 0.152*\"'s\" + 0.107*\"year\" + 0.103*\"version\" + 0.066*\"great\" + 0.025*\"last\" + 0.024*\"file\" + 0.017*\"license\" + 0.017*\"app\" + 0.015*\"issue\" + 0.015*\"turbotax\"\n",
      "Tópico 2: 0.127*\"product\" + 0.050*\".\" + 0.047*\"even\" + 0.046*\"support\" + 0.036*\"microsoft\" + 0.028*\"amazon\" + 0.022*\"review\" + 0.021*\"box\" + 0.020*\"information\" + 0.020*\"see\"\n",
      "Tópico 3: 0.065*\"get\" + 0.063*\"program\" + 0.057*\"work\" + 0.049*\"computer\" + 0.025*\"...\" + 0.024*\"price\" + 0.024*\"better\" + 0.022*\"know\" + 0.022*\"pc\" + 0.021*\"bit\"\n",
      "Tópico 4: 0.085*\"would\" + 0.084*\"windows\" + 0.051*\"'ve\" + 0.040*\"back\" + 0.033*\"make\" + 0.033*\"upgrade\" + 0.033*\"buy\" + 0.029*\"go\" + 0.027*\"7\" + 0.021*\"tutorial\"\n",
      "Tópico 5: 0.073*\"'m\" + 0.072*\"still\" + 0.057*\"user\" + 0.054*\"lot\" + 0.054*\"find\" + 0.051*\"since\" + 0.039*\"try\" + 0.038*\"love\" + 0.033*\"interface\" + 0.032*\"purchase\"\n",
      "Tópico 6: 0.112*\"software\" + 0.077*\"use\" + 0.067*\"time\" + 0.062*\"one\" + 0.039*\"good\" + 0.025*\"'ll\" + 0.024*\"first\" + 0.022*\"problem\" + 0.021*\"help\" + 0.020*\"norton\"\n",
      "Tópico 7: 0.058*\"want\" + 0.056*\"found\" + 0.042*\"old\" + 0.041*\"say\" + 0.037*\"word\" + 0.036*\"two\" + 0.034*\"2\" + 0.034*\"little\" + 0.032*\"update\" + 0.028*\"customer\"\n",
      "Tópico 8: 0.100*\"like\" + 0.051*\"much\" + 0.046*\"office\" + 0.043*\"pro\" + 0.039*\"10\" + 0.038*\"tax\" + 0.032*\"x\" + 0.025*\"run\" + 0.024*\"something\" + 0.020*\"turbo\"\n",
      "Tópico 9: 0.175*\"n't\" + 0.051*\"using\" + 0.045*\"well\" + 0.033*\"way\" + 0.026*\"system\" + 0.024*\"ca\" + 0.022*\"window\" + 0.019*\"vista\" + 0.018*\"going\" + 0.016*\"looking\"\n",
      "Tópico 10: 0.045*\"thing\" + 0.045*\"money\" + 0.045*\":\" + 0.042*\"need\" + 0.039*\"..\" + 0.029*\"day\" + 0.023*\"8\" + 0.022*\"everything\" + 0.020*\"hour\" + 0.019*\"music\"\n",
      "\n",
      "Experimento con 15 tópicos:\n",
      "Tópico 1: 0.124*\"need\" + 0.120*\"...\" + 0.073*\"file\" + 0.065*\"information\" + 0.065*\"everything\" + 0.065*\"see\" + 0.059*\"customer\" + 0.045*\"service\" + 0.034*\"e\" + 0.033*\"must\"\n",
      "Tópico 2: 0.155*\"product\" + 0.104*\"would\" + 0.073*\"great\" + 0.056*\"support\" + 0.049*\"back\" + 0.044*\"way\" + 0.041*\"upgrade\" + 0.034*\"system\" + 0.033*\"7\" + 0.026*\"worth\"\n",
      "Tópico 3: 0.145*\"thing\" + 0.102*\"without\" + 0.073*\"set\" + 0.064*\"card\" + 0.062*\"fine\" + 0.054*\"drive\" + 0.053*\"video\" + 0.052*\"disc\" + 0.041*\"12\" + 0.038*\"read\"\n",
      "Tópico 4: 0.195*\"time\" + 0.093*\"much\" + 0.078*\"pro\" + 0.077*\"make\" + 0.070*\"10\" + 0.050*\"try\" + 0.045*\"run\" + 0.044*\"something\" + 0.038*\"full\" + 0.031*\"return\"\n",
      "Tópico 5: 0.154*\"'m\" + 0.133*\"better\" + 0.096*\"norton\" + 0.090*\"free\" + 0.064*\"music\" + 0.057*\"app\" + 0.053*\"number\" + 0.048*\"play\" + 0.048*\"tech\" + 0.034*\"call\"\n",
      "Tópico 6: 0.266*\"'s\" + 0.188*\"use\" + 0.094*\"good\" + 0.093*\"well\" + 0.055*\"problem\" + 0.051*\"help\" + 0.031*\"basic\" + 0.031*\"'re\" + 0.025*\"enough\" + 0.023*\"fix\"\n",
      "Tópico 7: 0.280*\"software\" + 0.193*\"year\" + 0.071*\"found\" + 0.060*\"lot\" + 0.053*\"old\" + 0.040*\"far\" + 0.037*\"home\" + 0.035*\"business\" + 0.029*\"1\" + 0.024*\"game\"\n",
      "Tópico 8: 0.233*\"version\" + 0.124*\".\" + 0.118*\"even\" + 0.089*\"microsoft\" + 0.074*\"tax\" + 0.039*\"turbo\" + 0.035*\"feature\" + 0.032*\"nice\" + 0.029*\"change\" + 0.020*\"trial\"\n",
      "Tópico 9: 0.297*\"n't\" + 0.063*\"money\" + 0.048*\"first\" + 0.047*\"find\" + 0.041*\"ca\" + 0.040*\"day\" + 0.032*\"8\" + 0.028*\"take\" + 0.028*\"purchase\" + 0.028*\"hour\"\n",
      "Tópico 10: 0.443*\"like\" + 0.132*\"cd\" + 0.069*\"9\" + 0.056*\"email\" + 0.050*\"buying\" + 0.045*\"'d\" + 0.028*\"graphic\" + 0.013*\"build\" + 0.009*\"external\" + 0.001*\"70\"\n",
      "Tópico 11: 0.157*\"using\" + 0.090*\"pc\" + 0.081*\"since\" + 0.077*\"mac\" + 0.067*\"window\" + 0.051*\"looking\" + 0.046*\"os\" + 0.045*\"working\" + 0.039*\"open\" + 0.034*\"minute\"\n",
      "Tópico 12: 0.069*\"computer\" + 0.039*\":\" + 0.036*\"want\" + 0.035*\"office\" + 0.034*\"price\" + 0.034*\"..\" + 0.032*\"buy\" + 0.031*\"know\" + 0.029*\"bit\" + 0.028*\"go\"\n",
      "Tópico 13: 0.182*\"program\" + 0.080*\"still\" + 0.064*\"user\" + 0.056*\"amazon\" + 0.052*\"say\" + 0.048*\"word\" + 0.045*\"review\" + 0.043*\"little\" + 0.031*\"2007\" + 0.029*\"learning\"\n",
      "Tópico 14: 0.283*\"get\" + 0.249*\"work\" + 0.094*\"'ll\" + 0.064*\"love\" + 0.061*\"vista\" + 0.052*\"people\" + 0.035*\"fast\" + 0.032*\"dvd\" + 0.031*\"tool\" + 0.015*\"print\"\n",
      "Tópico 15: 0.178*\"windows\" + 0.164*\"one\" + 0.108*\"'ve\" + 0.047*\"last\" + 0.047*\"think\" + 0.038*\"interface\" + 0.037*\"come\" + 0.037*\"data\" + 0.032*\"license\" + 0.028*\"turbotax\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Asumiendo que 'opinion_sentences_transformed' y 'stopwords' ya están definidos\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def isnp(t):\n",
    "    if ' ' in t:  # Si el término es una multipalabra, asumimos que es nominal\n",
    "        return True\n",
    "    elif wn.synsets(t) == []:  # Si no tiene synset en WordNet, asumimos que es nominal\n",
    "        return True\n",
    "    else:\n",
    "        try:\n",
    "            wn.synset(t + '.n.01')  # Si existe un synset nominal, el término es nominal\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def get_nominals(sentence):\n",
    "    nps = [lem.lemmatize(token).lower() for token in word_tokenize(sentence) if isnp(token) and token.lower() not in stopwords]\n",
    "    return nps\n",
    "\n",
    "# Transformar las opiniones en listas de nombres y colocaciones\n",
    "nps_in_sentences = [get_nominals(sentence) for sentence in opinion_sentences_transformed if len(get_nominals(sentence)) > 0]\n",
    "\n",
    "# Creación del modelo LDA\n",
    "def perform_lda(terms, num_topics):\n",
    "    # Creación del diccionario y corpus\n",
    "    dictionary = corpora.Dictionary(terms)\n",
    "    corpus = [dictionary.doc2bow(text) for text in terms]\n",
    "    \n",
    "    # Entrenamiento del modelo LDA\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=100,\n",
    "                         update_every=1,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         alpha='auto')\n",
    "    \n",
    "    # Imprimir los temas\n",
    "    for idx, topic in lda_model.print_topics(-1, num_words=10):\n",
    "        print(f\"Tópico {idx + 1}: {topic}\")\n",
    "\n",
    "# Experimentos con diferentes números de temas\n",
    "num_topics_values = [4, 6, 8]  # Ejemplo de valores para n1, n2, n3\n",
    "\n",
    "for num_topics in num_topics_values:\n",
    "    print(f\"\\nExperimento con {num_topics} tópicos:\")\n",
    "    perform_lda(nps_in_sentences, num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "xs2hNFPBP4uF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs2hNFPBP4uF",
    "outputId": "61f44657-7fa9-48cd-daf6-4f1ac3380aa3"
   },
   "outputs": [],
   "source": [
    "# Lista de números de tópicos a explorar\n",
    "num_topics_list = [4, 6, 8]\n",
    "\n",
    "# Diccionario para almacenar modelos\n",
    "ldamodels = {}\n",
    "\n",
    "for num_topics in num_topics_list:\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=100)\n",
    "    ldamodels[num_topics] = ldamodel\n",
    "    print(f\"\\nExperimento con {num_topics} tópicos:\")\n",
    "    for idx, topic in ldamodel.print_topics(-1):\n",
    "        print(f\"Tópico {idx + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30283ece-3d7f-4a0a-be7b-da947f9d8f30",
   "metadata": {
    "id": "30283ece-3d7f-4a0a-be7b-da947f9d8f30"
   },
   "source": [
    "Usa la librería pyLDAvis para visualizar los tópicos del mejor modelo que encontraste. Para ejecutar las siguientes líneas es importante que los modelos previamente creados se denominen: *ldamodel1*, *ldamodel2* y *ldamodel3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGhUAJ-Qoyo4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "id": "lGhUAJ-Qoyo4",
    "outputId": "eeee6466-da0c-49db-b6da-36876290e4bb"
   },
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "\n",
    "#Instalar la versión pandas 1.5.3 si usas Google Colab como entorno de ejecución.\n",
    "#!pip install pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb8277",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "0aeb8277",
    "outputId": "62582306-1ad9-4618-94ad-484f1c96b69d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el53801406001178452328008357243\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el53801406001178452328008357243_data = {\"mdsDat\": {\"x\": [-0.15408792136229857, -0.2684120369298671, 0.08909515392923321, 0.3334048043629324], \"y\": [-0.32244682205979364, 0.15852303873907056, 0.2529111954095829, -0.08898741208885982], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [26.92879990331515, 26.58153299383003, 26.274216839808528, 20.21545026304629]}, \"tinfo\": {\"Term\": [\"software\", \"product\", \"program\", \"year\", \"version\", \"computer\", \"problem\", \"pc\", \"file\", \"price\", \"thing\", \"feature\", \"game\", \"user\", \"tax\", \"money\", \"issue\", \"day\", \"lot\", \"review\", \"drive\", \"customer\", \"hour\", \"everything\", \"video\", \"..\", \"error\", \"support\", \"update\", \"something\", \"software\", \"program\", \"thing\", \"tax\", \"lot\", \"everything\", \"people\", \"bit\", \"tool\", \"word\", \"return\", \"minute\", \"site\", \"state\", \"experience\", \"package\", \"part\", \"form\", \"photo\", \"image\", \"instruction\", \"piece\", \"language\", \"picture\", \"friend\", \"bug\", \"web\", \"person\", \"store\", \"operating\", \"business\", \"system\", \"way\", \"time\", \"anyone\", \"need\", \"product\", \"year\", \"version\", \"feature\", \"user\", \"money\", \"review\", \"customer\", \"video\", \"..\", \"service\", \"interface\", \"star\", \"reason\", \"website\", \"line\", \"job\", \"purchase\", \"page\", \"order\", \"subscription\", \"quality\", \"trial\", \"format\", \"function\", \"edition\", \"thanks\", \"map\", \"couple\", \"complaint\", \"company\", \"support\", \"way\", \"time\", \"anything\", \"one\", \"help\", \"computer\", \"problem\", \"file\", \"issue\", \"day\", \"hour\", \"update\", \"something\", \"window\", \"account\", \"installation\", \"data\", \"option\", \"card\", \"laptop\", \"refund\", \"process\", \"code\", \"machine\", \"week\", \"phone\", \"item\", \"email\", \"transaction\", \"case\", \"install\", \"step\", \"desktop\", \"copy\", \"access\", \"work\", \"virus\", \"number\", \"time\", \"support\", \"upgrade\", \"cd\", \"pc\", \"price\", \"game\", \"drive\", \"error\", \"screen\", \"use\", \"home\", \"music\", \"application\", \"disk\", \"question\", \"office\", \"security\", \"app\", \"device\", \"message\", \"document\", \"windows\", \"result\", \"menu\", \"note\", \"project\", \"button\", \"date\", \"value\", \"stuff\", \"start\", \"graphic\", \"amount\", \"month\", \"information\", \"download\", \"time\", \"work\"], \"Freq\": [4911.0, 4652.0, 4558.0, 3858.0, 3756.0, 3310.0, 2192.0, 1276.0, 1526.0, 1247.0, 1508.0, 1475.0, 1176.0, 1247.0, 1200.0, 1147.0, 1104.0, 1071.0, 1037.0, 993.0, 717.0, 827.0, 789.0, 771.0, 747.0, 744.0, 614.0, 1432.0, 699.0, 699.0, 4910.670282962232, 4558.154438563054, 1508.1722305883234, 1199.8701062224447, 1036.969402679143, 771.1326391709684, 683.9949701111449, 676.0330765437873, 621.7355010196598, 566.8087967011854, 511.9765011484225, 517.489727620715, 441.0000436819301, 422.0554390215103, 425.9067381130003, 400.40870120655035, 385.0869123991554, 381.46646333969994, 360.9350331647946, 348.9592549365846, 320.41094707334446, 313.1384259625762, 305.14337537049227, 295.4412310869903, 278.9036335718636, 261.7025230113685, 240.54560189149157, 239.96028069293345, 237.4021493952851, 232.28472531795904, 535.9761824074197, 748.4065119998446, 509.3267442589823, 673.6633902267223, 294.2668631814778, 281.897912328934, 4651.399883912153, 3857.813624652386, 3755.8362169015745, 1475.0574208370401, 1247.0621068138303, 1146.88068653504, 993.0843330609696, 826.359482951849, 747.1070795263856, 744.154983526051, 604.9359722870065, 594.7414763341551, 482.174007168537, 437.37702980765334, 427.7142953218142, 403.9890204637737, 390.7135009073254, 341.56381084670744, 329.3427719863367, 366.11826757733945, 292.09489517512617, 289.76253858852647, 283.7380285795778, 288.91962030182896, 269.99456239330243, 234.4178339268603, 232.44422488598235, 227.47798119904596, 224.83634978965424, 220.21829716011874, 550.678910404606, 873.2260511951669, 813.7764084592304, 1301.702836476677, 369.9992912856084, 304.99524838537025, 333.16508604439525, 3309.4587712171224, 2191.340919069261, 1525.6864708441046, 1103.6345901240102, 1070.4727361830521, 788.7652972076594, 698.9150444877682, 698.5857608685676, 695.8411605204765, 664.1500924905796, 669.1430584962155, 667.6138995799325, 633.5549402151314, 607.2261062495455, 476.20808846737407, 453.5263057870211, 442.38018356939654, 408.00669420209914, 399.0113499512382, 398.2789718652082, 392.4392337889497, 363.0005512425983, 350.49510775721745, 341.7578448884184, 338.2701540456467, 325.876111057411, 325.77369102260764, 322.44048551416876, 318.4833516935041, 318.61613424384416, 616.690187870544, 454.2554765650719, 425.8945777920602, 865.5236181050881, 558.9591146496172, 398.710547226893, 350.5944915404671, 1275.636302113348, 1246.5548210628817, 1175.2778502509298, 716.6668490033052, 613.7437544124463, 571.3266971024866, 553.1384822449719, 521.7855554402927, 474.0503176191322, 462.3381332922399, 450.6530608839609, 432.1928384643389, 424.92266561860544, 414.4790163769299, 411.55639103378877, 402.72287389073244, 368.5996617710428, 345.8132457750179, 302.99260255036023, 298.1417078014732, 284.4992140943762, 274.27278922974585, 270.25232859964774, 261.7116958540873, 243.52892641226495, 221.52609092810485, 213.15855988621524, 214.48459263863685, 211.06761896878933, 207.82524408064288, 387.03820123633693, 313.68147646366833, 267.65164083863306, 476.72735635968843, 228.71420415609353], \"Total\": [4911.0, 4652.0, 4558.0, 3858.0, 3756.0, 3310.0, 2192.0, 1276.0, 1526.0, 1247.0, 1508.0, 1475.0, 1176.0, 1247.0, 1200.0, 1147.0, 1104.0, 1071.0, 1037.0, 993.0, 717.0, 827.0, 789.0, 771.0, 747.0, 744.0, 614.0, 1432.0, 699.0, 699.0, 4911.460132157608, 4558.942674107402, 1508.9615407809686, 1200.6512983946557, 1037.7627851743096, 771.918663555371, 684.787117203945, 676.8235650488725, 622.5270198562182, 567.5906506815801, 512.7555945811515, 518.2806927499661, 441.7901444539617, 422.83402802302624, 426.6956829142414, 401.1925936995632, 385.8765700381463, 382.2567356997759, 361.7252020955423, 349.7457779824605, 321.20032358227394, 313.93819143854654, 305.9338120629889, 296.2232981523703, 279.6872476500351, 262.5012233208286, 241.32951128117713, 240.74897448167644, 238.20134383794422, 233.07034471945866, 664.2543789248028, 1303.2388545678598, 1323.6496659235952, 3317.6172011681756, 421.16762066184634, 365.26011373651926, 4652.1887908768485, 3858.6056465613065, 3756.626087513216, 1475.8620732785716, 1247.8585271805598, 1147.6679915688987, 993.8757751461293, 827.1440583666879, 747.9076715803717, 744.9540317801468, 605.7223113939368, 595.5415884342497, 482.95877653235215, 438.17406798998974, 428.5168346384602, 404.7882584335631, 391.51037096977564, 342.35955621001585, 330.14876835039155, 367.0186574665632, 292.8869842117134, 290.55718032715265, 284.53356122581346, 289.74050842867604, 270.8047937473034, 235.21150109560955, 233.23121989176624, 228.28021323224863, 225.63415177672246, 221.02811265618038, 726.2017190826892, 1432.7234445367872, 1323.6496659235952, 3317.6172011681756, 524.5576146552204, 416.9917109376898, 681.7306002171101, 3310.2451427116143, 2192.1243587802396, 1526.4732053584964, 1104.4215555958708, 1071.258294363959, 789.5499418005408, 699.7000651879559, 699.3733889342708, 696.6267033156145, 664.9262723099177, 669.9275720271758, 668.3982220349276, 634.3440340030688, 608.0166195296418, 476.9953881837263, 454.31253483112215, 443.16943556693934, 408.79173974443313, 399.7934206040501, 399.0615047884507, 393.2266069274166, 363.7883335658785, 351.28350767202284, 342.5345283106677, 339.0568234835121, 326.6581263714397, 326.56251541543105, 323.23351783629346, 319.267444334005, 319.4055281778273, 845.9187096850538, 606.6523984069809, 606.6343135586629, 3317.6172011681756, 1432.7234445367872, 661.9013902952654, 604.8761350112643, 1276.3951324466402, 1247.3131873936798, 1176.0297340183704, 717.4265296382771, 614.506536539245, 572.0859681790143, 553.8958955751058, 522.5472866609673, 474.8097376987983, 463.0976868078262, 451.4197212026824, 432.9587605331212, 425.6890744258984, 415.24289256454836, 412.32155457591784, 403.48201225813864, 369.3596307926179, 346.5769080598779, 303.751358052275, 298.90139578153514, 285.25839800141836, 275.040423364593, 271.0200518741188, 262.47080573152914, 244.29375473595763, 222.2828822371435, 213.91469615796862, 215.24589066721555, 211.82529843477286, 208.58525830973755, 623.4092829876777, 568.2291525664693, 440.95436769123364, 3317.6172011681756, 845.9187096850538], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.26, -2.3345, -3.4406, -3.6692, -3.8152, -4.1114, -4.2313, -4.243, -4.3267, -4.4192, -4.5209, -4.5102, -4.6702, -4.7141, -4.705, -4.7667, -4.8057, -4.8152, -4.8705, -4.9043, -4.9896, -5.0126, -5.0384, -5.0707, -5.1283, -5.192, -5.2763, -5.2787, -5.2895, -5.3112, -4.4751, -4.1413, -4.5261, -4.2465, -5.0747, -5.1177, -2.3013, -2.4884, -2.5152, -3.4498, -3.6177, -3.7014, -3.8454, -4.0292, -4.13, -4.134, -4.3411, -4.3581, -4.5679, -4.6654, -4.6878, -4.7448, -4.7783, -4.9127, -4.9491, -4.8433, -5.0692, -5.0772, -5.0982, -5.0801, -5.1478, -5.2891, -5.2976, -5.3192, -5.3309, -5.3516, -4.4351, -3.974, -4.0445, -3.5748, -4.8327, -5.0259, -4.9376, -2.6301, -3.0423, -3.4044, -3.7282, -3.7587, -4.0641, -4.1851, -4.1855, -4.1895, -4.2361, -4.2286, -4.2309, -4.2833, -4.3257, -4.5687, -4.6176, -4.6424, -4.7233, -4.7456, -4.7475, -4.7622, -4.8402, -4.8753, -4.9005, -4.9108, -4.9481, -4.9484, -4.9587, -4.971, -4.9706, -4.3102, -4.6159, -4.6804, -3.9713, -4.4085, -4.7464, -4.875, -3.3213, -3.3443, -3.4032, -3.8979, -4.0529, -4.1245, -4.1569, -4.2152, -4.3111, -4.3362, -4.3618, -4.4036, -4.4206, -4.4454, -4.4525, -4.4742, -4.5628, -4.6266, -4.7588, -4.7749, -4.8217, -4.8583, -4.8731, -4.9052, -4.9772, -5.0719, -5.1104, -5.1042, -5.1203, -5.1358, -4.5139, -4.7241, -4.8828, -4.3055, -5.04], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3118, 1.3118, 1.3115, 1.3113, 1.3112, 1.311, 1.3108, 1.3108, 1.3107, 1.3106, 1.3105, 1.3104, 1.3102, 1.3101, 1.3101, 1.31, 1.3099, 1.3099, 1.3098, 1.3097, 1.3095, 1.3094, 1.3094, 1.3093, 1.3092, 1.3089, 1.3087, 1.3087, 1.3086, 1.3086, 1.0974, 0.7573, 0.3569, -0.2823, 0.9534, 1.0529, 1.3248, 1.3247, 1.3247, 1.3244, 1.3243, 1.3243, 1.3242, 1.324, 1.3239, 1.3239, 1.3237, 1.3236, 1.3233, 1.3231, 1.3231, 1.323, 1.3229, 1.3226, 1.3225, 1.3225, 1.3222, 1.3222, 1.3222, 1.3221, 1.322, 1.3216, 1.3216, 1.3214, 1.3214, 1.3213, 1.0483, 0.8298, 0.8385, 0.3894, 0.9759, 1.0122, 0.609, 1.3363, 1.3362, 1.3361, 1.3359, 1.3358, 1.3356, 1.3355, 1.3355, 1.3355, 1.3354, 1.3354, 1.3354, 1.3353, 1.3353, 1.3349, 1.3349, 1.3348, 1.3347, 1.3346, 1.3346, 1.3346, 1.3344, 1.3343, 1.3343, 1.3343, 1.3342, 1.3342, 1.3341, 1.3341, 1.3341, 1.0205, 1.0473, 0.9828, -0.0071, 0.3953, 0.8297, 0.7912, 1.5981, 1.5981, 1.5981, 1.5977, 1.5975, 1.5974, 1.5974, 1.5973, 1.5971, 1.5971, 1.597, 1.597, 1.5969, 1.5969, 1.5969, 1.5968, 1.5967, 1.5965, 1.5962, 1.5962, 1.5961, 1.5959, 1.5959, 1.5958, 1.5956, 1.5953, 1.5952, 1.5952, 1.5951, 1.5951, 1.122, 1.0046, 1.0995, -0.3413, 0.2908]}, \"token.table\": {\"Topic\": [2, 3, 3, 4, 1, 2, 2, 4, 4, 4, 1, 1, 1, 4, 4, 3, 3, 1, 3, 4, 3, 1, 2, 2, 3, 3, 2, 2, 3, 4, 3, 3, 4, 4, 4, 3, 4, 4, 2, 3, 4, 1, 1, 2, 3, 1, 2, 1, 2, 4, 4, 1, 2, 3, 4, 3, 1, 1, 4, 3, 3, 1, 2, 3, 3, 2, 1, 3, 2, 1, 3, 2, 4, 4, 1, 2, 3, 4, 4, 1, 2, 4, 2, 3, 4, 1, 2, 3, 1, 3, 2, 1, 2, 1, 4, 1, 1, 3, 1, 1, 1, 4, 3, 3, 2, 1, 4, 2, 2, 4, 2, 3, 4, 1, 2, 4, 4, 2, 1, 1, 3, 2, 4, 1, 3, 1, 4, 2, 2, 3, 1, 3, 4, 1, 2, 1, 1, 2, 3, 4, 1, 3, 2, 3, 2, 3, 4, 2, 4, 2, 2, 3, 4, 1, 2, 1, 2, 3, 3, 4, 1, 3, 4, 2], \"Freq\": [0.9987193414097417, 0.9987303658138267, 0.9986069548632814, 0.9971941530553013, 0.6980593606364895, 0.29916829741563833, 0.7053562652849725, 0.29358071582131284, 0.9992201363902778, 0.9976296862647, 0.9987831909357159, 0.9980905867237959, 0.8069197840556172, 0.19269726186402797, 0.9982062548624524, 0.9983279741096086, 0.9968830490634161, 0.185161876154883, 0.5802840940211065, 0.23475880726779808, 0.9980632198074059, 0.24097987570320467, 0.7587423514998044, 0.995348498234794, 0.9996238518121972, 0.9960301485275177, 0.9971894690066688, 0.9986168571784859, 0.9994042144012364, 0.9987975348110101, 0.9988254052541958, 0.9961838182978344, 0.9988053686571032, 0.9990702196138791, 0.9983354111411883, 0.39233084571947946, 0.6077726396116792, 0.9994054727269535, 0.9948493118322599, 0.9963462341840962, 0.9991757019508732, 0.9988098959142407, 0.9983696040478075, 0.9994158849297776, 0.9996900008746729, 0.9967123255592206, 0.9974442357656789, 0.9975427994811725, 0.9970281406906911, 0.9991243979734663, 0.9961038721962334, 0.1848237411667789, 0.48846274451220145, 0.32564182967480093, 0.9989526561998543, 0.9993034743321155, 0.9978676569399563, 0.44700276086290386, 0.5525939642163457, 0.9979852747618733, 0.9986154144628367, 0.9962630063105572, 0.9990905951074323, 0.9996183019122229, 0.9978329883255156, 0.9986964049802527, 0.9969476663704088, 0.9979132121433785, 0.9980526647768553, 0.9992649715472487, 0.9980154235583685, 0.9943919220412408, 0.9955885680834116, 0.9990263397441508, 0.9975289591762124, 0.9994179574809039, 0.37856349983910137, 0.6207799764310687, 0.9982946059558029, 0.7720525439123664, 0.2272353232082497, 0.996217198359916, 0.2967191205259667, 0.7022352519114546, 0.9983812729353514, 0.009592516817672933, 0.7314294073475612, 0.25659982487275096, 0.9954076323148403, 0.9994576539154979, 0.9972245076760001, 0.9970273785750485, 0.9965204524126156, 0.9977283667726712, 0.9996904309358475, 0.9988505665714641, 0.9968889816320549, 0.9968806614155613, 0.9979951573975464, 0.9958703513194257, 0.9970115409206904, 0.9997489103804521, 0.9994870916990927, 0.9973611998637874, 0.9997444663296597, 0.9997932252772652, 0.9962362494322281, 0.998949770194832, 0.9980823728860347, 0.9977855615349124, 0.9973205443321294, 0.9993120708605623, 0.9969843038732614, 0.9985264040234048, 0.9991188283606163, 0.9981017395296881, 0.9970068300100883, 0.9988075205744451, 0.9982114937060483, 0.9999063145897091, 0.9994661093198873, 0.9980147859839381, 0.994211779544996, 0.9980275286099235, 0.9982774648378873, 0.9949566034406526, 0.9957240144113654, 0.9969715820110591, 0.6093290392705545, 0.3901660171274226, 0.573954649508995, 0.2509133293976489, 0.17494874343322306, 0.9994575457541032, 0.9947210330918065, 0.9993627797959178, 0.20315785671797093, 0.3924503404255166, 0.2610307179788766, 0.14377788969506253, 0.9991534185032805, 0.9984394907184863, 0.9981247863221661, 0.9989994781724539, 0.3973401534670885, 0.602808825982389, 0.9983825560321663, 0.9993119995882068, 0.9987273773207525, 0.9998333378146691, 0.9987863855194135, 0.7483692493298741, 0.25055534338797547, 0.38454283871619316, 0.6149663471807096, 0.9986346001389229, 0.9987938988700497, 0.9973399970287452, 0.9991003742569274, 0.9975264043028716, 0.9989593720740981, 0.7293845057874614, 0.2707115912890254, 0.9998430400469024], \"Term\": [\"..\", \"access\", \"account\", \"amount\", \"anyone\", \"anyone\", \"anything\", \"anything\", \"app\", \"application\", \"bit\", \"bug\", \"business\", \"business\", \"button\", \"card\", \"case\", \"cd\", \"cd\", \"cd\", \"code\", \"company\", \"company\", \"complaint\", \"computer\", \"copy\", \"couple\", \"customer\", \"data\", \"date\", \"day\", \"desktop\", \"device\", \"disk\", \"document\", \"download\", \"download\", \"drive\", \"edition\", \"email\", \"error\", \"everything\", \"experience\", \"feature\", \"file\", \"form\", \"format\", \"friend\", \"function\", \"game\", \"graphic\", \"help\", \"help\", \"help\", \"home\", \"hour\", \"image\", \"information\", \"information\", \"install\", \"installation\", \"instruction\", \"interface\", \"issue\", \"item\", \"job\", \"language\", \"laptop\", \"line\", \"lot\", \"machine\", \"map\", \"menu\", \"message\", \"minute\", \"money\", \"month\", \"month\", \"music\", \"need\", \"need\", \"note\", \"number\", \"number\", \"office\", \"one\", \"one\", \"one\", \"operating\", \"option\", \"order\", \"package\", \"page\", \"part\", \"pc\", \"people\", \"person\", \"phone\", \"photo\", \"picture\", \"piece\", \"price\", \"problem\", \"process\", \"product\", \"program\", \"project\", \"purchase\", \"quality\", \"question\", \"reason\", \"refund\", \"result\", \"return\", \"review\", \"screen\", \"security\", \"service\", \"site\", \"software\", \"something\", \"star\", \"start\", \"state\", \"step\", \"store\", \"stuff\", \"subscription\", \"support\", \"support\", \"system\", \"system\", \"system\", \"tax\", \"thanks\", \"thing\", \"time\", \"time\", \"time\", \"time\", \"tool\", \"transaction\", \"trial\", \"update\", \"upgrade\", \"upgrade\", \"use\", \"user\", \"value\", \"version\", \"video\", \"virus\", \"virus\", \"way\", \"way\", \"web\", \"website\", \"week\", \"window\", \"windows\", \"word\", \"work\", \"work\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el53801406001178452328008357243\", ldavis_el53801406001178452328008357243_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el53801406001178452328008357243\", ldavis_el53801406001178452328008357243_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el53801406001178452328008357243\", ldavis_el53801406001178452328008357243_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "#import pyLDAvis\n",
    "\n",
    "\n",
    "ldamodel1 = ldamodels[4]  # Modelo con n1 = 4 tópicos\n",
    "ldamodel2 = ldamodels[6]  # Modelo con n2 = 6 tópicos\n",
    "ldamodel3 = ldamodels[8]  # Modelo con n3 = 8 tópicos\n",
    "\n",
    "# visualizar cada modelo con pyLDAvis:\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Visualización del Modelo 1: con 4 tópicos\n",
    "vis1 = pyLDAvis.gensim_models.prepare(ldamodel1, corpus, dictionary)\n",
    "pyLDAvis.display(vis1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B0_drQA8zqjm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "B0_drQA8zqjm",
    "outputId": "aeeb81dc-d4ef-4642-d7be-42b5c36d8837"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el53801406001122826408113223435\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el53801406001122826408113223435_data = {\"mdsDat\": {\"x\": [0.053607604942611436, -0.17596135813697103, 0.1774962345432943, -0.20886244905452617, -0.16954386254161355, 0.3232638302472052], \"y\": [-0.16750768458921428, 0.05109176204092274, 0.36173548903362157, 0.08721887446738308, -0.16127502356258866, -0.17126341739012463], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [18.677191717965353, 17.521909269839593, 17.409634428261676, 16.336921769522526, 15.698270866716227, 14.356071947694621]}, \"tinfo\": {\"Term\": [\"program\", \"product\", \"year\", \"version\", \"software\", \"problem\", \"thing\", \"computer\", \"file\", \"support\", \"system\", \"price\", \"money\", \"pc\", \"game\", \"user\", \"lot\", \"day\", \"review\", \"feature\", \"time\", \"tax\", \"customer\", \"everything\", \"video\", \"drive\", \"..\", \"bit\", \"issue\", \"cd\", \"problem\", \"file\", \"day\", \"installation\", \"account\", \"process\", \"code\", \"machine\", \"transaction\", \"install\", \"step\", \"copy\", \"change\", \"bank\", \"backup\", \"solution\", \"area\", \"password\", \"credit\", \"player\", \"everyone\", \"sound\", \"performance\", \"speed\", \"apps\", \"hardware\", \"mistake\", \"server\", \"dollar\", \"background\", \"work\", \"computer\", \"hour\", \"laptop\", \"option\", \"book\", \"office\", \"home\", \"something\", \"others\", \"access\", \"time\", \"data\", \"device\", \"issue\", \"card\", \"version\", \"update\", \"upgrade\", \"star\", \"nothing\", \"refund\", \"reason\", \"point\", \"package\", \"question\", \"item\", \"email\", \"key\", \"piece\", \"quality\", \"trial\", \"note\", \"project\", \"thanks\", \"name\", \"kind\", \"type\", \"movie\", \"trouble\", \"choice\", \"entry\", \"mouse\", \"world\", \"answer\", \"design\", \"software\", \"issue\", \"tool\", \"business\", \"time\", \"form\", \"feature\", \"fact\", \"product\", \"year\", \"support\", \"user\", \"review\", \"customer\", \"..\", \"service\", \"interface\", \"one\", \"job\", \"phone\", \"purchase\", \"page\", \"someone\", \"format\", \"edition\", \"person\", \"market\", \"suite\", \"functionality\", \"wife\", \"call\", \"sale\", \"curve\", \"learning\", \"fix\", \"none\", \"today\", \"track\", \"feature\", \"line\", \"website\", \"online\", \"company\", \"way\", \"help\", \"number\", \"hour\", \"program\", \"lot\", \"everything\", \"video\", \"bit\", \"return\", \"site\", \"experience\", \"state\", \"part\", \"need\", \"friend\", \"course\", \"bug\", \"web\", \"family\", \"kid\", \"set\", \"idea\", \"deal\", \"setting\", \"text\", \"fun\", \"difference\", \"memory\", \"link\", \"try\", \"effect\", \"voice\", \"room\", \"information\", \"order\", \"card\", \"company\", \"tax\", \"software\", \"people\", \"time\", \"week\", \"thing\", \"system\", \"money\", \"pc\", \"virus\", \"anything\", \"minute\", \"application\", \"security\", \"instruction\", \"protection\", \"menu\", \"report\", \"button\", \"map\", \"operating\", \"couple\", \"driver\", \"hand\", \"list\", \"child\", \"start\", \"stuff\", \"amount\", \"waste\", \"click\", \"brand\", \"space\", \"processor\", \"icon\", \"tax\", \"anyone\", \"window\", \"box\", \"data\", \"time\", \"computer\", \"software\", \"way\", \"price\", \"game\", \"drive\", \"cd\", \"month\", \"word\", \"error\", \"music\", \"download\", \"disk\", \"app\", \"photo\", \"image\", \"message\", \"document\", \"language\", \"disc\", \"picture\", \"windows\", \"result\", \"store\", \"son\", \"improvement\", \"lesson\", \"value\", \"graphic\", \"school\", \"addition\", \"crash\", \"daughter\", \"screen\", \"help\", \"computer\", \"use\"], \"Freq\": [4692.0, 4671.0, 3874.0, 3766.0, 4891.0, 2169.0, 1572.0, 3347.0, 1510.0, 1423.0, 1338.0, 1217.0, 1227.0, 1222.0, 1148.0, 1253.0, 1068.0, 1060.0, 997.0, 1481.0, 3318.0, 1245.0, 830.0, 794.0, 790.0, 700.0, 748.0, 696.0, 1081.0, 629.0, 2168.466940851467, 1509.744968453988, 1059.267270554682, 662.1156099983028, 657.1671783459924, 437.7171754769022, 403.6977186384371, 394.79439019450393, 338.1311056422891, 322.4183410288213, 322.32614045872475, 315.10294169123955, 307.2733714197266, 300.189309483541, 262.50745631025137, 251.55879820673314, 236.45062674801818, 231.19718539571969, 230.20515529211073, 178.1722818633356, 176.7404219053112, 163.56361375692578, 162.7253880662425, 161.57447689451297, 161.1663077370166, 160.91259657380334, 158.29015457561027, 153.35221062415158, 150.40846499073416, 146.47454360081488, 676.8292767097616, 2079.6054115130405, 597.1302892215372, 385.2810727497275, 469.41821204534295, 336.419208871957, 307.9860182923852, 353.2249439098401, 428.27210408549433, 234.35468253214864, 241.38990826132462, 634.4641939196124, 283.46622968802274, 247.35364055348168, 280.81901239651324, 260.1131598044997, 3765.387211256711, 681.6652821689156, 651.9259483319556, 483.34164613494073, 481.17313462427927, 442.31354980640816, 438.4427804304705, 396.37366282739526, 390.83006697415846, 387.92984895058186, 354.01490842558417, 341.8150277333407, 339.74998238118786, 305.64929899776496, 290.447616363365, 284.4089914088622, 246.15919529033835, 242.55239580501697, 232.97515978443045, 219.06115741160218, 219.21987871953488, 214.36494027053934, 207.11184278017924, 206.68888065315565, 167.07587226622158, 164.04720562689997, 160.52938475639306, 160.5214707930562, 158.6310744912636, 150.21995740351582, 3221.1150630494603, 799.7917511821386, 473.25067268894776, 456.1009858034202, 1421.134152550295, 291.8916366141716, 441.6981714248483, 210.06658872411654, 4670.62888798041, 3873.7534940044684, 1422.6187430363555, 1252.1782110228423, 997.1430162582147, 829.7217467727905, 747.1897215877905, 607.3810235906699, 597.1556775071974, 414.8466770266219, 392.2790554266447, 383.33424366356854, 342.923214630833, 330.6639353973147, 313.2253084024059, 290.08365515314927, 235.33207836958115, 234.57187888854307, 186.02925431855186, 180.57700208415275, 176.63321933585254, 165.02191466743926, 154.500924432878, 150.16468742736978, 149.43742860149447, 148.02538668487418, 144.762405812652, 144.09187575377527, 141.62522282309226, 135.73951407082228, 1038.8853011362853, 303.8243996195449, 311.5934921017491, 210.6076585178068, 350.69304385027095, 360.88294153838115, 266.17315992838166, 221.2584703537687, 181.17615363949827, 4692.111128314179, 1067.403851134491, 793.743670863968, 789.7664311826929, 695.8539469558903, 526.9622943525138, 453.91037588300287, 438.3717639667542, 434.39803417769394, 396.35197253279824, 377.4257296471841, 287.0407668665827, 280.0944242724704, 269.3493412882821, 247.55546328295097, 237.58835322620257, 236.7322054267047, 235.58723604207276, 232.40514648801627, 223.93947259644733, 217.4902450773556, 216.13135839826563, 211.19453545752475, 182.9338937670739, 183.13900974090228, 178.3927181082334, 170.20390026015696, 159.83244796124865, 156.27414848293614, 149.54261395138187, 399.4356787444688, 300.2095899656797, 354.3073725425055, 392.92565297879185, 478.63309940107064, 763.9495785369235, 311.6169257945879, 449.7554612033973, 249.58013928906834, 1571.3119073968005, 1337.6394304535838, 1227.1001933054292, 1222.1309043834651, 618.0926391246425, 543.1719367457123, 539.1197617668448, 442.90363990239297, 397.05550573098924, 333.77758197858594, 331.5497467960924, 272.5136347218841, 258.31461879800815, 250.68014473634364, 243.35404110369882, 241.96089356518698, 240.52152495565878, 239.63701360328457, 229.13995031420671, 209.28032235145682, 207.71222971993828, 205.437790207133, 204.16014424492272, 199.05129783822048, 194.04364466079423, 185.152718875951, 169.96664418758135, 163.0571561842663, 145.47872587322925, 144.93466375289358, 765.8036332128812, 327.89616148436454, 465.6355663280378, 280.6000481445976, 396.9027385758721, 812.763117138253, 759.9165149304692, 905.6836178728121, 237.7009409470132, 1216.8990534389254, 1147.3081426507747, 699.5893070127377, 628.831636034755, 627.7141314719539, 601.6829531749141, 599.1125478109803, 462.73166101810443, 444.2616887912633, 439.8968177514627, 401.7258097466631, 383.12849256378837, 370.41205891473624, 359.78361522369664, 337.53757297487005, 323.89993292368445, 317.99112696372237, 313.58711779523253, 295.7329811025608, 290.99894988654455, 251.99155423014398, 233.07286043712477, 231.22848488491167, 216.81343308469928, 216.19663954260372, 205.98967260696512, 179.82589199332153, 173.78312848603076, 168.61347237316792, 165.75149379201093, 402.0031647242998, 319.74799667000036, 507.321742716786, 246.55894830760684], \"Total\": [4692.0, 4671.0, 3874.0, 3766.0, 4891.0, 2169.0, 1572.0, 3347.0, 1510.0, 1423.0, 1338.0, 1217.0, 1227.0, 1222.0, 1148.0, 1253.0, 1068.0, 1060.0, 997.0, 1481.0, 3318.0, 1245.0, 830.0, 794.0, 790.0, 700.0, 748.0, 696.0, 1081.0, 629.0, 2169.3138454023247, 1510.5920255694843, 1060.1147299260774, 662.9626898817779, 658.0132061055891, 438.564870740516, 404.5449637246606, 395.64098307310746, 338.9771948162194, 323.26552824969417, 323.17350940135066, 315.9510580635953, 308.1213668838836, 301.0351965883279, 263.3543508711985, 252.4066986752718, 237.2995775122917, 232.04428601798975, 231.052516534766, 179.0235506166531, 177.58813779023436, 164.41166544114256, 163.5718433429209, 162.42119598203118, 162.0133011096383, 161.7602454890153, 159.13826264663095, 154.19947731010245, 151.2566286269201, 147.32186161632825, 829.288996939875, 3347.3414958374547, 778.9919500438814, 478.21316295932826, 635.9282828918239, 417.63471392984013, 386.7863928687952, 474.4504431349924, 696.4667625833991, 296.8061171597384, 321.3960822420646, 3318.457978389877, 681.0435705576274, 376.14582849877456, 1081.2966893294122, 615.0981355637974, 3766.236544572191, 682.514250311537, 652.7751579856367, 484.19032343839893, 482.02235480227944, 443.16276865896714, 439.2925027928837, 397.225726198175, 391.6791984876296, 388.7794869022264, 354.86441793886956, 342.6652827801181, 340.5996409881584, 306.4998992538193, 291.2972869492065, 285.2587361401668, 247.01059586815458, 243.4030003066071, 233.82407549597696, 219.91057841931942, 220.07064753138448, 215.21570102576013, 207.96170454665284, 207.53804363397435, 167.92524803029985, 164.8986791205875, 161.38070994787623, 161.37364195773176, 159.47924238821037, 151.0688878277389, 4891.254108569935, 1081.2966893294122, 619.4410011089126, 648.3665784802463, 3318.457978389877, 378.6027372575079, 1481.2722766586458, 318.36314712960507, 4671.477603812564, 3874.6024655586066, 1423.4677319807697, 1253.0277772969166, 997.9920442851196, 830.5696383308323, 748.0392868688849, 608.2291838658965, 598.0056096672005, 415.6965473091178, 393.12866881989424, 384.18370890761855, 343.7729224128214, 331.5138360598832, 314.0761341828053, 290.93607922674744, 236.18146175514556, 235.4210512977302, 186.8786022417522, 181.42776398493632, 177.48323158395877, 165.87448819637243, 155.35020924700635, 151.01390087810242, 150.2871747443933, 148.8752903475761, 145.61356992285738, 144.94201153778712, 142.47526728554843, 136.59038353579254, 1481.2722766586458, 406.2944750430084, 430.095889170057, 285.7570846747563, 744.2977171350819, 1357.5543546956783, 700.263895530898, 607.8785175714664, 778.9919500438814, 4692.951477223381, 1068.2451103566775, 794.5843066211881, 790.6066754466277, 696.6944990844486, 527.8018193430514, 454.7508357736508, 439.2123356337601, 435.23718504446595, 397.1927224587843, 378.2668407932793, 287.88085810513195, 280.93583116882957, 270.19080981763767, 248.39540704084254, 238.42950072116128, 237.57202850867984, 236.43066111245238, 233.24568089210936, 224.7807319166395, 218.33256948096334, 216.97432517345462, 212.03403671476897, 183.77472674466156, 183.98150555635746, 179.23376886604265, 171.0454907653716, 160.67370989085765, 157.11469151372035, 150.38337825575897, 560.8863733669141, 383.57465606472095, 615.0981355637974, 744.2977171350819, 1245.1040923195294, 4891.254108569935, 686.5123525845763, 3318.457978389877, 406.7996228555134, 1572.1502292265495, 1338.4772241883627, 1227.9382216393926, 1222.9691673974037, 618.9296729080668, 544.0100613359953, 539.9579915774061, 443.74167848068197, 397.8934792621192, 334.61683670338414, 332.38741335343707, 273.3516378298474, 259.15456767368823, 251.51824237341935, 244.1923980488601, 242.79766532790802, 241.35959913789995, 240.4752968287541, 229.97893091857478, 210.12054125234357, 208.5507815110954, 206.27512629422492, 204.99730708013146, 199.88971100934452, 194.880262179091, 185.99121585689122, 170.80502427046503, 163.89521172961608, 146.31832925043128, 145.77431213630553, 1245.1040923195294, 435.32696889834773, 712.3745560259442, 384.9118384023197, 681.0435705576274, 3318.457978389877, 3347.3414958374547, 4891.254108569935, 1357.5543546956783, 1217.7338179843186, 1148.141770672574, 700.4240537378276, 629.6665815025211, 628.5492449087145, 602.5176894237152, 599.9475793642254, 463.56624224316374, 445.09617092797214, 440.73228085560083, 402.561246135702, 383.96351878224976, 371.2462270724896, 360.6183531416554, 338.37398398371823, 324.73467998947604, 318.8261196336555, 314.42181958913085, 296.5674757736852, 291.8333386090572, 252.82721294662846, 233.90647557859512, 232.06279452235032, 217.64754697791335, 217.03118636347074, 206.8237687468299, 180.66174522933744, 174.61868381306132, 169.4484922207772, 166.58538077835667, 555.6310749733955, 700.263895530898, 3347.3414958374547, 519.800228127566], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.7115, -3.0736, -3.428, -3.8979, -3.9054, -4.3117, -4.3927, -4.415, -4.5699, -4.6175, -4.6178, -4.6404, -4.6656, -4.6889, -4.823, -4.8656, -4.9276, -4.95, -4.9543, -5.2106, -5.2186, -5.2961, -5.3013, -5.3084, -5.3109, -5.3125, -5.3289, -5.3606, -5.38, -5.4065, -3.8759, -2.7534, -4.0012, -4.4393, -4.2418, -4.575, -4.6633, -4.5262, -4.3336, -4.9365, -4.9069, -3.9405, -4.7462, -4.8825, -4.7556, -4.8322, -2.0959, -3.8049, -3.8495, -4.1487, -4.1532, -4.2374, -4.2462, -4.3471, -4.3612, -4.3686, -4.4601, -4.4952, -4.5013, -4.607, -4.658, -4.6791, -4.8235, -4.8383, -4.8785, -4.9401, -4.9394, -4.9618, -4.9962, -4.9983, -5.211, -5.2293, -5.251, -5.251, -5.2629, -5.3174, -2.252, -3.6451, -4.1698, -4.2068, -3.0703, -4.6531, -4.2388, -4.982, -1.874, -2.0611, -3.0628, -3.1904, -3.4181, -3.6019, -3.7067, -3.9139, -3.9309, -4.2951, -4.3511, -4.3741, -4.4855, -4.5219, -4.5761, -4.6529, -4.862, -4.8653, -5.0971, -5.1269, -5.149, -5.217, -5.2828, -5.3113, -5.3162, -5.3257, -5.3479, -5.3526, -5.3699, -5.4123, -3.3771, -4.6066, -4.5813, -4.973, -4.4631, -4.4345, -4.7389, -4.9237, -5.1236, -1.8058, -3.2865, -3.5827, -3.5877, -3.7143, -3.9923, -4.1415, -4.1764, -4.1855, -4.2771, -4.3261, -4.5998, -4.6243, -4.6634, -4.7478, -4.7889, -4.7925, -4.7974, -4.811, -4.8481, -4.8773, -4.8836, -4.9067, -5.0503, -5.0492, -5.0755, -5.1224, -5.1853, -5.2078, -5.2519, -4.2694, -4.555, -4.3893, -4.2858, -4.0885, -3.6209, -4.5177, -4.1507, -4.7397, -2.8599, -3.0209, -3.1072, -3.1112, -3.7929, -3.9221, -3.9296, -4.1262, -4.2355, -4.4091, -4.4158, -4.6119, -4.6654, -4.6954, -4.725, -4.7308, -4.7368, -4.7404, -4.7852, -4.8759, -4.8834, -4.8944, -4.9007, -4.926, -4.9515, -4.9984, -5.084, -5.1255, -5.2395, -5.2433, -3.5786, -4.4269, -4.0762, -4.5826, -4.2359, -3.5191, -3.5864, -3.4109, -4.7486, -3.0261, -3.085, -3.5797, -3.6863, -3.6881, -3.7305, -3.7347, -3.993, -4.0338, -4.0436, -4.1344, -4.1818, -4.2156, -4.2447, -4.3085, -4.3498, -4.3682, -4.3821, -4.4407, -4.4569, -4.6008, -4.6788, -4.6868, -4.7512, -4.754, -4.8024, -4.9382, -4.9724, -5.0026, -5.0197, -4.1337, -4.3627, -3.901, -4.6226], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6775, 1.6773, 1.6771, 1.6766, 1.6766, 1.6759, 1.6758, 1.6757, 1.6754, 1.6752, 1.6752, 1.6752, 1.6751, 1.6751, 1.6746, 1.6745, 1.6743, 1.6742, 1.6742, 1.6731, 1.6731, 1.6727, 1.6727, 1.6726, 1.6726, 1.6726, 1.6725, 1.6724, 1.6722, 1.6721, 1.4747, 1.2019, 1.412, 1.4618, 1.3743, 1.4616, 1.45, 1.3828, 1.1916, 1.4416, 1.3916, 0.0234, 0.8013, 1.2587, 0.3297, 0.8172, 1.7415, 1.7405, 1.7404, 1.74, 1.74, 1.7398, 1.7398, 1.7396, 1.7395, 1.7395, 1.7393, 1.7392, 1.7392, 1.7389, 1.7388, 1.7387, 1.7383, 1.7382, 1.7381, 1.7378, 1.7378, 1.7378, 1.7376, 1.7376, 1.7366, 1.7365, 1.7364, 1.7364, 1.7364, 1.7361, 1.324, 1.4402, 1.4725, 1.39, 0.8937, 1.4816, 0.5317, 1.3259, 1.748, 1.7479, 1.7475, 1.7475, 1.7473, 1.7471, 1.747, 1.7468, 1.7467, 1.7461, 1.746, 1.7459, 1.7457, 1.7456, 1.7454, 1.7452, 1.7445, 1.7445, 1.7436, 1.7434, 1.7433, 1.743, 1.7427, 1.7425, 1.7425, 1.7424, 1.7423, 1.7423, 1.7422, 1.7419, 1.3934, 1.4575, 1.4258, 1.443, 0.9956, 0.4233, 0.7808, 0.7375, 0.2896, 1.8116, 1.811, 1.8107, 1.8107, 1.8105, 1.8102, 1.8099, 1.8098, 1.8098, 1.8096, 1.8095, 1.8088, 1.8087, 1.8086, 1.8084, 1.8082, 1.8082, 1.8082, 1.8081, 1.808, 1.8079, 1.8078, 1.8078, 1.8072, 1.8072, 1.807, 1.8068, 1.8065, 1.8064, 1.8061, 1.4723, 1.5667, 1.2601, 1.1729, 0.8557, -0.045, 1.0219, -0.1868, 1.3232, 1.8511, 1.851, 1.8509, 1.8509, 1.8503, 1.8501, 1.8501, 1.8497, 1.8495, 1.8491, 1.8491, 1.8485, 1.8484, 1.8483, 1.8482, 1.8482, 1.8481, 1.8481, 1.848, 1.8476, 1.8476, 1.8476, 1.8475, 1.8474, 1.8473, 1.8471, 1.8467, 1.8465, 1.8459, 1.8458, 1.3656, 1.5682, 1.4264, 1.5355, 1.3117, 0.4448, 0.3689, 0.1651, 0.1092, 1.9403, 1.9403, 1.9398, 1.9397, 1.9397, 1.9396, 1.9396, 1.9392, 1.9391, 1.9391, 1.9389, 1.9388, 1.9387, 1.9387, 1.9385, 1.9384, 1.9384, 1.9383, 1.9382, 1.9381, 1.9377, 1.9374, 1.9374, 1.9372, 1.9371, 1.937, 1.9364, 1.9362, 1.9361, 1.936, 1.6174, 1.1571, 0.0542, 1.1952]}, \"token.table\": {\"Topic\": [3, 1, 6, 1, 6, 5, 2, 3, 5, 5, 6, 5, 1, 1, 1, 1, 1, 4, 1, 6, 2, 5, 5, 4, 2, 4, 5, 3, 1, 4, 6, 1, 5, 2, 5, 1, 3, 4, 1, 5, 6, 1, 5, 4, 6, 1, 3, 3, 1, 5, 6, 1, 4, 2, 1, 6, 4, 6, 6, 6, 1, 6, 6, 5, 3, 4, 2, 2, 6, 1, 4, 4, 2, 3, 6, 4, 2, 3, 1, 3, 2, 5, 3, 4, 4, 3, 6, 6, 5, 1, 2, 3, 6, 1, 3, 1, 3, 5, 4, 6, 6, 4, 5, 1, 1, 5, 3, 1, 2, 2, 3, 2, 4, 2, 6, 1, 6, 3, 6, 2, 3, 4, 5, 4, 1, 5, 3, 4, 5, 6, 5, 1, 5, 6, 2, 2, 6, 2, 4, 3, 2, 2, 1, 2, 3, 4, 5, 1, 3, 3, 1, 3, 5, 1, 5, 3, 4, 1, 3, 2, 3, 4, 1, 5, 1, 2, 3, 4, 1, 3, 3, 6, 6, 2, 1, 2, 6, 1, 1, 5, 3, 4, 2, 5, 3, 2, 2, 2, 2, 5, 6, 4, 3, 4, 3, 6, 5, 6, 5, 1, 3, 4, 4, 4, 2, 4, 5, 1, 3, 1, 2, 5, 6, 1, 5, 1, 2, 5, 4, 1, 6, 5, 3, 3, 5, 4, 5, 4, 2, 5, 1, 2, 4, 5, 3, 2, 6, 3, 1, 2, 2, 4, 2, 2, 2, 1, 3, 6, 3, 6, 2, 4, 5, 4, 5, 1, 2, 3, 4, 5, 6, 4, 2, 3, 3, 4, 5, 3, 1, 5, 6, 6, 1, 6, 2, 3], \"Freq\": [0.9986106520243941, 0.7498535710789623, 0.24580262288480506, 0.9984602039956223, 0.9964569437842994, 0.9955489904665331, 0.9969949544465243, 0.24579226109234079, 0.7534566508251194, 0.9981433039427345, 0.9986058118085396, 0.9983285805308589, 0.9937455684027291, 0.9945234731308176, 0.9910273899486094, 0.9986544711715364, 0.9965612107817959, 0.9990031511869819, 0.8045308227333942, 0.1939493947660861, 0.2701917416509714, 0.7300373019607976, 0.995286881788733, 0.9955927079146719, 0.7033058382942126, 0.2961287740186158, 0.9979395435952119, 0.9977456789488484, 0.4226967779079425, 0.5755179206900448, 0.9989413738602253, 0.9963606325156081, 0.9973590052882823, 0.9944901196148127, 0.9946706308019735, 0.9986528970237496, 0.47158548510810133, 0.5280145175141989, 0.6213886460603313, 0.22704585144512104, 0.15146348247720576, 0.9969898563738822, 0.9985101104775431, 0.996668879277748, 0.9973532238918192, 0.995444686989126, 0.9914352322706012, 0.9993141594581075, 0.4155387588025891, 0.5829289301930314, 0.996486001498922, 0.9989484818061578, 0.9965266955491139, 0.9929245005830867, 0.65666021336936, 0.3402935518675226, 0.9957843673154363, 0.9974088709086799, 0.9983384905362973, 0.9988947614136428, 0.9916920756575923, 0.9975372267847491, 0.9993945757065813, 0.9980235107929087, 0.9949976524560153, 0.9958069687236618, 0.9980585054467132, 0.9945501132854417, 0.9984205630678108, 0.9966881921419264, 0.9992646386087427, 0.9972397504910449, 0.6596240861839118, 0.15391228677624608, 0.18532295754690856, 0.9981986259256418, 0.2983921369250452, 0.701424050373579, 0.9996080837450064, 0.9957863135751535, 0.771256970076778, 0.2271510254335716, 0.9967825261506398, 0.9969401991124736, 0.9951232512912067, 0.997277311328816, 0.9990055490517472, 0.9960170499173223, 0.9957433886892822, 0.9953001710233748, 0.16279576989125227, 0.37985679641292197, 0.4569705821508836, 0.7440186959622317, 0.2550319042816715, 0.7663750568492658, 0.2323515666494424, 0.9946882813236566, 0.9946593613766183, 0.9966431252855634, 0.9954202287163789, 0.7113740303670862, 0.2870456613761927, 0.9960851741398277, 0.998547897345551, 0.9981565879665196, 0.9983183942576055, 0.25987317151064965, 0.7398524455819208, 0.9975640895644305, 0.9971290091275146, 0.9982394550199211, 0.9975921891467162, 0.9951349825912981, 0.9977375992317795, 0.8050803069022674, 0.19238282658443792, 0.9941206472509132, 0.9970247908285451, 0.251049438930231, 0.7482257787724532, 0.9931164262524393, 0.9946671503620492, 0.9988344338348886, 0.9983798870679962, 0.9951169730983128, 0.9952985401687904, 0.994665194453163, 0.998713606281495, 0.9982852976387131, 0.9982258035025883, 0.9928473352184416, 0.9992359374251418, 0.9991261704422312, 0.997640920355356, 0.9953755690320518, 0.9987785084599263, 0.9958593241586443, 0.9966509335298264, 0.9935007695298782, 0.9959086942622737, 0.9978790303144782, 0.1398305706534642, 0.2138585198229452, 0.36355948369900687, 0.15792629156155955, 0.12337991528246839, 0.7963051588127587, 0.20166169606297132, 0.9983243851467454, 0.258961208553151, 0.7383893919556063, 0.9967146911119151, 0.7375045466876654, 0.2610357244139711, 0.21638551632044042, 0.7821163240497847, 0.7883934544181355, 0.2088905733928393, 0.998265931685287, 0.9984500313290381, 0.9969971190524316, 0.9954996262312239, 0.999207529164888, 0.1747965634532648, 0.1966461338849229, 0.1747965634532648, 0.4544710649784885, 0.9965040233622477, 0.998211496824905, 0.9969188987451231, 0.9974905981034199, 0.9986584277462612, 0.9983690067923796, 0.9942825923565506, 0.9969142829445959, 0.9993973904859329, 0.9993943497824858, 0.9987120018537685, 0.9909899924555939, 0.9998977617248611, 0.9997972539822757, 0.9983443083852728, 0.9988344523953886, 0.9977516483631215, 0.9955465189436086, 0.9979950410747304, 0.9970577626873521, 0.9973762040920411, 0.9955448685159121, 0.9971444708372625, 0.9984808325517911, 0.9990059597260315, 0.9974506607032929, 0.9932860427271472, 0.9963371037487908, 0.27536256860242364, 0.7235016508377405, 0.997754476238776, 0.9922212621532416, 0.9979790777909013, 0.9981784887356571, 0.993896606978376, 0.9983489073255392, 0.658522319328392, 0.15619715987795452, 0.1852285691746424, 0.9983887167915657, 0.9965736518452595, 0.6145304025886643, 0.19814298027391514, 0.18522061199518156, 0.9961246238422735, 0.9974961299732716, 0.9945379018693179, 0.9974067671433858, 0.997541620761964, 0.9938183225620422, 0.9971574463603344, 0.9963687945725364, 0.9967281490904893, 0.9951350235067155, 0.9976422352591424, 0.9996714137101521, 0.9996434573710045, 0.3847067911468039, 0.6152096075541791, 0.9955094909378069, 0.9964756601978262, 0.9992683719372573, 0.19105259253806137, 0.4282109369031312, 0.13560515243237795, 0.24499330872782946, 0.996664212009542, 0.763591688559917, 0.23569637744132746, 0.9956777078992692, 0.9971172254913808, 0.9955873879370051, 0.9974074939488045, 0.99388764497273, 0.994351243798822, 0.9992465354220776, 0.9988125191711817, 0.33474398544761325, 0.190457784823642, 0.47518255405494525, 0.9991797649537077, 0.995248671950105, 0.9996716763385526, 0.9992326456815142, 0.9984979345008639, 0.9929052369133602, 0.9954830613975567, 0.17236878891118548, 0.10754633838048325, 0.2659193709270853, 0.13701108862171155, 0.1753152639353083, 0.14143080115789577, 0.9984081547821151, 0.2743574234752651, 0.7254196281718873, 0.20894807965490714, 0.6145531754556093, 0.17699131453121547, 0.9947280126927225, 0.3453239562237268, 0.6541502585376289, 0.9980865205390281, 0.9991407896684157, 0.8163619709150485, 0.18328954147575682, 0.9976846159434783, 0.9998445090653914], \"Term\": [\"..\", \"access\", \"access\", \"account\", \"addition\", \"amount\", \"answer\", \"anyone\", \"anyone\", \"anything\", \"app\", \"application\", \"apps\", \"area\", \"background\", \"backup\", \"bank\", \"bit\", \"book\", \"book\", \"box\", \"box\", \"brand\", \"bug\", \"business\", \"business\", \"button\", \"call\", \"card\", \"card\", \"cd\", \"change\", \"child\", \"choice\", \"click\", \"code\", \"company\", \"company\", \"computer\", \"computer\", \"computer\", \"copy\", \"couple\", \"course\", \"crash\", \"credit\", \"curve\", \"customer\", \"data\", \"data\", \"daughter\", \"day\", \"deal\", \"design\", \"device\", \"device\", \"difference\", \"disc\", \"disk\", \"document\", \"dollar\", \"download\", \"drive\", \"driver\", \"edition\", \"effect\", \"email\", \"entry\", \"error\", \"everyone\", \"everything\", \"experience\", \"fact\", \"fact\", \"fact\", \"family\", \"feature\", \"feature\", \"file\", \"fix\", \"form\", \"form\", \"format\", \"friend\", \"fun\", \"functionality\", \"game\", \"graphic\", \"hand\", \"hardware\", \"help\", \"help\", \"help\", \"home\", \"home\", \"hour\", \"hour\", \"icon\", \"idea\", \"image\", \"improvement\", \"information\", \"information\", \"install\", \"installation\", \"instruction\", \"interface\", \"issue\", \"issue\", \"item\", \"job\", \"key\", \"kid\", \"kind\", \"language\", \"laptop\", \"laptop\", \"learning\", \"lesson\", \"line\", \"line\", \"link\", \"list\", \"lot\", \"machine\", \"map\", \"market\", \"memory\", \"menu\", \"message\", \"minute\", \"mistake\", \"money\", \"month\", \"mouse\", \"movie\", \"music\", \"name\", \"need\", \"none\", \"note\", \"nothing\", \"number\", \"number\", \"number\", \"number\", \"number\", \"office\", \"office\", \"one\", \"online\", \"online\", \"operating\", \"option\", \"option\", \"order\", \"order\", \"others\", \"others\", \"package\", \"page\", \"part\", \"password\", \"pc\", \"people\", \"people\", \"people\", \"people\", \"performance\", \"person\", \"phone\", \"photo\", \"picture\", \"piece\", \"player\", \"point\", \"price\", \"problem\", \"process\", \"processor\", \"product\", \"program\", \"project\", \"protection\", \"purchase\", \"quality\", \"question\", \"reason\", \"refund\", \"report\", \"result\", \"return\", \"review\", \"room\", \"sale\", \"school\", \"screen\", \"screen\", \"security\", \"server\", \"service\", \"set\", \"setting\", \"site\", \"software\", \"software\", \"software\", \"solution\", \"someone\", \"something\", \"something\", \"something\", \"son\", \"sound\", \"space\", \"speed\", \"star\", \"start\", \"state\", \"step\", \"store\", \"stuff\", \"suite\", \"support\", \"system\", \"tax\", \"tax\", \"text\", \"thanks\", \"thing\", \"time\", \"time\", \"time\", \"time\", \"today\", \"tool\", \"tool\", \"track\", \"transaction\", \"trial\", \"trouble\", \"try\", \"type\", \"update\", \"upgrade\", \"use\", \"use\", \"use\", \"user\", \"value\", \"version\", \"video\", \"virus\", \"voice\", \"waste\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"web\", \"website\", \"website\", \"week\", \"week\", \"week\", \"wife\", \"window\", \"window\", \"windows\", \"word\", \"work\", \"work\", \"world\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 5, 3, 2, 6, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el53801406001122826408113223435\", ldavis_el53801406001122826408113223435_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el53801406001122826408113223435\", ldavis_el53801406001122826408113223435_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el53801406001122826408113223435\", ldavis_el53801406001122826408113223435_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización del Modelo 2: con 6 tópicos\n",
    "vis2 = pyLDAvis.gensim_models.prepare(ldamodel2, corpus, dictionary)\n",
    "pyLDAvis.display(vis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OU8SmW48zq-l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "OU8SmW48zq-l",
    "outputId": "c8554391-2bb1-40b3-cd33-c3ee4b8b6d08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el362101338300619827688705410779\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el362101338300619827688705410779_data = {\"mdsDat\": {\"x\": [0.23971666282668094, -0.13055284867311687, -0.15021594875206554, 0.19189677490424473, -0.1064870300132608, 0.13338167532040274, -0.10980982487117887, -0.06792946074170622], \"y\": [-0.0082038020051375, 0.28118542301034594, 0.06170889230624896, 0.10838939445892358, -0.22726459820368813, -0.08373301093057893, -0.1227703857624627, -0.009311912873650994], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [15.373344240268974, 13.77623389293511, 13.153885961491948, 12.626895761380712, 12.586477535010362, 11.601131762612201, 10.461021168384251, 10.421009677916441]}, \"tinfo\": {\"Term\": [\"software\", \"product\", \"computer\", \"program\", \"version\", \"problem\", \"time\", \"year\", \"pc\", \"feature\", \"lot\", \"..\", \"issue\", \"game\", \"money\", \"price\", \"file\", \"review\", \"work\", \"upgrade\", \"option\", \"company\", \"everything\", \"day\", \"cd\", \"way\", \"update\", \"installation\", \"tax\", \"use\", \"money\", \"many_years\", \"set\", \"card\", \"difference\", \"new_version\", \"improvement\", \"waste\", \"life\", \"setting\", \"speed\", \"unit\", \"server\", \"letter\", \"64_bit\", \"difficulty\", \"default\", \"rest\", \"office\", \"glitch\", \"computer\", \"control\", \"virus\", \"star\", \"technical_support\", \"didn't_work\", \"keyboard\", \"title\", \"32_bit\", \"boot\", \"tax\", \"data\", \"year\", \"file\", \"client\", \"scan\", \"process\", \"program\", \"something\", \"system\", \"thing\", \"user\", \"level\", \"time\", \"page\", \"way\", \"word\", \"piece\", \"need\", \"item\", \"tech_support\", \"issue\", \"complaint\", \"area\", \"try\", \"customer_service\", \"project\", \"everyone\", \"icon\", \"stuff\", \"every_time\", \"practice\", \"example\", \"patch\", \"response\", \"works_great\", \"protection\", \"deduction\", \"number\", \"category\", \"space\", \"term\", \"reviewer\", \"camera\", \"instance\", \"sort\", \"software\", \"problem\", \"help\", \"person\", \"make_sure\", \"email\", \"return\", \"picture\", \"someone\", \"couple\", \"support\", \"company\", \"cd\", \"part\", \"windows\", \"installation\", \"user_friendly\", \"game\", \"highly_recommend\", \"product\", \"box\", \"fix\", \"anyone\", \"player\", \"task\", \"entry\", \"capability\", \"suite\", \"session\", \"tutorial\", \"instruction\", \"student\", \"basic\", \"first_time\", \"lesson\", \"gift\", \"training\", \"browser\", \"windows_8\", \"bug\", \"really_like\", \"mistake\", \"alternative\", \"order\", \"people\", \"market\", \"way\", \"support\", \"hour\", \"case\", \"point\", \"device\", \"reason\", \"disk\", \"result\", \"addition\", \"upgrade\", \"update\", \"download\", \"phone\", \"format\", \"place\", \"install\", \"solution\", \"driver\", \"would_recommend\", \"xp\", \"value\", \"sound\", \"graphic\", \"wife\", \"choice\", \"note\", \"answer\", \"amount\", \"several_years\", \"effect\", \"direction\", \"side\", \"color\", \"search\", \"opinion\", \"road\", \"pcs\", \"click\", \"child\", \"home\", \"hard_drive\", \"son\", \"bit\", \"crash\", \"functionality\", \"time\", \"website\", \"question\", \"subscription\", \"information\", \"site\", \"language\", \"program\", \"end\", \"version\", \"lot\", \"price\", \"review\", \"function\", \"type\", \"button\", \"key\", \"last_year\", \"apps\", \"none\", \"change\", \"mind\", \"minute\", \"go_back\", \"pc\", \"story\", \"mouse\", \"application\", \"mac\", \"font\", \"object\", \"everything\", \"success\", \"start\", \"desktop\", \"future\", \"customer_support\", \"backup\", \"downloads\", \"feature\", \"others\", \"older_version\", \"line\", \"business\", \"fun\", \"movie\", \"ability\", \"user\", \"thing\", \"window\", \"app\", \"interface\", \"music\", \"one\", \"operating_system\", \"date\", \"week\", \"photo\", \"kid\", \"day\", \"comment\", \"machine\", \"great_product\", \"hardware\", \"link\", \"state\", \"tablet\", \"edition\", \"daughter\", \"collection\", \"husband\", \"junk\", \"account\", \"character\", \"voice\", \"performance\", \"mess\", \"folder\", \"design\", \"vendor\", \"image\", \"text\", \"deal\", \"every_year\", \"work\", \"background\", \"security\", \"disc\", \"program\", \"bank\", \"window\", \"..\", \"laptop\", \"use\", \"transaction\", \"purchase\", \"even_though\", \"ease\", \"map\", \"world\", \"thanks\", \"learning_curve\", \"matter\", \"user_interface\", \"song\", \"chance\", \"book\", \"new_computer\", \"next_year\", \"internet\", \"_1\", \"seller\", \"release\", \"memory\", \"operation\", \"works_well\", \"course\", \"option\", \"tool\", \"location\", \"record\", \"video\", \"job\", \"month\", \"investment\", \"guy\", \"fact\", \"error\", \"document\", \"refund\", \"form\", \"screen\", \"nothing\", \"friend\", \"kind\", \"step\", \"cost\", \"much_better\", \"store\", \"password\", \"family\", \"menu\", \"copy\", \"name\", \"license\", \"10_years\", \"room\", \"online\", \"paper\", \"trial_version\", \"partition\", \"money_back\", \"effort\", \"half\", \"good_product\", \"template\", \"error_message\", \"headache\", \"lack\", \"drive\", \"look\", \"trouble\", \"service\", \"package\", \"experience\", \"anything\", \"code\", \"customer\", \"thing\"], \"Freq\": [4439.0, 4182.0, 2622.0, 4096.0, 1942.0, 1985.0, 2167.0, 1570.0, 1187.0, 1085.0, 1018.0, 865.0, 973.0, 900.0, 946.0, 775.0, 983.0, 730.0, 732.0, 641.0, 598.0, 647.0, 648.0, 601.0, 604.0, 1131.0, 579.0, 570.0, 600.0, 474.0, 944.1627002781458, 223.86796139281267, 213.45487240608793, 268.3134412588734, 183.98376231117265, 207.83061760524316, 168.53021178805895, 160.4828479412749, 150.39725659078746, 175.3128171099162, 143.81751307228896, 143.70864883610287, 143.3688987663695, 131.46191782080317, 120.20812947431901, 116.16925051149106, 125.36271962775724, 112.85847205006884, 288.3588246485252, 106.01172459248274, 2598.142168576703, 112.74135573528548, 307.67862840673433, 84.47688051683056, 92.0641173666669, 76.60398098099994, 76.06791187866686, 74.90604937642367, 73.38801164887528, 72.56319161529393, 592.4189201813284, 450.1226049014989, 1524.979643233551, 936.162507582125, 85.17458693936445, 94.99082584797812, 262.0006083139083, 3003.8345193794403, 343.47713647306847, 514.28298282324, 673.3146284098354, 432.25387422258643, 148.73511990329467, 628.0884688714284, 166.32756036146046, 255.87171987462978, 379.0941778341588, 307.5264898556703, 331.3061082299593, 290.1470090373098, 231.53609014405234, 969.4205176547574, 216.41960640143378, 205.8517452364542, 164.55477740185603, 202.9068834012512, 218.10945298808153, 158.65162856166643, 145.99440938171293, 155.78888809100758, 135.34984583251412, 123.60757192442996, 183.7522038382034, 114.24959787256805, 123.16633048609563, 110.50617656145309, 210.8149005014198, 108.1057613528156, 427.9038475106372, 98.65169371991853, 95.5882128510233, 98.89566367972567, 94.12010682033196, 93.75103049646734, 92.04340167390772, 88.58847348873755, 4385.533357673712, 1938.9553358921405, 511.22747912905095, 172.28409686817412, 112.75706137950918, 224.34609437278118, 225.3430580356968, 212.65578346605514, 187.55093478935694, 144.51258213243162, 153.85467252224848, 646.7963462023282, 602.9869357198986, 349.8550710740353, 306.6248486806359, 567.4957310190972, 198.75757476106787, 895.4032040447473, 172.8020801847964, 4156.538269651357, 302.52230530231424, 143.94731451846226, 359.1378119578806, 133.77931033934817, 130.42023818858024, 111.51226534426533, 106.8239340381025, 128.21144297234096, 101.36997841414956, 93.40631425282224, 267.62429952153843, 126.49142911460288, 81.24016893050658, 132.52199803013542, 189.3290229132483, 80.69589730714159, 79.27562305618768, 91.9641081352793, 74.9648836453538, 192.24321430930095, 74.23921477630617, 141.6345687683984, 105.12430962652365, 353.7449939468989, 494.9394874298094, 165.0981510207003, 754.6368274801679, 478.7809189447157, 249.27242147551627, 246.30325600311022, 245.10360932462638, 214.1465307576188, 249.25214264118068, 181.55197933372037, 156.20320290036491, 132.94008614312244, 639.7470187782569, 577.2092576813324, 332.53770827126505, 254.6902299483692, 260.7918236883438, 218.864939029962, 241.54048020973704, 218.160845456348, 209.74713961232308, 253.54447338846478, 157.88242483889638, 155.8199426933598, 152.00797442286265, 163.9647055564178, 181.40681877707505, 150.88529438825964, 210.2142386868292, 158.54097884826524, 150.73493124154868, 142.40195784263503, 132.44807090195863, 121.49238433477657, 121.40858445787677, 103.3933923944196, 170.46348452174914, 130.7827503861496, 93.10113233402434, 90.01919513247614, 89.98594889101427, 171.54146263694156, 266.7555717628676, 207.73455042460841, 160.70270420997545, 384.1629628023384, 137.9953679199308, 168.54941888351922, 1522.2245397264269, 346.12348164082255, 274.676294127217, 188.7730084631306, 260.9660774095616, 219.4282374844823, 174.87307108935445, 559.0283063231487, 210.2654647976977, 1940.4361313167342, 1017.1895877777348, 774.6257574033821, 728.9810041479325, 210.67200894956582, 206.22191187229657, 167.41395788284316, 157.36091424684082, 194.61607120747183, 153.8554784822893, 149.9121572484239, 236.15639054127305, 134.81138254387906, 118.69328117641128, 120.14261328354647, 1178.1739498929155, 112.2874524702095, 110.94370209379808, 417.05489238904295, 110.60497736779708, 101.02674575874646, 91.9482222029276, 641.6962180930093, 87.79544271489411, 90.74118829015276, 291.2130882959009, 83.75792347890159, 83.60898244307883, 194.54912124166538, 77.84692828512236, 1063.629461547519, 248.59948798031024, 137.93909725595597, 340.4527712979479, 405.6771849381089, 163.82984738243266, 170.7612915324637, 171.04853075867535, 158.2923690231804, 146.9421330495209, 121.18076699537913, 346.5262355073836, 361.42716167440324, 346.03640292319244, 346.3402096872931, 241.34011317311484, 220.1876918757289, 224.45392881933302, 226.21992757894276, 186.57542113825824, 598.6095403196642, 163.45229897122826, 315.636670844103, 158.45306913448934, 155.14602172856306, 152.14099258537897, 221.5856495739616, 140.5523492555351, 145.81622309632417, 122.54919702604738, 120.0754286621591, 119.32112550435251, 118.30575534521277, 428.7875425373747, 115.20330165529907, 113.5574576687754, 112.91031284084961, 108.32711009890694, 120.28312244719312, 107.14545267103348, 97.86438799096373, 279.42042884384375, 181.3071356607848, 123.8758097791784, 124.6359011777453, 663.1998035633416, 128.13636687604395, 159.20542119990483, 208.6658409420741, 485.2344664318492, 148.14730157000903, 156.72763141781041, 864.7468343448112, 424.9472125969886, 472.75977974133576, 240.7641831191728, 333.50897279978375, 174.73332148242156, 171.06314726664866, 163.57338138617763, 156.01344687446215, 255.8455419869967, 144.6482984744186, 137.32956069110813, 133.28483122837616, 129.7884316462424, 129.35375559544093, 364.54831460550446, 127.54477524532768, 133.48183901934473, 202.65064301824654, 111.3621041203508, 114.39705625163037, 101.10298889247501, 94.62035154848951, 92.36044073418024, 96.70615540801988, 285.99355939928586, 591.9139544636731, 464.4596487615686, 87.0264420873095, 82.04895560433928, 460.8688602015928, 218.33193928484258, 270.24834018404255, 107.70404007787668, 115.57284754069347, 198.84929969378152, 191.27052061861158, 134.1804953449269, 447.1569662101864, 312.36472986104087, 392.22454246890106, 427.1965552599479, 276.6239993287012, 239.95038916630108, 239.86042144078434, 302.92553809810903, 218.3628476374314, 201.42560542049273, 220.9481734398831, 187.0451602791915, 182.77856747452924, 195.98131172057606, 184.1268759981932, 142.00850477106857, 144.5965179211656, 119.2931243298851, 168.55729899737818, 111.10257554533702, 110.3992499300799, 105.94511725948935, 102.84813409684513, 98.35062413002328, 100.16546983638575, 94.35682632166, 91.85362041065937, 100.4033779971299, 88.02496790451355, 86.18812684188025, 249.7749426868629, 143.80675902675137, 163.85691375686946, 298.02862595588715, 312.9082556590706, 329.10634095881477, 291.83316016850677, 206.6745990513169, 269.9794154344826, 177.28977675019576], \"Total\": [4439.0, 4182.0, 2622.0, 4096.0, 1942.0, 1985.0, 2167.0, 1570.0, 1187.0, 1085.0, 1018.0, 865.0, 973.0, 900.0, 946.0, 775.0, 983.0, 730.0, 732.0, 641.0, 598.0, 647.0, 648.0, 601.0, 604.0, 1131.0, 579.0, 570.0, 600.0, 474.0, 946.9906489122362, 224.8258169195927, 214.3796582970289, 269.55433697428464, 184.93274354585117, 209.01647926119173, 169.4987287079941, 161.40892282616252, 151.32829588266628, 176.4269797234662, 144.73947234352107, 144.63072728226487, 144.29009917734248, 132.4551176755385, 121.13821568175295, 117.09144159538708, 126.36377498232974, 113.77952108195869, 290.74012577343785, 106.93252924487146, 2622.6048869801643, 113.86444101671866, 310.82038250489296, 85.39775019958266, 93.11611496744034, 77.52589199271226, 76.98959102572393, 75.83894201618531, 74.30889819064015, 73.48494825375927, 600.0552427543906, 456.80087509995406, 1570.8900777996193, 983.6032543368298, 86.32151517432503, 96.6896450141422, 282.0784088451208, 4096.88820907879, 458.5765514690824, 779.9087904236343, 1107.4938852230698, 859.6890993978894, 172.26290170362833, 2167.1255853461616, 214.18522699775104, 1131.6359454820185, 380.04557924679705, 308.44690079285715, 332.38526716051746, 291.10463349662245, 232.50793567226387, 973.7908727204862, 217.4399694896152, 206.92054008503035, 165.47956238191782, 204.1458399869627, 219.4483517823843, 159.62765479139583, 146.91477420599696, 156.77749151742748, 136.27303891221803, 124.52712502545236, 185.20142422183244, 115.18497381674928, 124.18527217749316, 111.4253716734463, 212.59539012409098, 109.0253568221038, 431.77779649045306, 99.57103810707223, 96.50761466321593, 99.84973809955632, 95.0401108873048, 94.67022293821438, 92.9634103453151, 89.52101663544921, 4439.691178618905, 1985.4327906863705, 527.1737488906585, 186.14064391751822, 118.11247553138912, 295.46764820568995, 297.9136971904576, 287.3048620651284, 252.32541216893136, 185.47881829960394, 683.1338068729672, 647.929333672315, 604.5261878723476, 350.9535505632518, 307.7179894420596, 570.243141598889, 199.73552552027857, 900.0633170215904, 173.77549775160261, 4182.529117050037, 304.4546136088171, 144.8700294773467, 361.51571882827585, 134.70736861323428, 131.34546046676746, 112.43511604798466, 107.74640425877361, 129.31913704448934, 102.29975782512852, 94.32970458418195, 270.51735410785403, 127.87334657372598, 82.16289347243867, 134.0392684314182, 191.56413251207988, 81.67017589414698, 80.23627493671893, 93.08703338801678, 75.88888221699966, 194.63771066444917, 75.1762709997326, 143.47288718828563, 106.48417507038917, 366.62133383023877, 601.0746339083513, 182.4368371946269, 1131.6359454820185, 683.1338068729672, 336.92284832879255, 332.77019512470787, 345.9822833348297, 287.7035002333747, 404.3949409372069, 300.74699998923563, 217.50143476218622, 159.41479900579722, 641.5562251234375, 579.252864587049, 333.7266847118389, 255.63040942316758, 261.7883291320505, 219.8291574015716, 242.60686036606438, 219.13261401604126, 210.76755294345605, 255.0019110876831, 158.80994941200092, 156.7454533144073, 152.93089112942195, 164.96026290478224, 182.5249881830194, 151.81672303730574, 211.51439192898036, 159.53512101090936, 151.6836041834039, 143.32610850861897, 133.3685551738477, 122.4232901920223, 122.4024626169962, 104.31081893956463, 172.02174329188648, 132.0473094163774, 94.02372417592042, 90.93682859162931, 90.90336273064753, 173.29988560124733, 271.611079244034, 211.06305450867677, 162.87758364634274, 392.9253020782319, 139.80953891005674, 172.60982159948236, 2167.1255853461616, 401.4367461831579, 361.4118439988414, 230.04918137958387, 403.93948143243927, 319.96701698892844, 204.7314199539172, 4096.88820907879, 340.1651993013264, 1942.5892275855604, 1018.436872145153, 775.597188582584, 730.0436493405565, 211.59162866332264, 207.20244269956592, 168.33287510106132, 158.2798890834065, 195.77411346367606, 154.77746238390495, 150.84974006319277, 237.6400464808795, 135.74626449401418, 119.6121046622475, 121.10771771384466, 1187.884779363343, 113.2214563849427, 111.87465171671674, 420.72010955325686, 111.60423950889846, 101.94559480942651, 92.86720494769433, 648.1508647006851, 88.71450133429632, 91.7076037538228, 294.3974003162929, 84.6768111120176, 84.53403989224694, 196.70533502061895, 78.76557516541125, 1085.6033310217645, 251.65268225257375, 139.73626789744446, 351.90816839044476, 434.1147998098985, 169.09889277259958, 180.73380719190638, 225.39636163806668, 859.6890993978894, 1107.4938852230698, 425.9761286103667, 347.45498668312274, 362.40971870925574, 346.9840875545971, 347.52695662654776, 242.31823206918202, 221.12462796128602, 225.4316294609874, 227.25927695383126, 187.5083365943597, 601.6772982677071, 164.3677660964949, 317.4636203885107, 159.37029613201318, 156.06440561741059, 153.0755206838051, 223.00158611760855, 141.48944343053873, 146.85118859587524, 123.46423395004396, 120.996642207036, 120.23684341638597, 119.2209650511729, 432.2005067181048, 116.12898176807805, 114.47277408344777, 113.85540019924734, 109.24339063660118, 121.3007427036944, 108.08754514490215, 98.78369169365024, 282.9931922954808, 183.42912354528755, 125.20821471241985, 126.11089196950863, 732.254437652889, 131.38089886336027, 174.4641295917703, 287.96574375244995, 4096.88820907879, 227.9391137418404, 425.9761286103667, 865.7309710421032, 426.1117650892514, 474.7967410234983, 241.85541863239197, 335.0559437861232, 175.65070518246142, 172.02201658966266, 164.50423500117572, 156.92818136614042, 257.4122734187673, 145.5636215414903, 138.24388941947237, 134.2136224316635, 130.70458969133617, 130.27085030205683, 367.254885331803, 128.49873898904028, 134.48092802521157, 204.234040015397, 112.2790158581762, 115.39056968307115, 102.01712855927595, 95.5431037706606, 93.27543547640612, 97.67893308528565, 288.93547669499884, 598.09677550909, 469.35899383962294, 87.95572766327336, 82.98544019851802, 467.46957155730064, 221.10417899104291, 278.287069545607, 109.14540150974915, 117.72959169959016, 335.86568691859287, 362.1141958938873, 268.73339559748723, 448.39816309934037, 313.3641811951776, 393.48485318934013, 428.6399764674711, 277.6270887814833, 240.8627977356185, 240.80060449781934, 304.21877113439785, 219.3419143900476, 202.3544352134568, 221.99316614252794, 187.9574928737164, 183.70028919258516, 197.04985160600663, 185.22547169079002, 142.9370347966008, 145.62164254954445, 120.20648953061503, 169.91133424727724, 112.0169748300838, 111.32795976206494, 106.86580010023845, 103.76027268385168, 99.26361025428852, 101.09542293715585, 95.26884971920866, 92.76630913593819, 101.4429727344173, 88.93735841209346, 87.10302761398397, 253.04750129872434, 145.5037895985089, 166.32986224277448, 306.71286107467955, 326.6234960061771, 384.4184100723514, 383.31547074099547, 273.2154467690202, 415.8088582906907, 1107.4938852230698], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4865, -4.9257, -4.9733, -4.7446, -5.1219, -5.0, -5.2097, -5.2586, -5.3235, -5.1702, -5.3682, -5.369, -5.3714, -5.4581, -5.5475, -5.5817, -5.5056, -5.6106, -4.6726, -5.6732, -2.4742, -5.6117, -4.6077, -5.9003, -5.8143, -5.9981, -6.0051, -6.0205, -6.041, -6.0523, -3.9526, -4.2273, -3.007, -3.495, -5.8921, -5.783, -4.7684, -2.3291, -4.4977, -4.094, -3.8246, -4.2678, -5.3346, -3.8941, -5.2228, -4.7921, -4.2893, -4.4985, -4.424, -4.5567, -4.7823, -3.3504, -4.8499, -4.8999, -5.1238, -4.9143, -4.8421, -5.1604, -5.2435, -5.1786, -5.3192, -5.41, -5.0135, -5.4887, -5.4135, -5.522, -4.8761, -5.544, -4.1682, -5.6355, -5.667, -5.633, -5.6825, -5.6864, -5.7048, -5.7431, -1.841, -2.6572, -3.9903, -5.0779, -5.5018, -4.8139, -4.8095, -4.8674, -4.993, -5.2537, -5.1911, -3.7088, -3.779, -4.3233, -4.4552, -3.8396, -4.8888, -3.3836, -5.0287, -1.8484, -4.4687, -5.2114, -4.2971, -5.2847, -5.3101, -5.4667, -5.5097, -5.3272, -5.5621, -5.6439, -4.5913, -5.3407, -5.7834, -5.2941, -4.9374, -5.7902, -5.8079, -5.6595, -5.8638, -4.9221, -5.8736, -5.2276, -5.5257, -4.3123, -3.9764, -5.0743, -3.5546, -4.0096, -4.6623, -4.6743, -4.6792, -4.8142, -4.6624, -4.9793, -5.1297, -5.291, -3.6789, -3.7818, -4.3332, -4.5999, -4.5762, -4.7515, -4.6529, -4.7547, -4.7941, -4.6044, -5.0781, -5.0913, -5.116, -5.0403, -4.9392, -5.1234, -4.7918, -5.074, -5.1244, -5.1813, -5.2538, -5.3401, -5.3408, -5.5014, -5.0014, -5.2664, -5.6063, -5.6399, -5.6403, -4.9951, -4.5536, -4.8037, -5.0604, -4.1889, -5.2127, -5.0127, -2.812, -4.2932, -4.5244, -4.8994, -4.5756, -4.7489, -4.9759, -3.8138, -4.7916, -2.5661, -3.212, -3.4844, -3.5451, -4.7865, -4.8078, -5.0163, -5.0782, -4.8657, -5.1007, -5.1267, -4.6723, -5.2329, -5.3602, -5.3481, -3.065, -5.4157, -5.4277, -4.1035, -5.4308, -5.5214, -5.6155, -3.6726, -5.6618, -5.6287, -4.4627, -5.7088, -5.7106, -4.8661, -5.782, -3.1673, -4.6209, -5.2099, -4.3065, -4.1312, -5.0379, -4.9965, -4.9948, -5.0723, -5.1467, -5.3395, -4.2073, -4.1652, -4.2087, -4.2078, -4.569, -4.6608, -4.6416, -4.6337, -4.8264, -3.6606, -4.9587, -4.3006, -4.9898, -5.0109, -5.0304, -4.6544, -5.1097, -5.0729, -5.2467, -5.2671, -5.2734, -5.282, -3.9943, -5.3085, -5.3229, -5.3286, -5.3701, -5.2654, -5.3811, -5.4717, -4.4225, -4.855, -5.236, -5.2298, -3.5582, -5.2021, -4.985, -4.7145, -3.8706, -5.057, -5.0007, -3.1894, -3.8998, -3.7932, -4.468, -4.1421, -4.7885, -4.8098, -4.8545, -4.9019, -4.4072, -4.9775, -5.0294, -5.0593, -5.0859, -5.0892, -4.0531, -5.1033, -5.0578, -4.6403, -5.239, -5.2121, -5.3357, -5.4019, -5.4261, -5.3801, -4.2958, -3.5684, -3.8109, -5.4856, -5.5445, -3.8187, -4.5658, -4.3525, -5.2724, -5.2019, -4.6592, -4.6981, -5.0526, -3.8451, -4.2038, -3.9761, -3.8907, -4.3253, -4.4675, -4.4679, -4.2345, -4.5618, -4.6425, -4.55, -4.7166, -4.7397, -4.6699, -4.7323, -4.9921, -4.974, -5.1664, -4.8207, -5.2375, -5.2439, -5.285, -5.3147, -5.3594, -5.3411, -5.4009, -5.4278, -5.3388, -5.4703, -5.4914, -4.4274, -4.9795, -4.849, -4.2508, -4.2021, -4.1516, -4.2718, -4.6168, -4.3496, -4.7702], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8695, 1.8683, 1.8682, 1.8679, 1.8674, 1.8668, 1.8668, 1.8668, 1.8664, 1.8662, 1.8661, 1.8661, 1.8661, 1.865, 1.8648, 1.8646, 1.8646, 1.8644, 1.8643, 1.8639, 1.8632, 1.8626, 1.8624, 1.8617, 1.8612, 1.8606, 1.8605, 1.8602, 1.8601, 1.8599, 1.8597, 1.8578, 1.8429, 1.8231, 1.8592, 1.8548, 1.7987, 1.5622, 1.5835, 1.4561, 1.3749, 1.185, 1.7257, 0.6341, 1.6197, 0.3858, 1.9797, 1.9792, 1.979, 1.9789, 1.978, 1.9777, 1.9775, 1.977, 1.9766, 1.9761, 1.9761, 1.9761, 1.9759, 1.9759, 1.9754, 1.9748, 1.9744, 1.9741, 1.974, 1.9739, 1.9738, 1.9738, 1.9732, 1.9729, 1.9727, 1.9726, 1.9725, 1.9725, 1.9723, 1.9718, 1.97, 1.9585, 1.9515, 1.9049, 1.9358, 1.7069, 1.703, 1.6814, 1.6856, 1.7327, 0.4915, 2.0267, 2.0259, 2.0253, 2.0249, 2.0236, 2.0235, 2.0233, 2.0228, 2.0222, 2.0221, 2.0221, 2.0219, 2.0215, 2.0214, 2.0202, 2.0199, 2.0199, 2.0193, 2.0186, 2.0177, 2.0176, 2.0172, 2.0171, 2.0167, 2.0165, 2.0164, 2.0163, 2.0162, 2.0161, 2.0159, 2.0156, 2.0156, 1.9927, 1.8342, 1.9286, 1.6233, 1.673, 1.7271, 1.7276, 1.6837, 1.7332, 1.5445, 1.5237, 1.6974, 1.8468, 2.0665, 2.0658, 2.0658, 2.0657, 2.0655, 2.0649, 2.0649, 2.0649, 2.0645, 2.0636, 2.0635, 2.0634, 2.0633, 2.0633, 2.0632, 2.0632, 2.0632, 2.0631, 2.0631, 2.0629, 2.0624, 2.0617, 2.0612, 2.0605, 2.0602, 2.0597, 2.0595, 2.0592, 2.0592, 2.0591, 2.0513, 2.0534, 2.0559, 2.0468, 2.0563, 2.0455, 1.7161, 1.9211, 1.7949, 1.8716, 1.6325, 1.6921, 1.9117, 0.0776, 1.5883, 2.0714, 2.0713, 2.0713, 2.0711, 2.0682, 2.0678, 2.0671, 2.0667, 2.0666, 2.0666, 2.0663, 2.0663, 2.0656, 2.0648, 2.0645, 2.0643, 2.0643, 2.0642, 2.0638, 2.0636, 2.0635, 2.0626, 2.0625, 2.0621, 2.062, 2.0617, 2.0616, 2.0615, 2.0615, 2.0608, 2.0521, 2.0603, 2.0596, 2.0395, 2.0048, 2.0409, 2.0158, 1.7966, 0.3804, 0.0527, 0.8154, 2.1514, 2.1514, 2.1513, 2.1506, 2.15, 2.1498, 2.1497, 2.1495, 2.1491, 2.149, 2.1485, 2.1483, 2.1483, 2.1482, 2.1479, 2.1477, 2.1474, 2.147, 2.1466, 2.1464, 2.1464, 2.1464, 2.1461, 2.1461, 2.146, 2.1457, 2.1456, 2.1456, 2.1453, 2.1447, 2.1414, 2.1424, 2.1434, 2.1423, 2.055, 2.1291, 2.0625, 1.832, 0.0207, 1.7232, 1.1542, 2.2564, 2.2548, 2.2532, 2.253, 2.2529, 2.2523, 2.2519, 2.2518, 2.2517, 2.2514, 2.2512, 2.2509, 2.2506, 2.2505, 2.2504, 2.2501, 2.2501, 2.2501, 2.2497, 2.2493, 2.2489, 2.2485, 2.2478, 2.2477, 2.2475, 2.2473, 2.2471, 2.247, 2.2469, 2.2462, 2.2433, 2.2449, 2.2282, 2.2442, 2.239, 1.7334, 1.6192, 1.563, 2.2586, 2.2582, 2.2581, 2.258, 2.2577, 2.2576, 2.2574, 2.2571, 2.2569, 2.2567, 2.2566, 2.2565, 2.2563, 2.2559, 2.2554, 2.2548, 2.2543, 2.2537, 2.2533, 2.2531, 2.253, 2.2527, 2.2525, 2.2521, 2.2521, 2.2517, 2.2515, 2.251, 2.251, 2.2508, 2.2483, 2.2496, 2.2464, 2.2326, 2.2184, 2.106, 1.9887, 1.9822, 1.8295, 0.4293]}, \"token.table\": {\"Topic\": [7, 8, 1, 1, 7, 1, 2, 5, 6, 7, 1, 3, 3, 5, 4, 4, 3, 4, 1, 2, 4, 8, 6, 1, 2, 5, 5, 2, 6, 8, 5, 7, 2, 4, 6, 3, 4, 7, 8, 1, 3, 7, 1, 1, 3, 3, 3, 4, 3, 4, 5, 6, 7, 5, 2, 3, 1, 2, 3, 5, 6, 8, 2, 3, 8, 7, 5, 6, 1, 4, 4, 4, 1, 3, 8, 6, 4, 6, 3, 2, 1, 2, 3, 4, 5, 1, 8, 8, 2, 3, 4, 5, 1, 7, 4, 5, 1, 2, 3, 5, 6, 7, 8, 2, 5, 1, 2, 4, 8, 6, 6, 2, 6, 7, 6, 8, 2, 1, 6, 2, 5, 7, 3, 6, 1, 1, 1, 4, 2, 3, 6, 1, 3, 8, 1, 2, 7, 4, 5, 6, 8, 4, 7, 6, 4, 8, 1, 2, 4, 1, 2, 3, 4, 5, 3, 1, 2, 4, 6, 7, 8, 7, 2, 4, 6, 2, 4, 5, 2, 3, 1, 2, 3, 5, 6, 7, 8, 1, 2, 7, 8, 8, 2, 3, 5, 7, 8, 1, 2, 3, 4, 6, 8, 3, 6, 3, 6, 5, 8, 4, 8, 2, 5, 6, 5, 1, 4, 5, 3, 5, 6, 3, 1, 5, 8, 4, 6, 3, 7, 8, 3, 4, 6, 8, 6, 8, 2, 4, 3, 4, 7, 2, 3, 6, 2, 4, 6, 1, 2, 4, 5, 6, 7, 4, 3, 4, 2, 3, 4, 8, 6, 1, 7, 7, 2, 5, 2, 6, 7, 6, 5, 1, 6, 8, 8, 3, 4, 7, 5, 7, 1, 3, 1, 1, 8, 8, 1, 5, 6, 6, 7, 6, 8, 5, 5, 5, 6, 2, 3, 8, 1, 7, 1, 2, 3, 7, 7, 8, 6, 5, 5, 2, 3, 1, 6, 8, 3, 6, 7, 5, 2, 3, 4, 5, 8, 6, 8, 2, 7, 1, 7, 5, 4, 8, 1, 2, 3, 5, 1, 8, 2, 5, 6, 8, 6, 7, 4, 7, 8, 3, 6, 5, 7, 1, 3, 5, 7, 8, 1, 2, 4, 5, 6, 8, 3, 8, 8, 2, 1, 3, 5, 7, 4, 1, 2, 3, 5, 6, 2, 8, 4, 6, 2, 3, 4, 2, 4, 3, 2, 3, 5, 7, 8, 2, 5, 1, 2, 3, 4, 5, 6, 8, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 2, 4, 2, 6, 2, 7, 2, 4, 3, 1, 3, 8, 7, 8, 7, 2, 1, 1, 3, 4, 2, 5, 7, 8, 5, 2, 4, 8, 1, 6, 8, 4, 8, 3, 6, 7, 7, 1, 2, 8, 3, 1, 1, 4, 4, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 4, 1, 2, 3, 4, 5, 1, 2, 4, 6, 7, 2, 4, 2, 1, 1, 5, 1, 6, 8, 8, 5, 3, 2, 2, 4, 8, 5, 3, 2, 3, 4, 5, 1, 2, 4, 6, 7, 8, 6, 3, 1, 2, 4, 6, 2, 1, 8, 2, 6, 8, 4, 7, 1, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7, 3, 7, 8, 2, 3, 8, 2, 3, 5, 1, 2, 4, 1, 4, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 3, 7, 4, 6, 5, 6, 2, 7, 1, 6, 6, 1, 1, 3, 5, 6, 7, 8, 3, 4, 5, 6, 6, 4, 3, 4, 5, 6, 7, 3, 3, 2, 1, 6, 7, 2, 7, 7, 4, 4, 1, 2, 3, 4, 5, 8], \"Freq\": [0.9991556602841374, 0.9957311115390493, 0.9823857139251054, 0.9906039916854711, 0.9886085939710073, 0.02661977308060803, 0.20852155579809625, 0.7586635327973289, 0.9925948566270602, 0.004627481849077204, 0.16309652655933465, 0.8343014627842887, 0.9860620127882093, 0.009391066788459137, 0.9954932229684011, 0.9966457479236013, 0.9930411910264105, 0.002766131451327049, 0.21914064631312108, 0.015652903308080078, 0.0026088172180133463, 0.7617746276598971, 0.9986905161803371, 0.004753754228966395, 0.0023768771144831976, 0.9911577567394935, 0.9949769018568314, 0.9955512387283927, 0.9742664352838954, 0.015222913051310866, 0.9913305095642668, 0.005083746202893676, 0.3421966450582121, 0.004387136475105284, 0.6492961983155819, 0.9858464883198307, 0.9772849902232693, 0.0025450129953730975, 0.01781509096761168, 0.0027229045546842273, 0.0027229045546842273, 0.993860162459743, 0.9934007131353669, 0.003284561820714809, 0.9952222316765871, 0.9883223973473763, 0.9864481006509754, 0.010275501048447662, 0.006910614430362011, 0.0023035381434540034, 0.9352364862423255, 0.0023035381434540034, 0.05298137729944208, 0.9920819085382988, 0.9929204461824095, 0.9930725831278696, 0.9942336784793304, 0.1171979960085803, 0.7392488979002757, 0.012020307282931313, 0.0751269205183207, 0.05409138277319091, 0.9942650180421121, 0.9974753982491329, 0.0016541880567978986, 0.9902445535658198, 0.9930986106711965, 0.9902782083258704, 0.005770344259204765, 0.9924992125832196, 0.9946203354876456, 0.990062383794049, 0.98469077875132, 0.2415676008823807, 0.757643839131103, 0.9917630589671186, 0.9874335284403808, 0.9916786233154016, 0.9985656866821452, 0.9933776228308201, 0.9906181494962072, 0.0007626005769793743, 0.00800730605828343, 0.00038130028848968716, 0.00038130028848968716, 0.9924081564973236, 0.9946721522627393, 0.9959937674790639, 0.7817604259575425, 0.21026659732651143, 0.005391451213500293, 0.005391451213500293, 0.0069219606497515915, 0.9898403729144776, 0.9870571140984817, 0.007152587783322332, 0.036074267541249795, 0.2525198727887486, 0.019239609355333226, 0.012024755847083266, 0.026454462863583186, 0.0048099023388333065, 0.6493368157424964, 0.9943871499559537, 0.9936825461917156, 0.9851119481799023, 0.004378275325244011, 0.004378275325244011, 0.004378275325244011, 0.9949140538000909, 0.9962399317179436, 0.0016620204931765022, 0.9955502754127249, 0.0016620204931765022, 0.9903503558836384, 0.007986696418416438, 0.9905952445193381, 0.9892075479501903, 0.9899382935985438, 0.0033967691254257887, 0.9884598154989046, 0.0033967691254257887, 0.7438213293422253, 0.2537334441214133, 0.9932165631481971, 0.9949563093697363, 0.9906787244181467, 0.9883740243397326, 0.06250743496585386, 0.21183075182872693, 0.7257807726590808, 0.219453560641876, 0.6051598187397187, 0.17622785930332466, 0.4279334161067524, 0.07070204266111561, 0.49863545876786797, 0.9978225154141738, 0.9902803329525176, 0.007903654411663153, 0.9879568014578941, 0.9963582964610214, 0.9940588035768668, 0.9942037336979433, 0.989738546900626, 0.9872701561926726, 0.010153395873349723, 0.7581202252101127, 0.23014363979592706, 0.09995143556669944, 0.25575808512655446, 0.0029397481049029247, 0.6173471020296142, 0.023517984839223398, 0.9961300698280156, 0.22920946193537806, 0.01104623912941581, 0.22368634237067017, 0.008284679347061858, 0.527457918429605, 0.9857755279097048, 0.9962954593220363, 0.9906581747763175, 0.007929529197539751, 0.9911911496924689, 0.996068007187, 0.007714253381902052, 0.9905101342362236, 0.9935128780630035, 0.0053995265112119755, 0.02861465453210134, 0.09624929251706814, 0.0026013322301910308, 0.013006661150955154, 0.0026013322301910308, 0.0052026644603820615, 0.8558383037328491, 0.35133091767305435, 0.04466070987369335, 0.5924987509909985, 0.00893214197473867, 0.9949058009921439, 0.005526880609654016, 0.002763440304827008, 0.9801001614453122, 0.010132614451032362, 0.0009211467682756693, 0.9516031955700217, 0.029483432341378877, 0.0010166700807372026, 0.0010166700807372026, 0.012200040968846432, 0.0040666803229488105, 0.992246537573801, 0.007460500282509781, 0.9939944136100094, 0.9892767127826103, 0.9907245152554736, 0.9956466588173077, 0.9969886773231482, 0.9977412550618328, 0.023654797109636375, 0.9698466814950913, 0.005913699277409094, 0.997203912711197, 0.017380239271442458, 0.9790868122912585, 0.9920071256447971, 0.9943744879656407, 0.0033330988423429294, 0.0011110329474476432, 0.9917941171693374, 0.9912792744036195, 0.9908534506738703, 0.9866813788247847, 0.9941788229002974, 0.9914018097144144, 0.008494041179992314, 0.9853087768791083, 0.9891644655580817, 0.004737920629111762, 0.9854874908552466, 0.004737920629111762, 0.004737920629111762, 0.9931797028720312, 0.9894604648841695, 0.9693198894582796, 0.0284536171073859, 0.9955373584789777, 0.9830232284453644, 0.014726939751990478, 0.2582193532778739, 0.7390415973125356, 0.9897132743904236, 0.9937734362596214, 0.010600961725141498, 0.9858894404381593, 0.9970576256719111, 0.1287321551624491, 0.6461363941807542, 0.0024756183685086365, 0.13120777353095775, 0.09159787963481955, 0.9974985853031991, 0.9943127038936486, 0.0017536379257383572, 0.9896366716567682, 0.9906942971693773, 0.007393241023652069, 0.0036966205118260345, 0.9961101520282719, 0.004896343429942486, 0.9939577162783247, 0.9895057281946337, 0.9950801831741328, 0.003080743601158306, 0.9962053730187869, 0.009045509719113094, 0.9859605593833273, 0.9897588058388151, 0.9919137605489978, 0.9871464309325493, 0.9972889920331415, 0.9964178870970122, 0.9873365180958775, 0.14164899557931843, 0.8547784215993353, 0.9973909073150832, 0.9960458844635776, 0.9961280055035615, 0.005220183898136259, 0.9866147567477529, 0.9890142585573556, 0.8649569845070225, 0.13351684995745985, 0.9934444225883501, 0.9912224222514461, 0.9661611481060236, 0.028416504356059517, 0.9929739211142276, 0.9891339917403419, 0.0068726732324932375, 0.9896649454790262, 0.9985891397057076, 0.9945858731571726, 0.0031499672270359795, 0.9953896437433696, 0.9567151944924696, 0.016933012291902118, 0.025399518437853173, 0.996326859028436, 0.9969348205462789, 0.08770158618202152, 0.005481349136376345, 0.904422607502097, 0.9910022104796398, 0.9943156151597896, 0.9961878710389453, 0.988618161434248, 0.9945025043834846, 0.9948825859725826, 0.00696995801504752, 0.9897340381367478, 0.9968419446214475, 0.0010559766362515334, 0.9926727960115509, 0.017967058290434067, 0.010780234974260441, 0.9702211476834397, 0.9921818597573694, 0.033197994847909626, 0.011065998282636543, 0.005532999141318272, 0.9461428531654243, 0.9938820886387391, 0.997163882754588, 0.9933838921850026, 0.9958323448799298, 0.996118724643027, 0.9951368463157323, 0.9889878211954789, 0.9943669769478104, 0.992840241672591, 0.9961740001924538, 0.002316006075643843, 0.9912506003755648, 0.004632012151287686, 0.9906618816816682, 0.9905753436470165, 0.00687899544199317, 0.0071563382581100714, 0.9875746796191899, 0.9956062210501022, 0.9946364128601866, 0.994559913804564, 0.9863261375313682, 0.9920686803767051, 0.9898063728835513, 0.008359851122327292, 0.9655739241948125, 0.03273131946423093, 0.9894589549818056, 0.007947461485797636, 0.003061629099644099, 0.009184887298932297, 0.018369774597864593, 0.009184887298932297, 0.958289908188603, 0.7750301098111823, 0.004668856083199893, 0.004668856083199893, 0.004668856083199893, 0.2100985237439952, 0.9909212435737849, 0.9972829721718973, 0.991898249024231, 0.9955261409178232, 0.9897124270858934, 0.0008418324886155698, 0.005050994931693419, 0.9916786715891412, 0.0016836649772311396, 0.9896980287729592, 0.0049910607281714444, 0.1680323778484386, 0.8235250201482882, 0.0033273738187809625, 0.9924869597950524, 0.9240324755522811, 0.0698396638498817, 0.997533902853772, 0.9944588534703158, 0.7413727650446639, 0.0034806233100688443, 0.25408550163502563, 0.9985511256825457, 0.9962281736809966, 0.9947488498920557, 0.011561285628400048, 0.7081287447395029, 0.017341928442600074, 0.17052896301890072, 0.09249028502720039, 0.9957669863064402, 0.9992300274016267, 0.0005036685223952088, 0.9766132649243098, 0.007555027835928131, 0.009569701925508966, 0.0035256796567664614, 0.002014674089580835, 0.0005036685223952088, 0.9288197599833132, 0.03190602228950313, 0.0354511358772257, 0.0004781795760481429, 0.00023908978802407145, 0.993896248816065, 0.0014345387281444286, 0.0009563591520962858, 0.0009563591520962858, 0.0016736285161685, 0.0004781795760481429, 0.7332394360537037, 0.0009763507803644523, 0.0007322630852733392, 0.1364450215559322, 0.00951942010855341, 0.11838253211918984, 0.00048817539018222614, 0.9934000334446779, 0.004556880887360908, 0.9924956504317438, 0.0047037708551267475, 0.0029845762134526753, 0.9968484552931935, 0.2379556769597061, 0.7609047809758044, 0.9843531611226529, 0.18298943065039597, 0.6157347058371432, 0.1978264115139416, 0.9881251434449145, 0.9968818714829779, 0.9900298256416328, 0.9904556139652446, 0.9931488454640517, 0.27126257840326423, 0.7172366479815122, 0.009195341640788618, 0.7552522832011865, 0.01678338407113748, 0.0033566768142274956, 0.2215406697390147, 0.9985704288483309, 0.9890560850824539, 0.9891120652272292, 0.9899631913773861, 0.9825250675613189, 0.010342369132224409, 0.9962264031834902, 0.9882471642642524, 0.005813218613319132, 0.07451388448971591, 0.91136212568191, 0.011463674536879371, 0.9879490179579626, 0.9910589902931811, 0.02608302753255636, 0.9715927755877244, 0.9872946148381863, 0.9935644160085499, 0.9919117828480494, 0.9907476137989246, 0.9885422026076013, 0.003125322132920351, 0.20939658290566351, 0.009375966398761053, 0.6844455471095569, 0.08438369758884948, 0.006250644265840702, 0.0006757226751373364, 0.9879065510507857, 0.0027028907005493454, 0.0013514453502746727, 0.0031533724839742363, 0.00022524089171244545, 0.002027168025412009, 0.0018019271336995636, 0.9948313763283162, 0.01585254519398945, 0.7450696241175041, 0.01585254519398945, 0.003963136298497362, 0.22589876901434966, 0.7479667220253091, 0.24859535367605023, 0.9884724244778855, 0.006139580276260158, 0.9946092964830073, 0.994179951758469, 0.9939129948008074, 0.994740159468376, 0.9948910111972357, 0.9836324704536596, 0.9922841321236321, 0.004484273037737997, 0.9955086143778353, 0.9966752388371741, 0.9933066195854414, 0.9892117940897187, 0.9853499839965016, 0.9950407962909583, 0.0347751726479735, 0.8215634538083739, 0.139100690591894, 0.9919460592851228, 0.9897993670957183, 0.22543167744095738, 0.7011803473650557, 0.02195763091957377, 0.05123447214567213, 0.6590514253862984, 0.15386414600458326, 0.061545658401833306, 0.05769905475171873, 0.012822012167048607, 0.055134652318309, 0.9965407777522356, 0.9897563230431714, 0.9865758313894314, 0.0033330264573967275, 0.006666052914793455, 0.0016665132286983637, 0.9978154050063055, 0.9880137292258101, 0.9917393594390476, 0.9914898314634628, 0.9867571544892225, 0.005451696986128301, 0.003884818647994942, 0.9945135738867051, 0.6076782987063132, 0.000902939522594819, 0.024379367110060116, 0.1327321098214384, 0.07313810133018034, 0.000902939522594819, 0.15982029549928298, 0.2897847749325001, 0.0004614407244148091, 0.0050758479685629, 0.7023127825593394, 0.0009228814488296182, 0.0004614407244148091, 0.0004614407244148091, 0.0009228814488296182, 0.9889378465220906, 0.002130565330855669, 0.006391695992567006, 0.9885823135170303, 0.9845920696381536, 0.9964630991638349, 0.988071641976525, 0.006012149511315073, 0.006012149511315073, 0.9859925198556719, 0.9971019842268436, 0.9859036494384937, 0.9941967735326875, 0.9956390506075937, 0.0017263617689882342, 0.9961107407062112, 0.001558709838420782, 0.9975742965893004, 0.0021061644143646483, 0.0021061644143646483, 0.9962157679944788, 0.5025072439589672, 0.013958534554415754, 0.13260607826694967, 0.015121745767283734, 0.18378737163314077, 0.09770974188091028, 0.004652844851471918, 0.04769165972758716, 0.9963175027659068, 0.9909575316597878, 0.9952441790262838, 0.9920665883182354, 0.9986671255308162, 0.0005147768688303176, 0.01283506000189906, 0.9861604434792445, 0.9909260052955229, 0.006434584449970928, 0.9958699866650987, 0.991271097028013, 0.2262211632831769, 0.6671756964015569, 0.009720440609824008, 0.04595117379189531, 0.0008836764190749098, 0.05036955588726986, 0.09465999403717336, 0.86190415623321, 0.007473157423987371, 0.034874734645274394, 0.9936493851177385, 0.9916450443405027, 0.15728580899255928, 0.1784137534840971, 0.28405347594178615, 0.36856525390793743, 0.009390197551794583, 0.9976667290613674, 0.9882870561400818, 0.9972488056593916, 0.08603566842412763, 0.9054229867491527, 0.006828227652708543, 0.9961824522812188, 0.9930493396698666, 0.994085311140038, 0.996070966357038, 0.994899882438098, 0.9707872126457768, 0.000636581778784116, 0.001273163557568232, 0.000636581778784116, 0.008275563124193508, 0.01846087158473936], \"Term\": [\"..\", \"10_years\", \"32_bit\", \"64_bit\", \"_1\", \"ability\", \"ability\", \"ability\", \"account\", \"account\", \"addition\", \"addition\", \"alternative\", \"alternative\", \"amount\", \"answer\", \"anyone\", \"anyone\", \"anything\", \"anything\", \"anything\", \"anything\", \"app\", \"application\", \"application\", \"application\", \"apps\", \"area\", \"background\", \"background\", \"backup\", \"backup\", \"bank\", \"bank\", \"bank\", \"basic\", \"bit\", \"bit\", \"bit\", \"book\", \"book\", \"book\", \"boot\", \"box\", \"box\", \"browser\", \"bug\", \"bug\", \"business\", \"business\", \"business\", \"business\", \"business\", \"button\", \"camera\", \"capability\", \"card\", \"case\", \"case\", \"case\", \"case\", \"case\", \"category\", \"cd\", \"cd\", \"chance\", \"change\", \"character\", \"child\", \"child\", \"choice\", \"click\", \"client\", \"code\", \"code\", \"collection\", \"color\", \"comment\", \"company\", \"complaint\", \"computer\", \"computer\", \"computer\", \"computer\", \"computer\", \"control\", \"copy\", \"cost\", \"couple\", \"couple\", \"couple\", \"couple\", \"course\", \"course\", \"crash\", \"crash\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer_service\", \"customer_support\", \"data\", \"data\", \"data\", \"data\", \"date\", \"daughter\", \"day\", \"day\", \"day\", \"deal\", \"deal\", \"deduction\", \"default\", \"design\", \"desktop\", \"desktop\", \"desktop\", \"device\", \"device\", \"didn't_work\", \"difference\", \"difficulty\", \"direction\", \"disc\", \"disc\", \"disc\", \"disk\", \"disk\", \"disk\", \"document\", \"document\", \"document\", \"download\", \"downloads\", \"drive\", \"drive\", \"driver\", \"ease\", \"edition\", \"effect\", \"effort\", \"email\", \"email\", \"email\", \"end\", \"end\", \"end\", \"end\", \"end\", \"entry\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error_message\", \"even_though\", \"every_time\", \"every_year\", \"every_year\", \"everyone\", \"everything\", \"everything\", \"example\", \"example\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"fact\", \"fact\", \"fact\", \"fact\", \"family\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"file\", \"file\", \"file\", \"file\", \"file\", \"file\", \"first_time\", \"first_time\", \"fix\", \"folder\", \"font\", \"form\", \"format\", \"friend\", \"fun\", \"fun\", \"fun\", \"function\", \"functionality\", \"functionality\", \"future\", \"game\", \"game\", \"game\", \"gift\", \"glitch\", \"go_back\", \"good_product\", \"graphic\", \"great_product\", \"guy\", \"guy\", \"half\", \"hard_drive\", \"hard_drive\", \"hard_drive\", \"hard_drive\", \"hardware\", \"headache\", \"help\", \"help\", \"highly_recommend\", \"home\", \"home\", \"hour\", \"hour\", \"husband\", \"icon\", \"image\", \"image\", \"improvement\", \"information\", \"information\", \"information\", \"information\", \"information\", \"install\", \"installation\", \"installation\", \"instance\", \"instruction\", \"instruction\", \"instruction\", \"interface\", \"internet\", \"internet\", \"investment\", \"issue\", \"issue\", \"item\", \"job\", \"job\", \"junk\", \"key\", \"keyboard\", \"kid\", \"kind\", \"lack\", \"language\", \"language\", \"laptop\", \"last_year\", \"learning_curve\", \"lesson\", \"lesson\", \"letter\", \"level\", \"level\", \"license\", \"life\", \"line\", \"line\", \"link\", \"location\", \"look\", \"look\", \"lot\", \"mac\", \"machine\", \"machine\", \"make_sure\", \"make_sure\", \"make_sure\", \"many_years\", \"map\", \"market\", \"market\", \"market\", \"matter\", \"memory\", \"menu\", \"mess\", \"mind\", \"minute\", \"mistake\", \"mistake\", \"money\", \"money\", \"money_back\", \"month\", \"month\", \"month\", \"mouse\", \"movie\", \"movie\", \"movie\", \"movie\", \"much_better\", \"music\", \"name\", \"need\", \"new_computer\", \"new_version\", \"next_year\", \"none\", \"note\", \"nothing\", \"number\", \"number\", \"number\", \"object\", \"office\", \"office\", \"older_version\", \"older_version\", \"one\", \"online\", \"operating_system\", \"operation\", \"opinion\", \"option\", \"option\", \"order\", \"order\", \"others\", \"others\", \"package\", \"package\", \"package\", \"package\", \"package\", \"page\", \"page\", \"page\", \"page\", \"page\", \"paper\", \"part\", \"partition\", \"password\", \"patch\", \"pc\", \"pc\", \"pc\", \"pc\", \"pcs\", \"people\", \"people\", \"people\", \"people\", \"performance\", \"person\", \"person\", \"phone\", \"photo\", \"picture\", \"picture\", \"picture\", \"piece\", \"place\", \"player\", \"point\", \"point\", \"point\", \"point\", \"point\", \"practice\", \"price\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"project\", \"project\", \"protection\", \"protection\", \"purchase\", \"purchase\", \"question\", \"question\", \"really_like\", \"reason\", \"reason\", \"reason\", \"record\", \"refund\", \"release\", \"response\", \"rest\", \"result\", \"result\", \"result\", \"return\", \"return\", \"return\", \"return\", \"review\", \"reviewer\", \"road\", \"room\", \"scan\", \"scan\", \"screen\", \"search\", \"search\", \"security\", \"security\", \"security\", \"seller\", \"server\", \"service\", \"service\", \"session\", \"set\", \"setting\", \"several_years\", \"side\", \"site\", \"site\", \"site\", \"site\", \"site\", \"site\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"solution\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"something\", \"something\", \"son\", \"son\", \"song\", \"sort\", \"sound\", \"space\", \"speed\", \"star\", \"start\", \"state\", \"state\", \"step\", \"store\", \"story\", \"student\", \"stuff\", \"subscription\", \"subscription\", \"subscription\", \"success\", \"suite\", \"support\", \"support\", \"support\", \"support\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tablet\", \"task\", \"tax\", \"tax\", \"tax\", \"tax\", \"tech_support\", \"technical_support\", \"template\", \"term\", \"text\", \"text\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"title\", \"tool\", \"tool\", \"tool\", \"training\", \"transaction\", \"trial_version\", \"trouble\", \"trouble\", \"trouble\", \"try\", \"tutorial\", \"type\", \"unit\", \"update\", \"update\", \"upgrade\", \"upgrade\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user_friendly\", \"user_interface\", \"value\", \"vendor\", \"version\", \"version\", \"video\", \"video\", \"virus\", \"virus\", \"voice\", \"waste\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"website\", \"website\", \"website\", \"website\", \"week\", \"wife\", \"window\", \"window\", \"window\", \"window\", \"window\", \"windows\", \"windows_8\", \"word\", \"work\", \"work\", \"work\", \"works_great\", \"works_well\", \"world\", \"would_recommend\", \"xp\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [7, 4, 6, 8, 1, 2, 5, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el362101338300619827688705410779\", ldavis_el362101338300619827688705410779_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el362101338300619827688705410779\", ldavis_el362101338300619827688705410779_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el362101338300619827688705410779\", ldavis_el362101338300619827688705410779_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "6      0.239717 -0.008204       1        1  15.373344\n",
       "3     -0.130553  0.281185       2        1  13.776234\n",
       "5     -0.150216  0.061709       3        1  13.153886\n",
       "7      0.191897  0.108389       4        1  12.626896\n",
       "0     -0.106487 -0.227265       5        1  12.586478\n",
       "1      0.133382 -0.083733       6        1  11.601132\n",
       "4     -0.109810 -0.122770       7        1  10.461021\n",
       "2     -0.067929 -0.009312       8        1  10.421010, topic_info=            Term         Freq        Total Category  logprob  loglift\n",
       "76      software  4439.000000  4439.000000  Default  30.0000  30.0000\n",
       "47       product  4182.000000  4182.000000  Default  29.0000  29.0000\n",
       "16      computer  2622.000000  2622.000000  Default  28.0000  28.0000\n",
       "95       program  4096.000000  4096.000000  Default  27.0000  27.0000\n",
       "13       version  1942.000000  1942.000000  Default  26.0000  26.0000\n",
       "...          ...          ...          ...      ...      ...      ...\n",
       "297   experience   329.106341   384.418410   Topic8  -4.1516   2.1060\n",
       "104     anything   291.833160   383.315471   Topic8  -4.2718   1.9887\n",
       "1412        code   206.674599   273.215447   Topic8  -4.6168   1.9822\n",
       "179     customer   269.979415   415.808858   Topic8  -4.3496   1.8295\n",
       "238        thing   177.289777  1107.493885   Topic8  -4.7702   0.4293\n",
       "\n",
       "[367 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "201       7  0.999156        ..\n",
       "488       8  0.995731  10_years\n",
       "2237      1  0.982386    32_bit\n",
       "2233      1  0.990604    64_bit\n",
       "2037      7  0.988609        _1\n",
       "...     ...       ...       ...\n",
       "31        2  0.000637      year\n",
       "31        3  0.001273      year\n",
       "31        4  0.000637      year\n",
       "31        5  0.008276      year\n",
       "31        8  0.018461      year\n",
       "\n",
       "[612 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[7, 4, 6, 8, 1, 2, 5, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización del Modelo 3: con 8 tópicos\n",
    "vis3 = pyLDAvis.gensim_models.prepare(ldamodel3, corpus, dictionary)\n",
    "pyLDAvis.display(vis3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J_8O14ttpxob",
   "metadata": {
    "id": "J_8O14ttpxob"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<b>Análisis </b> de los resultados de los tres experimentos realizados (con diferente número de tópicos) y los resultados de la visualización de cada modelo LDA. En cada caso, ¿cuál resulta ser el mejor modelo? o ¿en ambos casos se confirma cuál es el mejor modelo?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47228c3d",
   "metadata": {},
   "source": [
    "Para analizar los resultados consideramos varios aspectos, como la coherencia y la distintividad de los temas, la interpretación:\n",
    "\n",
    "**Modelo de 4 Tópicos**: Los tópicos parecen tener una buena separación, con poca superposición en el espacio de tópicos, lo que sugiere una distintividad clara entre ellos. Los términos más relevantes dentro de cada tópico parecen coherentes y relacionados, lo que indica temas bien definidos. La distribución del tamaño de los tópicos parece desequilibrada, con un tópico claramente más predominante que los demás.\n",
    "\n",
    "**Modelo de 6 Tópicos**: La separación entre tópicos sigue siendo buena, pero comienza a haber más superposición en comparación con el modelo de 4 tópicos. Los términos relevantes todavía mantienen una buena coherencia temática, aunque comienza a verse una mezcla más amplia de términos entre los tópicos.\n",
    "La distribución del tamaño de los tópicos es más equilibrada que en el modelo de 4 tópicos.\n",
    "\n",
    "**Modelo de 8 Tópicos**: La separación entre tópicos se reduce más aún, con varias áreas de superposición en el espacio de tópicos. Algunos tópicos comienzan a mostrar una mezcla de términos que pueden ser menos intuitivos o que representan temas más especializados o nichos. La distribución del tamaño de los tópicos se mantiene relativamente equilibrada, pero algunos tópicos parecen tener una relevancia mucho menor que otros.\n",
    "\n",
    "Conclusión:\n",
    "\n",
    "Si el objetivo es tener una vista amplia y claramente diferenciada de los temas, el modelo de 4 tópicos podría ser preferible debido a su claridad y distintividad.  \n",
    "Si es importante capturar una variedad más amplia de temas, a pesar de una superposición ligeramente mayor, el modelo de 6 tópicos podría ser una mejor opción.  \n",
    "El modelo de 8 tópicos podría ser útil para análisis muy detallados o especializados, pero corre el riesgo de fragmentar demasiado los temas y de perder claridad en la interpretación general de los datos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YF6i9Y4Lodhq",
   "metadata": {
    "id": "YF6i9Y4Lodhq"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Toma como base los resultados de cada modelo LDA y calcula las métricas de calidad denominadas perplejidad y coherencia, las cuales están definidas en el paquete gensim. Luego de los cálculos confirma o no tu decisión anterior de cuál es el mejor modelo.\n",
    "<br>\n",
    "\n",
    "Tanto la perplejidad como la coherencia son métricas complementarias. Una perplejidad baja indica que el modelo puede generalizar bien a nuevos datos, mientras que una alta coherencia indica que los temas generados son interpretables y distintos entre sí. Cuando evaluamos el rendimiento de un modelo LDA, se sugiere elegir modelos que evidencien un equilibrio entre ambas métricas.\n",
    "<br>\n",
    "\n",
    "Como indica el enunciado este ejercicio es opcional, pero ha sido incorporado en la práctica por sí alguien quiere aprender un poco más el tema. Las métricas mencionadas nos puede ayudar a tener un criterio adicional para elegir el modelo de temas máas apropiado. Para una introducción básica de este tema podrían revisar el siguiente post (https://medium.com/@iqra.bismi/topic-modelling-using-lda-fe81a2a806e0)\n",
    "\n",
    "<b>Salida esperada:</b>\n",
    "<br>\n",
    "- Valores de coherencia y perplejidad (puedes usar el valor absoluto calculado para una mejor interpretación) de cada uno de los 3 modelos de LDA.\n",
    "<br>\n",
    "- Respuesta a la pregunta ¿según los valores de las métricas cuál es el mejor modelo?\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ggDQcxhpoexZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggDQcxhpoexZ",
    "outputId": "dec95007-4311-4a05-a578-ccc3590895ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplejidad del modelo con 4 tópicos:  -6.31072358545357\n",
      "Coherencia del modelo con 4 tópicos:  0.31015649215167546\n",
      "\n",
      "Perplejidad del modelo con 6 tópicos:  -6.3885176682881335\n",
      "Coherencia del modelo con 6 tópicos:  nan\n",
      "\n",
      "Perplejidad del modelo con 8 tópicos:  -6.446560464772546\n",
      "Coherencia del modelo con 8 tópicos:  nan\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "for num_topics, ldamodel in ldamodels.items():\n",
    "    # Calcula la perplejidad del modelo LDA\n",
    "    perplexity = ldamodel.log_perplexity(corpus)\n",
    "    print(f\"\\nPerplejidad del modelo con {num_topics} tópicos: \", perplexity)\n",
    "\n",
    "    # Calcula la coherencia para el modelo LDA\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, texts=nps_in_sentences, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Coherencia del modelo con {num_topics} tópicos: \", coherence_lda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90868ee4",
   "metadata": {},
   "source": [
    "* La perplejidad es más baja en el modelo con 4 tópicos. Una perplejidad más baja es generalmente mejor, ya que sugiere que el modelo puede predecir mejor los datos no vistos.\n",
    "\n",
    "* La coherencia es más alta en el modelo con 4 tópicos. Una puntuación de coherencia más alta indica que los tópicos son más interpretables y significativos.\n",
    "\n",
    "Dado que ambos, la perplejidad más baja y la coherencia más alta se encuentran en el modelo con 4 tópicos, este sería considerado el mejor modelo según estas métricas ya que sugiere que tiene un buen equilibrio entre la generalización y la interpretación significativa de los tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ed8c2",
   "metadata": {
    "id": "1f6ed8c2"
   },
   "source": [
    "# 4. Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32290b6e",
   "metadata": {
    "id": "32290b6e"
   },
   "source": [
    "\n",
    "Creamos un clasificador automático de opiniones positivas y negativas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ae89",
   "metadata": {
    "id": "3a45ae89"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Primer paso</i>: crea dos listas. Una con los textos y otra con las etiquetas de valoración (0 y 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec874492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec874492",
    "outputId": "af574a50-1e93-4c0a-e323-00c2f0e4c8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We run a top of the line system utilizing Windows 10 Pro. I personally tried to get this to work. When you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset. I let it just sit once to see if it ever started and after about 20 minutes all these windows started popping up saying that the file required a version of Adobe that doesn't exist and that the wrong volume was inserted. Now we are taking all our computers off line to do a complete virus scan just in case. Avoid. At the least it's a headache and a waste of time and money.\",\n",
       " \"I don't really know why there were so many complaints about Windows 8. I got this in 2013 and had no problem since them. All you have to do is add on a free start button utility and it works pretty much like windows has always worked. Of course now that Windows 10 is out maybe that's a moot point\",\n",
       " 'I have used both WordPerfect Office and Microsoft Office for years. I love WordPerfect because it is easier to edit and manipulate graphics within the documents. I also like the Reveal Codes feature much easier than Word I highly recommend WordPerfect and most businesses tend to have both.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions = df['text'].to_list()\n",
    "labels = df['sentiment'].to_list()\n",
    "opinions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720bc32",
   "metadata": {
    "id": "9720bc32"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Segundo paso</i>: Vectorizamos las opiniones con un vectorizador tf.idf. Usad 'word' como analyzer.\n",
    "<br>\n",
    "<b>Salida esperada:</b> Imprime la matriz de los vectores correpsondientes a las primeras 5 opiniones.\n",
    "</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbcfdd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbcfdd4",
    "outputId": "bea88594-50f2-4cf6-d252-9cf0af7cc9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12763)\t0.07165332291380765\n",
      "  (0, 20039)\t0.057763230501279406\n",
      "  (0, 21698)\t0.09629326653285565\n",
      "  (0, 9402)\t0.1384751303360284\n",
      "  (0, 11463)\t0.09525096532570564\n",
      "  (0, 2102)\t0.056567443722624185\n",
      "  (0, 2304)\t0.11934999607995561\n",
      "  (0, 10120)\t0.04149480035578295\n",
      "  (0, 17390)\t0.11126997886416871\n",
      "  (0, 21450)\t0.09017893676841064\n",
      "  (0, 4429)\t0.09397503608003152\n",
      "  (0, 6295)\t0.055498707063947376\n",
      "  (0, 13655)\t0.08298912674988412\n",
      "  (0, 4501)\t0.08846860750126671\n",
      "  (0, 19573)\t0.11718885953581729\n",
      "  (0, 1913)\t0.0536343982622682\n",
      "  (0, 13468)\t0.06311963864529029\n",
      "  (0, 10434)\t0.14717668072103804\n",
      "  (0, 21687)\t0.04420337138219251\n",
      "  (0, 21533)\t0.14512200577645984\n",
      "  (0, 22252)\t0.10123563829229933\n",
      "  (0, 7471)\t0.13539362115998319\n",
      "  (0, 6323)\t0.08166907852168598\n",
      "  (0, 1219)\t0.11344048781205482\n",
      "  (0, 21330)\t0.05916927239222464\n",
      "  :\t:\n",
      "  (3, 19349)\t0.20381568254448895\n",
      "  (3, 12029)\t0.17649313193418234\n",
      "  (3, 12289)\t0.12690388088914883\n",
      "  (3, 3273)\t0.600015368162822\n",
      "  (3, 3371)\t0.22262046268344182\n",
      "  (3, 8244)\t0.08061858977063022\n",
      "  (3, 11636)\t0.13690847018871796\n",
      "  (3, 6354)\t0.1507944722605338\n",
      "  (3, 10120)\t0.09818048439957638\n",
      "  (3, 1913)\t0.12690388088914883\n",
      "  (3, 9942)\t0.12536726555841826\n",
      "  (3, 2558)\t0.12009301783760007\n",
      "  (3, 9360)\t0.0919643583122116\n",
      "  (3, 1618)\t0.06646780263329471\n",
      "  (3, 22378)\t0.210288366288504\n",
      "  (4, 15685)\t0.3904560277548758\n",
      "  (4, 10557)\t0.39584614097898835\n",
      "  (4, 13022)\t0.10641011987018688\n",
      "  (4, 11080)\t0.5219446969040115\n",
      "  (4, 20147)\t0.22168472808158765\n",
      "  (4, 7497)\t0.37088907942581417\n",
      "  (4, 12337)\t0.4002370671386882\n",
      "  (4, 13413)\t0.11802449309734524\n",
      "  (4, 5914)\t0.1921539638720235\n",
      "  (4, 8244)\t0.09672165325607349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear el vectorizador y transformar las opiniones\n",
    "vectorizer = TfidfVectorizer(analyzer='word')\n",
    "X_tfidf = vectorizer.fit_transform(opinions)\n",
    "\n",
    "# Imprimir la matriz de los vectores correspondientes a las primeras 5 opiniones\n",
    "print(X_tfidf[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122e026",
   "metadata": {
    "id": "3122e026"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Tercer paso</i>: Preparamos el corpus de entrenamiento y evaluación, y entrenamos al clasificador con Logistic Regression.\n",
    "<br>\n",
    "<b>Salida esperada:</b> Tiempo de ejecución que conlleva realizar el entrenamiento (fit()).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b251acc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b251acc",
    "outputId": "7c039b39-af3f-4d68-e2dd-b4f92d8c2a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento: 0.34036993980407715 segundos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar al clasificador\n",
    "clf = LogisticRegression()\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Tiempo de ejecución para realizar el entrenamiento\n",
    "print(f\"Tiempo de entrenamiento: {end_time - start_time} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78a34d",
   "metadata": {
    "id": "8c78a34d"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Cuarto paso</i>: Utilizar el modelo entrenado para predecir la categoría 1 (positivo) o 0 (negativo) de las opiniones del conjunto de test y mostrar las palabras más informativas para cada categoría.\n",
    "<br>\n",
    "<b>Salida esperada:</b> Imprime las 10 palabras más informativas de cada categoría.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160fc014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "160fc014",
    "outputId": "79a85e34-a6f4-4827-e4ce-0b628690df27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más informativas para la categoría negativa:\n",
      "['not' 'waste' 'useless' 'work' 'disappointed' 'money' 'cannot' 'support'\n",
      " 'intuit' 'tried']\n",
      "\n",
      "Palabras más informativas para la categoría positiva:\n",
      "['easy' 'great' 'love' 'best' 'works' 'good' 'excellent' 'price' 'well'\n",
      " 'and']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Realizar predicciones en el conjunto de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Identificar las palabras más informativas para cada categoría\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "\n",
    "# Las 10 palabras más informativas para la categoría negativa\n",
    "print('Palabras más informativas para la categoría negativa:')\n",
    "print(feature_names[sorted_coef_index[:10]])\n",
    "\n",
    "# Las 10 palabras más informativas para la categoría positiva\n",
    "print('\\nPalabras más informativas para la categoría positiva:')\n",
    "print(feature_names[sorted_coef_index[:-11:-1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af23001",
   "metadata": {
    "id": "4af23001"
   },
   "source": [
    "\n",
    "**Mostramos sobre qué aspectos se hacen valoraciones negativas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb0943",
   "metadata": {
    "id": "d8eb0943"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Primer paso</i>: Elige dos palabras más informativas de la categoría 0 y toma un conjunto de opiniones en las que aparezcan estas palabras. Preprocesa las opiniones quitando los caracteres de salto de línea.\n",
    "<br>\n",
    "<b>Salida esperada:</b> Imprime las 3 primeras opiniones en las que aparezcan los términos seleccionados.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6ab69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eb6ab69",
    "outputId": "1f5b79ec-88e7-4c0f-e7e6-02d429bf9ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tres primeras opiniones con términos negativos seleccionados:\n",
      "------------\n",
      "\n",
      "We run a top of the line system utilizing Windows 10 Pro. I personally tried to get this to work. When you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset. I let it just sit once to see if it ever started and after about 20 minutes all these windows started popping up saying that the file required a version of Adobe that doesn't exist and that the wrong volume was inserted. Now we are taking all our computers off line to do a complete virus scan just in case. Avoid. At the least it's a headache and a waste of time and money.\n",
      "\n",
      "Did not meet expectations. Too juvenile for my intended purposes.\n",
      "\n",
      "This product took me more than 8 hours over 3 days to install and use properly. Like one of the reviewers below I installed this product but got a message saying that my subscription service had already expired. I did have an earlier version of Norton Antivirus and Family something or other on my computer so I uninstalled both of those installed the new version of AntiVirus again and still got the same message. I then tried to contact Norton through several different toll free and toll call phone numbers that I had for them. All numbers were always busy. Except once when they were closed. I tried contacting them through their website and got no response within 24 hours. I contacted them again with a more detailed description of my problem. To their credit I did get responses to both of my inquiries eventually. One rep was helpful enough to provide a password that I could use to renew my service but the other just said that I should try calling customer service because the problem was complex. However I had resolved the problem myself by that time. Using their Knowledge base I got the instructions for manually un installing all previous Norton software. This requires editing the registry something that can render a computer nearly useless if done improperly. I don't recommend it. Once I did the manual un install however I was able to install this version of AntiVirus a third time and get it to work properly. Bottom line: The software seems to work ok now although it still has its problems. For example the scan scheduler does not seem to work. I've set it to automatically scan my hard drive once a week but that is not actually happening. I am able to update my virus definitions whenever I want to which I do manually 2 or 3 times a week. I manually scan my hard drive when I feel like it which is not very often. Only once has a virus been found and that was with the old software earlier this year. I still think that it is better to have a program that doesn't work perfectly than to not have any virus scanning software at all but be aware that this product may not work exactly as advertised. Also be prepared to wait 24 hours or more to get customer service from Norton/Symantec if you need it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Las palabras más informativas para la categoría negativa\n",
    "negative_features = feature_names[sorted_coef_index[:2]]\n",
    "\n",
    "# Filtrar las opiniones que contienen estas palabras\n",
    "selected_opinions = []\n",
    "for opinion in opinions:\n",
    "    if any(negative_word in opinion.lower() for negative_word in negative_features):\n",
    "        # Preprocesar las opiniones quitando los caracteres de salto de línea\n",
    "        preprocessed_opinion = opinion.replace('\\n', ' ')\n",
    "        selected_opinions.append(preprocessed_opinion)\n",
    "\n",
    "\n",
    "print(\"Tres primeras opiniones con términos negativos seleccionados:\\n------------\")\n",
    "for opinion in selected_opinions[:3]:\n",
    "    print(\"\\n\"+opinion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c24dbb",
   "metadata": {
    "id": "b5c24dbb"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Segundo paso</i>: Utiliza el diccionario de opiniones (archivo AFINN-111) para extraer la polaridad de cada opinión como la media de los valores de las opinion words del texto.\n",
    "<br>\n",
    "<b>Salida esperada:</b> Lista de las 3 primeras opioniones y el respectivo puntaje de polaridad.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f6b09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895f6b09",
    "outputId": "fe58e7af-186d-4138-9195-694047655468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Puntaje de polaridad de las tres primeras opiniones:\n",
      "--------------\n",
      "\n",
      "Opinión: We run a top of the line system utilizing Windows 10 Pro. I personally tried to get this to work. When you click the application file as per the directions on the back of the case our entire system would bog down and have to be reset. I let it just sit once to see if it ever started and after about 20 minutes all these windows started popping up saying that the file required a version of Adobe that doesn't exist and that the wrong volume was inserted. Now we are taking all our computers off line to do a complete virus scan just in case. Avoid. At the least it's a headache and a waste of time and money.\n",
      "Polaridad: -0.008264462809917356\n",
      "\n",
      "Opinión: Did not meet expectations. Too juvenile for my intended purposes.\n",
      "Polaridad: 0.0\n",
      "\n",
      "Opinión: This product took me more than 8 hours over 3 days to install and use properly. Like one of the reviewers below I installed this product but got a message saying that my subscription service had already expired. I did have an earlier version of Norton Antivirus and Family something or other on my computer so I uninstalled both of those installed the new version of AntiVirus again and still got the same message. I then tried to contact Norton through several different toll free and toll call phone numbers that I had for them. All numbers were always busy. Except once when they were closed. I tried contacting them through their website and got no response within 24 hours. I contacted them again with a more detailed description of my problem. To their credit I did get responses to both of my inquiries eventually. One rep was helpful enough to provide a password that I could use to renew my service but the other just said that I should try calling customer service because the problem was complex. However I had resolved the problem myself by that time. Using their Knowledge base I got the instructions for manually un installing all previous Norton software. This requires editing the registry something that can render a computer nearly useless if done improperly. I don't recommend it. Once I did the manual un install however I was able to install this version of AntiVirus a third time and get it to work properly. Bottom line: The software seems to work ok now although it still has its problems. For example the scan scheduler does not seem to work. I've set it to automatically scan my hard drive once a week but that is not actually happening. I am able to update my virus definitions whenever I want to which I do manually 2 or 3 times a week. I manually scan my hard drive when I feel like it which is not very often. Only once has a virus been found and that was with the old software earlier this year. I still think that it is better to have a program that doesn't work perfectly than to not have any virus scanning software at all but be aware that this product may not work exactly as advertised. Also be prepared to wait 24 hours or more to get customer service from Norton/Symantec if you need it.\n",
      "Polaridad: 0.01728395061728395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar el diccionario de opiniones AFINN\n",
    "afinn = {}\n",
    "with open('AFINN-111.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, score = line.strip().split('\\t')\n",
    "        afinn[word] = int(score)\n",
    "\n",
    "# Función para calcular la polaridad\n",
    "def calculate_polarity(text, afinn_dict):\n",
    "    # Tokenizar y calcular la suma de las polaridades\n",
    "    words = text.split()\n",
    "    scores = [afinn_dict.get(word, 0) for word in words]\n",
    "    return sum(scores) / len(words) if words else 0\n",
    "\n",
    "# Calcular la polaridad para las opiniones seleccionadas\n",
    "polarity_scores = [calculate_polarity(opinion, afinn) for opinion in selected_opinions]\n",
    "\n",
    "# Imprimir opiniones y puntaje de polaridad\n",
    "print(\"\\nPuntaje de polaridad de las tres primeras opiniones:\\n--------------\\n\")\n",
    "for opinion, score in zip(selected_opinions[:3], polarity_scores[:3]):\n",
    "    print(f\"Opinión: {opinion}\\nPolaridad: {score}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3jukmO2e_nW2",
   "metadata": {
    "id": "3jukmO2e_nW2"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<b>Análisis</b>: De los tres casos presentados en el ejercicio anterior ¿la polaridad que se calcula es coherente con el comentario emitido por los clientes? Discute si hay alguna diferencia y ¿por qué se generaría una diferencia?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17fd040",
   "metadata": {},
   "source": [
    "1. La primera opinión claramente indica una experiencia negativa, describiendo problemas con la instalación de un software y la necesidad de realizar un escaneo completo de virus como medida preventiva. A pesar de esto, la polaridad calculada es casi neutral (-0.008). Esta discrepancia podría deberse a que el texto contiene una mezcla de términos neutros y negativos, y la presencia de términos técnicos específicos que no están reflejados en el diccionario AFINN con puntuaciones negativas.\n",
    "\n",
    "2. La segunda opinión es breve y directa sobre no cumplir con las expectativas, calificada como \"demasiado juvenil\" para el propósito del cliente. A pesar de ser una opinión negativa, la polaridad calculada es 0, lo que podría deberse a que las palabras utilizadas no están presentes en el diccionario AFINN o no están calificadas como negativas.\n",
    "\n",
    "3. La tercera opinión expresa varias frustraciones con la instalación y el servicio al cliente, junto con problemas técnicos específicos. Sin embargo, la polaridad es ligeramente positiva (0.017), lo que parece incoherente con el contenido general del comentario. Esto podría ser un resultado de algunas palabras positivas o neutras que contrarrestan las negativas, o de nuevo, la falta de representación de términos técnicos específicos en el diccionario de polaridad.\n",
    "\n",
    "La discrepancia en las puntuaciones de polaridad puede generarse por varias razones:\n",
    "\n",
    "* Vocabulario no incluido en AFINN: Si las opiniones contienen jerga técnica, nombres de productos o cualquier palabra que no esté en el diccionario AFINN, no contribuirán a la puntuación de polaridad.\n",
    "\n",
    "* Contexto: La polaridad de las palabras puede cambiar dependiendo del contexto en el que se usen, y los diccionarios de polaridad no siempre pueden capturar estos matices.\n",
    "\n",
    "* Sarcasmo o Ironía: El tono sarcástico o irónico puede ser interpretado incorrectamente por un análisis de texto basado en diccionario, ya que requiere un nivel de comprensión del lenguaje que va más allá del simple análisis de palabras individuales.\n",
    "\n",
    "* Neutralización: Las opiniones pueden contener una mezcla de comentarios positivos y negativos que se neutralizan entre sí, llevando a una puntuación de polaridad equilibrada o neutra.\n",
    "\n",
    "Para mejorar el análisis de sentimiento, podrían explorarse enfoques más avanzados como el análisis de sentimiento basado en aprendizaje automático con modelos que puedan entender el contexto y el tono de las opiniones, o el uso de un diccionario de polaridad más extenso o específico del dominio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561cfe1",
   "metadata": {
    "id": "1561cfe1"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Tercer paso:</i> Selecciona opiniones con polaridad negativa que ejemplifiquen los aspectos peor valorados (puedes utilizar un umbral para seleccionar las peores reseñas). Comenta cuáles son estos aspectos.\n",
    "<br>\n",
    "<b>Salida esperada: </b>\n",
    "<br>\n",
    "- Lista de las tres primeras opiniones negativas.\n",
    "<br>\n",
    "- Respuesta a la pregunta ¿Cuáles son los aspectos peor valorados por los clientes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f2bf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "412f2bf5",
    "outputId": "fa23122b-717c-4467-fd8b-b9dd6716275d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuartiles de puntuaciones de polaridad: [-0.0036431   0.01886792  0.05882353]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cálculo de cuartiles\n",
    "quartiles = np.percentile(polarity_scores, [25, 50, 75])\n",
    "print(f\"Cuartiles de puntuaciones de polaridad: {quartiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142b7d3-4933-4bd3-b8f4-fe62622d027f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "9142b7d3-4933-4bd3-b8f4-fe62622d027f",
    "outputId": "a627ea4c-4558-4c59-f5e6-7b39df93ac94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tres primeras opiniones negativas:\n",
      "Opinión: Not bad for the price.\n",
      "Polaridad: -0.6\n",
      "\n",
      "Opinión: This Product is a terrible waste of time and money\n",
      "Polaridad: -0.4\n",
      "\n",
      "Opinión: piss poor program. Does not teach you the language.\n",
      "Polaridad: -0.6666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar opiniones con polaridad negativa (por ejemplo, puntajes menores a -0.5)\n",
    "negative_opinions = [(opinion, score) for opinion, score in zip(selected_opinions, polarity_scores) if score <= -0.4]\n",
    "\n",
    "# Imprimir las tres primeras opiniones negativas\n",
    "print(\"Tres primeras opiniones negativas:\")\n",
    "for opinion, score in negative_opinions:\n",
    "    print(f\"Opinión: {opinion}\\nPolaridad: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652470c",
   "metadata": {},
   "source": [
    "**Valor por el precio**: La opinión \"Not bad for the price\" sugiere que, aunque el producto no es malo, hay una percepción de que podría ofrecer más por su costo. Esto podría implicar que los clientes sienten que no están obteniendo un valor adecuado por su dinero.\n",
    "\n",
    "**Eficiencia del producto**: La declaración \"This Product is a terrible waste of time and money\" indica una fuerte insatisfacción con el producto, tanto en términos de tiempo invertido como de coste económico. Los clientes pueden estar encontrando que el producto no cumple con las promesas publicitarias o no funciona como se esperaba.\n",
    "\n",
    "**Capacidad educativa/funcional**: La última opinión \"piss poor program. Does not teach you the language.\" es particularmente crítica sobre la capacidad educativa del producto, lo que indica que los usuarios no están logrando los resultados de aprendizaje esperados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22b5d6-b863-4ff6-92a6-a4e35325fa38",
   "metadata": {
    "id": "fd22b5d6-b863-4ff6-92a6-a4e35325fa38"
   },
   "source": [
    "# 5. Evaluación: comparación de modelos y discusión de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d8d2f",
   "metadata": {
    "id": "b69d8d2f"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Obtenemos los resultados de las métricas de evaluación del clasificador basado en regresión logística.\n",
    "\n",
    "\n",
    "<b>Salida esperada: </b> Métricas de redimiento del modelo de regresión logística (por clase).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6ad8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afc6ad8f",
    "outputId": "68c4459c-7a0b-41ee-fd34-d110ed688fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.85      0.74      0.79       827\n",
      "    Positivo       0.87      0.93      0.90      1636\n",
      "\n",
      "    accuracy                           0.87      2463\n",
      "   macro avg       0.86      0.83      0.84      2463\n",
      "weighted avg       0.87      0.87      0.86      2463\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 609  218]\n",
      " [ 111 1525]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de evaluación\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calcular las métricas de rendimiento\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5988e-ce7e-4b4c-afca-023941c34fa7",
   "metadata": {
    "id": "82c5988e-ce7e-4b4c-afca-023941c34fa7"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Obtenemos también los resultados de las métricas de evaluación para un clasificador diferente. Por ejemplo, utiliza SVM.\n",
    "<br>\n",
    "<b>Salida esperada: </b>\n",
    "<br>\n",
    "- Tiempo de ejecución que conlleva realizar el entrenamiento (fit()).\n",
    "<br>\n",
    "- Métricas de redimiento del modelo SVM (por clase).\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a7df8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "210a7df8",
    "outputId": "41deecf3-910e-4885-c823-6af9ef82b0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento del modelo SVM: 22.77092409133911 segundos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import time\n",
    "\n",
    "# Crear el clasificador SVM con kernel lineal\n",
    "svm_clf = SVC(kernel='linear')\n",
    "\n",
    "# Medir el tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Tiempo de ejecución para realizar el entrenamiento\n",
    "print(f\"Tiempo de entrenamiento del modelo SVM: {end_time - start_time} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c53609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de rendimiento del modelo SVM (por clase):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.83      0.79      0.81       827\n",
      "    Positivo       0.90      0.92      0.91      1636\n",
      "\n",
      "    accuracy                           0.87      2463\n",
      "   macro avg       0.86      0.85      0.86      2463\n",
      "weighted avg       0.87      0.87      0.87      2463\n",
      "\n",
      "Matriz de confusión del modelo SVM:\n",
      "[[ 654  173]\n",
      " [ 138 1498]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de evaluación con el modelo SVM\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "# Calcular las métricas de rendimiento para el modelo SVM\n",
    "print(\"Métricas de rendimiento del modelo SVM (por clase):\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['Negativo', 'Positivo']))\n",
    "\n",
    "# Mostrar la matriz de confusión para el modelo SVM\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"Matriz de confusión del modelo SVM:\")\n",
    "print(conf_matrix_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e4564",
   "metadata": {
    "id": "ce2e4564"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<strong>Análisis:</strong> Comparamos los dos modelos en función de las métricas de evaluación y del tiempo de ejecución, para ello, contesta las siguientes interrogantes:\n",
    "\n",
    "- ¿Cuál de los dos modelos tiene mejores valores de precision, recall y f1?\n",
    "- ¿Las diferencias en el rendimiento son significativas?\n",
    "- ¿Cuál de los dos modelos elegirías para predecir nuevas reseñas sobre productos tech? Considera las métricas de rendimiento, pero, además, analiza las diferencias en el tiempo de ejecución de cada modelo.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26dabf",
   "metadata": {},
   "source": [
    "**Precision**: La precisión del modelo SVM es ligeramente superior para la clase Positivo y ligeramente inferior para la clase Negativo en comparación con la Regresión Logística. Esto indica que el SVM es un poco más preciso al predecir reseñas positivas, pero un poco menos al predecir las negativas.\n",
    "\n",
    "**Recall**: El recall del modelo SVM también es superior para ambas clases en comparación con la Regresión Logística. Esto sugiere que el modelo SVM es capaz de identificar una mayor proporción de reseñas positivas y negativas reales como tales.\n",
    "\n",
    "**F1-Score**: El F1-Score, que es una medida de la precisión y recall, es ligeramente superior para ambas clases en el modelo SVM, indicando un equilibrio más favorable entre precisión y capacidad de identificar correctamente las clases.\n",
    "\n",
    "**Diferencias en el Rendimiento**\n",
    "Las diferencias en el rendimiento, aunque no son extremadamente grandes, muestran una mejora sutil pero consistente en favor del modelo SVM en términos de recall y F1-Score, lo que puede ser significativo en aplicaciones donde es crítico maximizar la identificación correcta de las clases, especialmente en entornos donde las falsas negativas o positivas tienen grandes implicaciones.\n",
    "\n",
    "**Tiempo de Ejecución**\n",
    "El tiempo de entrenamiento del modelo SVM es significativamente mayor que el de la Regresión Logística. Mientras que la Regresión Logística se entrenó en aproximadamente 0.35 segundos, el modelo SVM tomó más de 22 segundos. Esta diferencia de tiempo es sustancial y puede ser un factor crítico en situaciones que requieran un entrenamiento rápido o frecuente reentrenamiento del modelo.\n",
    "\n",
    "**Elección del Modelo**\n",
    "Si el principal criterio de selección es el rendimiento predictivo y las métricas de precisión y recall, el modelo SVM podría ser la mejor elección debido a su mejor rendimiento general en estas métricas.\n",
    "\n",
    "Sin embargo, si se considera el tiempo de ejecución junto con las métricas de rendimiento, la Regresión Logística presenta un compromiso atractivo, ofreciendo un rendimiento sólido con una fracción del tiempo de entrenamiento requerido por el modelo SVM.\n",
    "\n",
    "En un entorno de producción, donde tanto el rendimiento predictivo como el tiempo de ejecución son importantes, uno podría inclinarse por la Regresión Logística si los tiempos de entrenamiento rápidos son críticos o si el volumen de datos es muy grande y requiere entrenamientos frecuentes. Para aplicaciones donde el máximo rendimiento predictivo es prioritario y el tiempo de entrenamiento es menos crítico, el modelo SVM podría ser la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc46d32-c604-4f08-891e-d62f9198fd16",
   "metadata": {
    "id": "3cc46d32-c604-4f08-891e-d62f9198fd16"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "Como recordarás, durante la preparación del dataset, existen más reseñas de sentimiento positivo y menos de negativo, por tanto, en estos últimos pasos el clasificador fue entrenado con un dataset desbalanceado.\n",
    "\n",
    "De lo que habíamos visto antes, hay 4277 reseñas con etiqueta negativa y 8037 con etiqueta positiva. Para balancear el dataset vamos a adoptar la técnica undersampling, por tanto, tomaremos únicamente alrededor de 4276 reseñas con etiqueta positiva, lo cual equivale a aproximadamente al 50% del nuevo dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q5yt-btYMwAw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "Q5yt-btYMwAw",
    "outputId": "1f77a9e5-7d29-4740-ba68-f2817c5fa660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4276, 4) (4277, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Bought this software earlier in the year.  Took a little while to get it installed, but finally got it done.\\n\\nI wanted the software to allow me to have a portable B...</td>\n",
       "      <td>0</td>\n",
       "      <td>Bought this software earlier in the year. Took a little while to get it installed but finally got it done. I wanted the software to allow me to have a portable Blu Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have been using Quicken for over 10 years.  It is a great product.  I find it essential for managing my finances, planning retirement and preparing taxes. Online ac...</td>\n",
       "      <td>1</td>\n",
       "      <td>I have been using Quicken for over 10 years. It is a great product. I find it essential for managing my finances planning retirement and preparing taxes. Online acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Best music app...actually shuffles my music in my playlists....so i don't listen to the music in the same exact order everyday</td>\n",
       "      <td>1</td>\n",
       "      <td>Best music app...actually shuffles my music in my playlists....so i don't listen to the music in the same exact order everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6713</th>\n",
       "      <td>2.0</td>\n",
       "      <td>I was very excited to order Snow Leopard (mainly due to the Grand Central Dispatch) technology.  After installing last night I found my Rev1 MBP overheats like crazy....</td>\n",
       "      <td>0</td>\n",
       "      <td>I was very excited to order Snow Leopard mainly due to the Grand Central Dispatch technology. After installing last night I found my Rev1 MBP overheats like crazy. Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This year's edition offered no surprises.  The conversion process for the data file from the previous year completed without incident.  Quick Update with banks and cr...</td>\n",
       "      <td>1</td>\n",
       "      <td>This year's edition offered no surprises. The conversion process for the data file from the previous year completed without incident. Quick Update with banks and cred...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall  \\\n",
       "7038      1.0   \n",
       "1197      5.0   \n",
       "2862      5.0   \n",
       "6713      2.0   \n",
       "2860      5.0   \n",
       "\n",
       "                                                                                                                                                                     reviewText  \\\n",
       "7038  Bought this software earlier in the year.  Took a little while to get it installed, but finally got it done.\\n\\nI wanted the software to allow me to have a portable B...   \n",
       "1197  I have been using Quicken for over 10 years.  It is a great product.  I find it essential for managing my finances, planning retirement and preparing taxes. Online ac...   \n",
       "2862                                             Best music app...actually shuffles my music in my playlists....so i don't listen to the music in the same exact order everyday   \n",
       "6713  I was very excited to order Snow Leopard (mainly due to the Grand Central Dispatch) technology.  After installing last night I found my Rev1 MBP overheats like crazy....   \n",
       "2860  This year's edition offered no surprises.  The conversion process for the data file from the previous year completed without incident.  Quick Update with banks and cr...   \n",
       "\n",
       "      sentiment  \\\n",
       "7038          0   \n",
       "1197          1   \n",
       "2862          1   \n",
       "6713          0   \n",
       "2860          1   \n",
       "\n",
       "                                                                                                                                                                           text  \n",
       "7038  Bought this software earlier in the year. Took a little while to get it installed but finally got it done. I wanted the software to allow me to have a portable Blu Ra...  \n",
       "1197  I have been using Quicken for over 10 years. It is a great product. I find it essential for managing my finances planning retirement and preparing taxes. Online acces...  \n",
       "2862                                             Best music app...actually shuffles my music in my playlists....so i don't listen to the music in the same exact order everyday  \n",
       "6713  I was very excited to order Snow Leopard mainly due to the Grand Central Dispatch technology. After installing last night I found my Rev1 MBP overheats like crazy. Af...  \n",
       "2860  This year's edition offered no surprises. The conversion process for the data file from the previous year completed without incident. Quick Update with banks and cred...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar las reseñas según su sentimiento para tomar igual cantidad para la clase 1 y 0.\n",
    "opinionsPos = df[df[\"sentiment\"] == 1] # opiniones positivas\n",
    "opinionsNeg = df[df[\"sentiment\"] == 0]  # opiniones negativas\n",
    "\n",
    "opinionsPos = opinionsPos.sample(frac=0.532, random_state = 42) # tomar una fracción de la opiones de clase mayoritaria.\n",
    "\n",
    "print(opinionsPos.shape, opinionsNeg.shape)\n",
    "\n",
    "# Integrar reseñas y separar en listas:\n",
    "\n",
    "dfb = pd.concat([opinionsPos, opinionsNeg], ignore_index=True)\n",
    "opinions = dfb[\"text\"].to_list()\n",
    "labels = dfb['sentiment'].to_list()\n",
    "dfb.sample(5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chDzxzFbWWed",
   "metadata": {
    "id": "chDzxzFbWWed"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<i>Paso 1</i>: En este último ejercicio, repite los experimentos pero con el nuevo dataset, y repite los experimentos con los modelos de regresión logística y SVM.\n",
    "\n",
    "<br>\n",
    "<b>Salida esperada:</b> Métricas de cada modelo y tiempo de ejecución del fit().\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PozWG7Hdw6pc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PozWG7Hdw6pc",
    "outputId": "245354ff-a5ef-4fe2-87d9-f2c04b0b593a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de entrenamiento Regresión Logística: 0.20613789558410645 segundos\n",
      "Métricas Regresión Logística (dataset balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       848\n",
      "           1       0.87      0.86      0.86       863\n",
      "\n",
      "    accuracy                           0.86      1711\n",
      "   macro avg       0.86      0.86      0.86      1711\n",
      "weighted avg       0.86      0.86      0.86      1711\n",
      "\n",
      "Tiempo de entrenamiento SVM: 11.935551166534424 segundos\n",
      "Métricas SVM (dataset balanceado):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       848\n",
      "           1       0.87      0.86      0.87       863\n",
      "\n",
      "    accuracy                           0.87      1711\n",
      "   macro avg       0.87      0.87      0.87      1711\n",
      "weighted avg       0.87      0.87      0.87      1711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar las opiniones con un vectorizador TF-IDF, usando 'word' como analyzer\n",
    "vectorizer = TfidfVectorizer(analyzer='word')\n",
    "X_tfidf_balanced = vectorizer.fit_transform(opinions)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y evaluación\n",
    "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_tfidf_balanced, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar al clasificador de Regresión Logística\n",
    "start_time = time.time()\n",
    "clf_logistic_balanced = LogisticRegression()\n",
    "clf_logistic_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "end_time = time.time()\n",
    "\n",
    "# Tiempo de ejecución para realizar el entrenamiento\n",
    "print(f\"Tiempo de entrenamiento Regresión Logística: {end_time - start_time} segundos\")\n",
    "\n",
    "# Métricas de rendimiento\n",
    "y_pred_logistic_balanced = clf_logistic_balanced.predict(X_test_balanced)\n",
    "print(\"Métricas Regresión Logística (dataset balanceado):\")\n",
    "print(classification_report(y_test_balanced, y_pred_logistic_balanced))\n",
    "\n",
    "# Entrenar al clasificador SVM\n",
    "start_time = time.time()\n",
    "clf_svm_balanced = SVC(kernel='linear')\n",
    "clf_svm_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "end_time = time.time()\n",
    "\n",
    "# Tiempo de ejecución para realizar el entrenamiento\n",
    "print(f\"Tiempo de entrenamiento SVM: {end_time - start_time} segundos\")\n",
    "\n",
    "# Métricas de rendimiento\n",
    "y_pred_svm_balanced = clf_svm_balanced.predict(X_test_balanced)\n",
    "print(\"Métricas SVM (dataset balanceado):\")\n",
    "print(classification_report(y_test_balanced, y_pred_svm_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FT0LKCCyxROl",
   "metadata": {
    "id": "FT0LKCCyxROl"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "\n",
    "<strong>Análisis:</strong> ¿Qué diferencias observas entre los clasificadores entrenados con el dataset desbalanceado, vs. los clasificadores y el dataset balanceado? Comenta las principales diferencias y similitudes.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f1493",
   "metadata": {},
   "source": [
    "**Diferencias**\n",
    "\n",
    "Equilibrio entre las clases: Al balancear el dataset, ambos modelos muestran una mejora notable en el equilibrio entre las métricas de precisión, recall y F1-score para ambas clases. Esto es especialmente importante para la clase minoritaria (reseñas negativas en el dataset desbalanceado), donde anteriormente podría haber habido un rendimiento subóptimo.\n",
    "\n",
    "Rendimiento general: Las métricas de rendimiento son relativamente similares entre los modelos entrenados con el dataset desbalanceado y balanceado, pero se observa una ligera mejora en la uniformidad del rendimiento a través de las clases en el dataset balanceado. Esto indica que balancear el dataset mejora la capacidad del modelo para generalizar a través de ambas clases.\n",
    "\n",
    "Tiempo de entrenamiento: Hay una reducción notable en el tiempo de entrenamiento para ambos modelos cuando se entrena con el dataset balanceado. Esto es esperado, ya que al reducir el número de reseñas positivas para balancear el dataset, se reduce el tamaño total del dataset.\n",
    "\n",
    "**Similitudes**\n",
    "\n",
    "Precisión y recall equilibrados: Ambos clasificadores mantienen un buen equilibrio entre precisión y recall para ambas clases en el dataset balanceado, similar a lo observado en el dataset desbalanceado, aunque con mejoras en la clase minoritaria.\n",
    "\n",
    "Diferencia de tiempo de entrenamiento entre modelos: La diferencia de tiempo de entrenamiento entre la Regresión Logística y SVM se mantiene, con SVM tomando significativamente más tiempo para entrenar que la Regresión Logística en ambos escenarios. Esto es consistente con la naturaleza computacionalmente más intensiva de SVM, especialmente con kernels lineales en grandes volúmenes de datos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
